{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "instrumental-washer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "smaller-graphics",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank = pd.read_csv('Bank Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "operating-proxy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Income</th>\n",
       "      <th>ZIP_Code</th>\n",
       "      <th>Family_members</th>\n",
       "      <th>CCAvg</th>\n",
       "      <th>Education</th>\n",
       "      <th>Mortgage</th>\n",
       "      <th>Personal_Loan</th>\n",
       "      <th>Securities_Account</th>\n",
       "      <th>CD_Account</th>\n",
       "      <th>Online</th>\n",
       "      <th>CreditCard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>91107</td>\n",
       "      <td>4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>34</td>\n",
       "      <td>90089</td>\n",
       "      <td>3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>94720</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>94112</td>\n",
       "      <td>1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>8</td>\n",
       "      <td>45</td>\n",
       "      <td>91330</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Age  Experience  Income  ZIP_Code  Family_members  CCAvg  Education  \\\n",
       "0   1   25           1      49     91107               4    1.6          1   \n",
       "1   2   45          19      34     90089               3    1.5          1   \n",
       "2   3   39          15      11     94720               1    1.0          1   \n",
       "3   4   35           9     100     94112               1    2.7          2   \n",
       "4   5   35           8      45     91330               4    1.0          2   \n",
       "\n",
       "   Mortgage  Personal_Loan  Securities_Account  CD_Account  Online  CreditCard  \n",
       "0         0              0                   1           0       0           0  \n",
       "1         0              0                   1           0       0           0  \n",
       "2         0              0                   0           0       0           0  \n",
       "3         0              0                   0           0       0           0  \n",
       "4         0              0                   0           0       0           1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "periodic-aluminum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4982, 14)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "basic-feelings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4982 entries, 0 to 4981\n",
      "Data columns (total 14 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   ID                  4982 non-null   int64  \n",
      " 1   Age                 4982 non-null   int64  \n",
      " 2   Experience          4982 non-null   int64  \n",
      " 3   Income              4982 non-null   int64  \n",
      " 4   ZIP_Code            4982 non-null   int64  \n",
      " 5   Family_members      4982 non-null   int64  \n",
      " 6   CCAvg               4982 non-null   float64\n",
      " 7   Education           4982 non-null   int64  \n",
      " 8   Mortgage            4982 non-null   int64  \n",
      " 9   Personal_Loan       4982 non-null   int64  \n",
      " 10  Securities_Account  4982 non-null   int64  \n",
      " 11  CD_Account          4982 non-null   int64  \n",
      " 12  Online              4982 non-null   int64  \n",
      " 13  CreditCard          4982 non-null   int64  \n",
      "dtypes: float64(1), int64(13)\n",
      "memory usage: 545.0 KB\n"
     ]
    }
   ],
   "source": [
    "bank.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "finished-carolina",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank.isnull().sum().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "compact-exemption",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Income</th>\n",
       "      <th>Family_members</th>\n",
       "      <th>CCAvg</th>\n",
       "      <th>Education</th>\n",
       "      <th>Mortgage</th>\n",
       "      <th>Personal_Loan</th>\n",
       "      <th>Securities_Account</th>\n",
       "      <th>CD_Account</th>\n",
       "      <th>Online</th>\n",
       "      <th>CreditCard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35</td>\n",
       "      <td>8</td>\n",
       "      <td>45</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4977</th>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4978</th>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4979</th>\n",
       "      <td>63</td>\n",
       "      <td>39</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4980</th>\n",
       "      <td>65</td>\n",
       "      <td>40</td>\n",
       "      <td>49</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4981</th>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>83</td>\n",
       "      <td>3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4982 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Age  Experience  Income  Family_members  CCAvg  Education  Mortgage  \\\n",
       "0      25           1      49               4    1.6          1         0   \n",
       "1      45          19      34               3    1.5          1         0   \n",
       "2      39          15      11               1    1.0          1         0   \n",
       "3      35           9     100               1    2.7          2         0   \n",
       "4      35           8      45               4    1.0          2         0   \n",
       "...   ...         ...     ...             ...    ...        ...       ...   \n",
       "4977   29           3      40               1    1.9          3         0   \n",
       "4978   30           4      15               4    0.4          1        85   \n",
       "4979   63          39      24               2    0.3          3         0   \n",
       "4980   65          40      49               3    0.5          2         0   \n",
       "4981   28           4      83               3    0.8          1         0   \n",
       "\n",
       "      Personal_Loan  Securities_Account  CD_Account  Online  CreditCard  \n",
       "0                 0                   1           0       0           0  \n",
       "1                 0                   1           0       0           0  \n",
       "2                 0                   0           0       0           0  \n",
       "3                 0                   0           0       0           0  \n",
       "4                 0                   0           0       0           1  \n",
       "...             ...                 ...         ...     ...         ...  \n",
       "4977              0                   0           0       1           0  \n",
       "4978              0                   0           0       1           0  \n",
       "4979              0                   0           0       0           0  \n",
       "4980              0                   0           0       1           0  \n",
       "4981              0                   0           0       1           1  \n",
       "\n",
       "[4982 rows x 12 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank.drop(['ID','ZIP_Code'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "disciplinary-hollywood",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baking-sheet",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = bank['Personal_Loan']\n",
    "x = bank.drop(['Personal_Loan'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "applied-tyler",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y , test_size=0.30, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "passing-hypothetical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:  (3487, 13)\n",
      "x_test:  (1495, 13)\n",
      "y_train:  (3487,)\n",
      "y_test:  (1495,)\n"
     ]
    }
   ],
   "source": [
    "# Checking dimensions on the train and test data\n",
    "print('x_train: ',x_train.shape)\n",
    "print('x_test: ',x_test.shape)\n",
    "print('y_train: ',y_train.shape)\n",
    "print('y_test: ',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "beginning-chapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "lonely-toyota",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "growing-orbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = sc.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "sustained-monthly",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.06261795,  0.75866264,  0.34199978, ..., -0.25823838,\n",
       "         0.81708209, -0.64657259],\n",
       "       [ 1.44189417,  1.54405303,  1.47477723, ..., -0.25823838,\n",
       "         0.81708209,  1.54661674],\n",
       "       [-0.28646199,  1.19499064,  1.30050378, ..., -0.25823838,\n",
       "        -1.22386723, -0.64657259],\n",
       "       ...,\n",
       "       [-1.11264272,  1.02045944,  1.0390936 , ..., -0.25823838,\n",
       "        -1.22386723, -0.64657259],\n",
       "       [ 1.03472999, -1.33571173, -1.2264613 , ..., -0.25823838,\n",
       "         0.81708209, -0.64657259],\n",
       "       [-1.5818576 ,  0.49686585,  0.51627324, ..., -0.25823838,\n",
       "        -1.22386723, -0.64657259]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "proud-extreme",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = sc.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "shaped-falls",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.46559893, -0.98664934, -1.13932457, ..., -0.25823838,\n",
       "         0.81708209, -0.64657259],\n",
       "       [ 0.33474226, -0.28852455, -0.18082058, ..., -0.25823838,\n",
       "         0.81708209, -0.64657259],\n",
       "       [ 1.29757399, -1.59750853, -1.48787148, ..., -0.25823838,\n",
       "        -1.22386723, -0.64657259],\n",
       "       ...,\n",
       "       [-1.5212013 ,  1.10772504,  1.0390936 , ..., -0.25823838,\n",
       "         0.81708209, -0.64657259],\n",
       "       [-0.37709785,  0.75866264,  0.34199978, ..., -0.25823838,\n",
       "         0.81708209, -0.64657259],\n",
       "       [-1.28136486,  1.71858423,  1.82332414, ...,  3.8723911 ,\n",
       "         0.81708209, -0.64657259]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "anonymous-player",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=100, max_iter=5000, solver='sgd', verbose=True, random_state=1, tol=0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "sublime-statement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.55148186\n",
      "Iteration 2, loss = 0.47344196\n",
      "Iteration 3, loss = 0.41166049\n",
      "Iteration 4, loss = 0.37065489\n",
      "Iteration 5, loss = 0.34284418\n",
      "Iteration 6, loss = 0.32277304\n",
      "Iteration 7, loss = 0.30732533\n",
      "Iteration 8, loss = 0.29474324\n",
      "Iteration 9, loss = 0.28426676\n",
      "Iteration 10, loss = 0.27512518\n",
      "Iteration 11, loss = 0.26713147\n",
      "Iteration 12, loss = 0.26000083\n",
      "Iteration 13, loss = 0.25345890\n",
      "Iteration 14, loss = 0.24756049\n",
      "Iteration 15, loss = 0.24206441\n",
      "Iteration 16, loss = 0.23692905\n",
      "Iteration 17, loss = 0.23225323\n",
      "Iteration 18, loss = 0.22784432\n",
      "Iteration 19, loss = 0.22375707\n",
      "Iteration 20, loss = 0.21989675\n",
      "Iteration 21, loss = 0.21632681\n",
      "Iteration 22, loss = 0.21286450\n",
      "Iteration 23, loss = 0.20963389\n",
      "Iteration 24, loss = 0.20665625\n",
      "Iteration 25, loss = 0.20375244\n",
      "Iteration 26, loss = 0.20103807\n",
      "Iteration 27, loss = 0.19841429\n",
      "Iteration 28, loss = 0.19595709\n",
      "Iteration 29, loss = 0.19358344\n",
      "Iteration 30, loss = 0.19130855\n",
      "Iteration 31, loss = 0.18915653\n",
      "Iteration 32, loss = 0.18707682\n",
      "Iteration 33, loss = 0.18512113\n",
      "Iteration 34, loss = 0.18318974\n",
      "Iteration 35, loss = 0.18137996\n",
      "Iteration 36, loss = 0.17957917\n",
      "Iteration 37, loss = 0.17785690\n",
      "Iteration 38, loss = 0.17620253\n",
      "Iteration 39, loss = 0.17460533\n",
      "Iteration 40, loss = 0.17304587\n",
      "Iteration 41, loss = 0.17155023\n",
      "Iteration 42, loss = 0.17010999\n",
      "Iteration 43, loss = 0.16870569\n",
      "Iteration 44, loss = 0.16733449\n",
      "Iteration 45, loss = 0.16596710\n",
      "Iteration 46, loss = 0.16464603\n",
      "Iteration 47, loss = 0.16340738\n",
      "Iteration 48, loss = 0.16215959\n",
      "Iteration 49, loss = 0.16094158\n",
      "Iteration 50, loss = 0.15976569\n",
      "Iteration 51, loss = 0.15862007\n",
      "Iteration 52, loss = 0.15750767\n",
      "Iteration 53, loss = 0.15639947\n",
      "Iteration 54, loss = 0.15532028\n",
      "Iteration 55, loss = 0.15427988\n",
      "Iteration 56, loss = 0.15323968\n",
      "Iteration 57, loss = 0.15225316\n",
      "Iteration 58, loss = 0.15126445\n",
      "Iteration 59, loss = 0.15031132\n",
      "Iteration 60, loss = 0.14936615\n",
      "Iteration 61, loss = 0.14843927\n",
      "Iteration 62, loss = 0.14754865\n",
      "Iteration 63, loss = 0.14667074\n",
      "Iteration 64, loss = 0.14582058\n",
      "Iteration 65, loss = 0.14495607\n",
      "Iteration 66, loss = 0.14412183\n",
      "Iteration 67, loss = 0.14331773\n",
      "Iteration 68, loss = 0.14249754\n",
      "Iteration 69, loss = 0.14171909\n",
      "Iteration 70, loss = 0.14094309\n",
      "Iteration 71, loss = 0.14017218\n",
      "Iteration 72, loss = 0.13943220\n",
      "Iteration 73, loss = 0.13870326\n",
      "Iteration 74, loss = 0.13798095\n",
      "Iteration 75, loss = 0.13724548\n",
      "Iteration 76, loss = 0.13656050\n",
      "Iteration 77, loss = 0.13585265\n",
      "Iteration 78, loss = 0.13517994\n",
      "Iteration 79, loss = 0.13454038\n",
      "Iteration 80, loss = 0.13384930\n",
      "Iteration 81, loss = 0.13321829\n",
      "Iteration 82, loss = 0.13258111\n",
      "Iteration 83, loss = 0.13196280\n",
      "Iteration 84, loss = 0.13133738\n",
      "Iteration 85, loss = 0.13072710\n",
      "Iteration 86, loss = 0.13012198\n",
      "Iteration 87, loss = 0.12955036\n",
      "Iteration 88, loss = 0.12895710\n",
      "Iteration 89, loss = 0.12839353\n",
      "Iteration 90, loss = 0.12781913\n",
      "Iteration 91, loss = 0.12727913\n",
      "Iteration 92, loss = 0.12672678\n",
      "Iteration 93, loss = 0.12619330\n",
      "Iteration 94, loss = 0.12564987\n",
      "Iteration 95, loss = 0.12514429\n",
      "Iteration 96, loss = 0.12463012\n",
      "Iteration 97, loss = 0.12413106\n",
      "Iteration 98, loss = 0.12363732\n",
      "Iteration 99, loss = 0.12313979\n",
      "Iteration 100, loss = 0.12265871\n",
      "Iteration 101, loss = 0.12219403\n",
      "Iteration 102, loss = 0.12172546\n",
      "Iteration 103, loss = 0.12125923\n",
      "Iteration 104, loss = 0.12079632\n",
      "Iteration 105, loss = 0.12035694\n",
      "Iteration 106, loss = 0.11990259\n",
      "Iteration 107, loss = 0.11949671\n",
      "Iteration 108, loss = 0.11903597\n",
      "Iteration 109, loss = 0.11861779\n",
      "Iteration 110, loss = 0.11820339\n",
      "Iteration 111, loss = 0.11779444\n",
      "Iteration 112, loss = 0.11736802\n",
      "Iteration 113, loss = 0.11695797\n",
      "Iteration 114, loss = 0.11656016\n",
      "Iteration 115, loss = 0.11617243\n",
      "Iteration 116, loss = 0.11578205\n",
      "Iteration 117, loss = 0.11539590\n",
      "Iteration 118, loss = 0.11501839\n",
      "Iteration 119, loss = 0.11465023\n",
      "Iteration 120, loss = 0.11428609\n",
      "Iteration 121, loss = 0.11391986\n",
      "Iteration 122, loss = 0.11354521\n",
      "Iteration 123, loss = 0.11320104\n",
      "Iteration 124, loss = 0.11284590\n",
      "Iteration 125, loss = 0.11251472\n",
      "Iteration 126, loss = 0.11215936\n",
      "Iteration 127, loss = 0.11182913\n",
      "Iteration 128, loss = 0.11149552\n",
      "Iteration 129, loss = 0.11115824\n",
      "Iteration 130, loss = 0.11082834\n",
      "Iteration 131, loss = 0.11051170\n",
      "Iteration 132, loss = 0.11019863\n",
      "Iteration 133, loss = 0.10986587\n",
      "Iteration 134, loss = 0.10955708\n",
      "Iteration 135, loss = 0.10925955\n",
      "Iteration 136, loss = 0.10893812\n",
      "Iteration 137, loss = 0.10865262\n",
      "Iteration 138, loss = 0.10833636\n",
      "Iteration 139, loss = 0.10805099\n",
      "Iteration 140, loss = 0.10776171\n",
      "Iteration 141, loss = 0.10747370\n",
      "Iteration 142, loss = 0.10718504\n",
      "Iteration 143, loss = 0.10689424\n",
      "Iteration 144, loss = 0.10661656\n",
      "Iteration 145, loss = 0.10634853\n",
      "Iteration 146, loss = 0.10607494\n",
      "Iteration 147, loss = 0.10579123\n",
      "Iteration 148, loss = 0.10552967\n",
      "Iteration 149, loss = 0.10526404\n",
      "Iteration 150, loss = 0.10499642\n",
      "Iteration 151, loss = 0.10474058\n",
      "Iteration 152, loss = 0.10447834\n",
      "Iteration 153, loss = 0.10422782\n",
      "Iteration 154, loss = 0.10397527\n",
      "Iteration 155, loss = 0.10371297\n",
      "Iteration 156, loss = 0.10345950\n",
      "Iteration 157, loss = 0.10321449\n",
      "Iteration 158, loss = 0.10296706\n",
      "Iteration 159, loss = 0.10273170\n",
      "Iteration 160, loss = 0.10250314\n",
      "Iteration 161, loss = 0.10225038\n",
      "Iteration 162, loss = 0.10200474\n",
      "Iteration 163, loss = 0.10177646\n",
      "Iteration 164, loss = 0.10154048\n",
      "Iteration 165, loss = 0.10131235\n",
      "Iteration 166, loss = 0.10108115\n",
      "Iteration 167, loss = 0.10085383\n",
      "Iteration 168, loss = 0.10064470\n",
      "Iteration 169, loss = 0.10041089\n",
      "Iteration 170, loss = 0.10018556\n",
      "Iteration 171, loss = 0.09997728\n",
      "Iteration 172, loss = 0.09975914\n",
      "Iteration 173, loss = 0.09954369\n",
      "Iteration 174, loss = 0.09933805\n",
      "Iteration 175, loss = 0.09912760\n",
      "Iteration 176, loss = 0.09891499\n",
      "Iteration 177, loss = 0.09871245\n",
      "Iteration 178, loss = 0.09850652\n",
      "Iteration 179, loss = 0.09830098\n",
      "Iteration 180, loss = 0.09809043\n",
      "Iteration 181, loss = 0.09788552\n",
      "Iteration 182, loss = 0.09769358\n",
      "Iteration 183, loss = 0.09749674\n",
      "Iteration 184, loss = 0.09729772\n",
      "Iteration 185, loss = 0.09710282\n",
      "Iteration 186, loss = 0.09691228\n",
      "Iteration 187, loss = 0.09672293\n",
      "Iteration 188, loss = 0.09653274\n",
      "Iteration 189, loss = 0.09634344\n",
      "Iteration 190, loss = 0.09615486\n",
      "Iteration 191, loss = 0.09595847\n",
      "Iteration 192, loss = 0.09577601\n",
      "Iteration 193, loss = 0.09559018\n",
      "Iteration 194, loss = 0.09541277\n",
      "Iteration 195, loss = 0.09523304\n",
      "Iteration 196, loss = 0.09504856\n",
      "Iteration 197, loss = 0.09486516\n",
      "Iteration 198, loss = 0.09469637\n",
      "Iteration 199, loss = 0.09452225\n",
      "Iteration 200, loss = 0.09434040\n",
      "Iteration 201, loss = 0.09416431\n",
      "Iteration 202, loss = 0.09399505\n",
      "Iteration 203, loss = 0.09382679\n",
      "Iteration 204, loss = 0.09365321\n",
      "Iteration 205, loss = 0.09348358\n",
      "Iteration 206, loss = 0.09331144\n",
      "Iteration 207, loss = 0.09314094\n",
      "Iteration 208, loss = 0.09297534\n",
      "Iteration 209, loss = 0.09281264\n",
      "Iteration 210, loss = 0.09264250\n",
      "Iteration 211, loss = 0.09247915\n",
      "Iteration 212, loss = 0.09231716\n",
      "Iteration 213, loss = 0.09214933\n",
      "Iteration 214, loss = 0.09199258\n",
      "Iteration 215, loss = 0.09182692\n",
      "Iteration 216, loss = 0.09166562\n",
      "Iteration 217, loss = 0.09150660\n",
      "Iteration 218, loss = 0.09136384\n",
      "Iteration 219, loss = 0.09120005\n",
      "Iteration 220, loss = 0.09104623\n",
      "Iteration 221, loss = 0.09087868\n",
      "Iteration 222, loss = 0.09072350\n",
      "Iteration 223, loss = 0.09057303\n",
      "Iteration 224, loss = 0.09042688\n",
      "Iteration 225, loss = 0.09026430\n",
      "Iteration 226, loss = 0.09011330\n",
      "Iteration 227, loss = 0.08997013\n",
      "Iteration 228, loss = 0.08981805\n",
      "Iteration 229, loss = 0.08967332\n",
      "Iteration 230, loss = 0.08952510\n",
      "Iteration 231, loss = 0.08937995\n",
      "Iteration 232, loss = 0.08923250\n",
      "Iteration 233, loss = 0.08909925\n",
      "Iteration 234, loss = 0.08895608\n",
      "Iteration 235, loss = 0.08880800\n",
      "Iteration 236, loss = 0.08865606\n",
      "Iteration 237, loss = 0.08851317\n",
      "Iteration 238, loss = 0.08837225\n",
      "Iteration 239, loss = 0.08824087\n",
      "Iteration 240, loss = 0.08809471\n",
      "Iteration 241, loss = 0.08795638\n",
      "Iteration 242, loss = 0.08782146\n",
      "Iteration 243, loss = 0.08768124\n",
      "Iteration 244, loss = 0.08755450\n",
      "Iteration 245, loss = 0.08741242\n",
      "Iteration 246, loss = 0.08729101\n",
      "Iteration 247, loss = 0.08715309\n",
      "Iteration 248, loss = 0.08701091\n",
      "Iteration 249, loss = 0.08688300\n",
      "Iteration 250, loss = 0.08674946\n",
      "Iteration 251, loss = 0.08661953\n",
      "Iteration 252, loss = 0.08649764\n",
      "Iteration 253, loss = 0.08637237\n",
      "Iteration 254, loss = 0.08623241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 255, loss = 0.08610292\n",
      "Iteration 256, loss = 0.08598602\n",
      "Iteration 257, loss = 0.08585570\n",
      "Iteration 258, loss = 0.08573337\n",
      "Iteration 259, loss = 0.08561058\n",
      "Iteration 260, loss = 0.08547765\n",
      "Iteration 261, loss = 0.08535151\n",
      "Iteration 262, loss = 0.08522880\n",
      "Iteration 263, loss = 0.08510682\n",
      "Iteration 264, loss = 0.08498405\n",
      "Iteration 265, loss = 0.08486921\n",
      "Iteration 266, loss = 0.08475379\n",
      "Iteration 267, loss = 0.08462800\n",
      "Iteration 268, loss = 0.08450652\n",
      "Iteration 269, loss = 0.08438816\n",
      "Iteration 270, loss = 0.08427141\n",
      "Iteration 271, loss = 0.08415750\n",
      "Iteration 272, loss = 0.08403027\n",
      "Iteration 273, loss = 0.08391273\n",
      "Iteration 274, loss = 0.08380498\n",
      "Iteration 275, loss = 0.08368326\n",
      "Iteration 276, loss = 0.08356750\n",
      "Iteration 277, loss = 0.08344806\n",
      "Iteration 278, loss = 0.08333515\n",
      "Iteration 279, loss = 0.08321948\n",
      "Iteration 280, loss = 0.08310471\n",
      "Iteration 281, loss = 0.08300229\n",
      "Iteration 282, loss = 0.08287895\n",
      "Iteration 283, loss = 0.08277239\n",
      "Iteration 284, loss = 0.08266363\n",
      "Iteration 285, loss = 0.08254977\n",
      "Iteration 286, loss = 0.08243953\n",
      "Iteration 287, loss = 0.08232790\n",
      "Iteration 288, loss = 0.08222242\n",
      "Iteration 289, loss = 0.08211272\n",
      "Iteration 290, loss = 0.08200717\n",
      "Iteration 291, loss = 0.08189105\n",
      "Iteration 292, loss = 0.08178664\n",
      "Iteration 293, loss = 0.08167899\n",
      "Iteration 294, loss = 0.08157239\n",
      "Iteration 295, loss = 0.08146383\n",
      "Iteration 296, loss = 0.08136769\n",
      "Iteration 297, loss = 0.08126039\n",
      "Iteration 298, loss = 0.08114917\n",
      "Iteration 299, loss = 0.08105418\n",
      "Iteration 300, loss = 0.08093657\n",
      "Iteration 301, loss = 0.08083085\n",
      "Iteration 302, loss = 0.08072840\n",
      "Iteration 303, loss = 0.08062631\n",
      "Iteration 304, loss = 0.08052013\n",
      "Iteration 305, loss = 0.08041966\n",
      "Iteration 306, loss = 0.08031789\n",
      "Iteration 307, loss = 0.08022150\n",
      "Iteration 308, loss = 0.08010868\n",
      "Iteration 309, loss = 0.08001851\n",
      "Iteration 310, loss = 0.07990960\n",
      "Iteration 311, loss = 0.07980724\n",
      "Iteration 312, loss = 0.07970578\n",
      "Iteration 313, loss = 0.07960547\n",
      "Iteration 314, loss = 0.07949439\n",
      "Iteration 315, loss = 0.07940012\n",
      "Iteration 316, loss = 0.07929778\n",
      "Iteration 317, loss = 0.07920491\n",
      "Iteration 318, loss = 0.07910429\n",
      "Iteration 319, loss = 0.07900225\n",
      "Iteration 320, loss = 0.07890873\n",
      "Iteration 321, loss = 0.07881779\n",
      "Iteration 322, loss = 0.07871906\n",
      "Iteration 323, loss = 0.07862275\n",
      "Iteration 324, loss = 0.07852004\n",
      "Iteration 325, loss = 0.07842502\n",
      "Iteration 326, loss = 0.07833810\n",
      "Iteration 327, loss = 0.07824161\n",
      "Iteration 328, loss = 0.07813760\n",
      "Iteration 329, loss = 0.07804425\n",
      "Iteration 330, loss = 0.07795175\n",
      "Iteration 331, loss = 0.07785726\n",
      "Iteration 332, loss = 0.07776717\n",
      "Iteration 333, loss = 0.07768040\n",
      "Iteration 334, loss = 0.07758029\n",
      "Iteration 335, loss = 0.07749441\n",
      "Iteration 336, loss = 0.07739536\n",
      "Iteration 337, loss = 0.07731581\n",
      "Iteration 338, loss = 0.07722232\n",
      "Iteration 339, loss = 0.07712912\n",
      "Iteration 340, loss = 0.07704092\n",
      "Iteration 341, loss = 0.07695633\n",
      "Iteration 342, loss = 0.07686555\n",
      "Iteration 343, loss = 0.07678282\n",
      "Iteration 344, loss = 0.07669096\n",
      "Iteration 345, loss = 0.07660533\n",
      "Iteration 346, loss = 0.07651555\n",
      "Iteration 347, loss = 0.07643189\n",
      "Iteration 348, loss = 0.07634534\n",
      "Iteration 349, loss = 0.07625878\n",
      "Iteration 350, loss = 0.07617005\n",
      "Iteration 351, loss = 0.07609022\n",
      "Iteration 352, loss = 0.07601375\n",
      "Iteration 353, loss = 0.07591914\n",
      "Iteration 354, loss = 0.07583382\n",
      "Iteration 355, loss = 0.07575322\n",
      "Iteration 356, loss = 0.07566508\n",
      "Iteration 357, loss = 0.07559092\n",
      "Iteration 358, loss = 0.07551442\n",
      "Iteration 359, loss = 0.07542100\n",
      "Iteration 360, loss = 0.07533962\n",
      "Iteration 361, loss = 0.07525522\n",
      "Iteration 362, loss = 0.07517740\n",
      "Iteration 363, loss = 0.07509563\n",
      "Iteration 364, loss = 0.07501262\n",
      "Iteration 365, loss = 0.07493588\n",
      "Iteration 366, loss = 0.07485738\n",
      "Iteration 367, loss = 0.07477538\n",
      "Iteration 368, loss = 0.07469466\n",
      "Iteration 369, loss = 0.07461274\n",
      "Iteration 370, loss = 0.07453462\n",
      "Iteration 371, loss = 0.07445755\n",
      "Iteration 372, loss = 0.07437639\n",
      "Iteration 373, loss = 0.07430311\n",
      "Iteration 374, loss = 0.07421885\n",
      "Iteration 375, loss = 0.07414555\n",
      "Iteration 376, loss = 0.07406607\n",
      "Iteration 377, loss = 0.07398924\n",
      "Iteration 378, loss = 0.07391128\n",
      "Iteration 379, loss = 0.07383546\n",
      "Iteration 380, loss = 0.07375945\n",
      "Iteration 381, loss = 0.07368687\n",
      "Iteration 382, loss = 0.07361314\n",
      "Iteration 383, loss = 0.07354382\n",
      "Iteration 384, loss = 0.07346433\n",
      "Iteration 385, loss = 0.07339248\n",
      "Iteration 386, loss = 0.07331613\n",
      "Iteration 387, loss = 0.07324368\n",
      "Iteration 388, loss = 0.07317053\n",
      "Iteration 389, loss = 0.07309443\n",
      "Iteration 390, loss = 0.07301774\n",
      "Iteration 391, loss = 0.07294511\n",
      "Iteration 392, loss = 0.07287509\n",
      "Iteration 393, loss = 0.07280267\n",
      "Iteration 394, loss = 0.07273492\n",
      "Iteration 395, loss = 0.07265560\n",
      "Iteration 396, loss = 0.07259221\n",
      "Iteration 397, loss = 0.07251502\n",
      "Iteration 398, loss = 0.07243935\n",
      "Iteration 399, loss = 0.07237007\n",
      "Iteration 400, loss = 0.07229766\n",
      "Iteration 401, loss = 0.07222991\n",
      "Iteration 402, loss = 0.07214975\n",
      "Iteration 403, loss = 0.07208473\n",
      "Iteration 404, loss = 0.07200952\n",
      "Iteration 405, loss = 0.07195004\n",
      "Iteration 406, loss = 0.07186943\n",
      "Iteration 407, loss = 0.07180569\n",
      "Iteration 408, loss = 0.07173352\n",
      "Iteration 409, loss = 0.07166034\n",
      "Iteration 410, loss = 0.07159054\n",
      "Iteration 411, loss = 0.07151927\n",
      "Iteration 412, loss = 0.07145347\n",
      "Iteration 413, loss = 0.07138888\n",
      "Iteration 414, loss = 0.07132176\n",
      "Iteration 415, loss = 0.07124989\n",
      "Iteration 416, loss = 0.07118863\n",
      "Iteration 417, loss = 0.07111338\n",
      "Iteration 418, loss = 0.07103938\n",
      "Iteration 419, loss = 0.07097268\n",
      "Iteration 420, loss = 0.07091019\n",
      "Iteration 421, loss = 0.07084233\n",
      "Iteration 422, loss = 0.07077372\n",
      "Iteration 423, loss = 0.07070607\n",
      "Iteration 424, loss = 0.07063537\n",
      "Iteration 425, loss = 0.07057592\n",
      "Iteration 426, loss = 0.07050414\n",
      "Iteration 427, loss = 0.07044181\n",
      "Iteration 428, loss = 0.07037118\n",
      "Iteration 429, loss = 0.07031776\n",
      "Iteration 430, loss = 0.07024603\n",
      "Iteration 431, loss = 0.07017251\n",
      "Iteration 432, loss = 0.07011635\n",
      "Iteration 433, loss = 0.07004495\n",
      "Iteration 434, loss = 0.06998066\n",
      "Iteration 435, loss = 0.06991461\n",
      "Iteration 436, loss = 0.06984748\n",
      "Iteration 437, loss = 0.06978473\n",
      "Iteration 438, loss = 0.06972038\n",
      "Iteration 439, loss = 0.06966093\n",
      "Iteration 440, loss = 0.06959361\n",
      "Iteration 441, loss = 0.06952863\n",
      "Iteration 442, loss = 0.06946827\n",
      "Iteration 443, loss = 0.06940588\n",
      "Iteration 444, loss = 0.06934068\n",
      "Iteration 445, loss = 0.06927871\n",
      "Iteration 446, loss = 0.06921874\n",
      "Iteration 447, loss = 0.06915240\n",
      "Iteration 448, loss = 0.06910090\n",
      "Iteration 449, loss = 0.06903087\n",
      "Iteration 450, loss = 0.06896416\n",
      "Iteration 451, loss = 0.06890663\n",
      "Iteration 452, loss = 0.06884706\n",
      "Iteration 453, loss = 0.06878444\n",
      "Iteration 454, loss = 0.06872532\n",
      "Iteration 455, loss = 0.06866364\n",
      "Iteration 456, loss = 0.06860401\n",
      "Iteration 457, loss = 0.06854169\n",
      "Iteration 458, loss = 0.06848352\n",
      "Iteration 459, loss = 0.06841751\n",
      "Iteration 460, loss = 0.06836037\n",
      "Iteration 461, loss = 0.06830044\n",
      "Iteration 462, loss = 0.06824263\n",
      "Iteration 463, loss = 0.06817842\n",
      "Iteration 464, loss = 0.06812204\n",
      "Iteration 465, loss = 0.06806004\n",
      "Iteration 466, loss = 0.06800258\n",
      "Iteration 467, loss = 0.06794263\n",
      "Iteration 468, loss = 0.06788473\n",
      "Iteration 469, loss = 0.06783085\n",
      "Iteration 470, loss = 0.06778695\n",
      "Iteration 471, loss = 0.06771391\n",
      "Iteration 472, loss = 0.06767171\n",
      "Iteration 473, loss = 0.06759991\n",
      "Iteration 474, loss = 0.06754812\n",
      "Iteration 475, loss = 0.06749557\n",
      "Iteration 476, loss = 0.06743293\n",
      "Iteration 477, loss = 0.06738097\n",
      "Iteration 478, loss = 0.06732592\n",
      "Iteration 479, loss = 0.06726733\n",
      "Iteration 480, loss = 0.06721630\n",
      "Iteration 481, loss = 0.06716417\n",
      "Iteration 482, loss = 0.06710592\n",
      "Iteration 483, loss = 0.06704592\n",
      "Iteration 484, loss = 0.06699697\n",
      "Iteration 485, loss = 0.06694527\n",
      "Iteration 486, loss = 0.06689307\n",
      "Iteration 487, loss = 0.06683772\n",
      "Iteration 488, loss = 0.06678717\n",
      "Iteration 489, loss = 0.06672682\n",
      "Iteration 490, loss = 0.06667545\n",
      "Iteration 491, loss = 0.06662265\n",
      "Iteration 492, loss = 0.06657080\n",
      "Iteration 493, loss = 0.06651983\n",
      "Iteration 494, loss = 0.06646333\n",
      "Iteration 495, loss = 0.06641473\n",
      "Iteration 496, loss = 0.06635848\n",
      "Iteration 497, loss = 0.06631091\n",
      "Iteration 498, loss = 0.06625403\n",
      "Iteration 499, loss = 0.06620100\n",
      "Iteration 500, loss = 0.06614437\n",
      "Iteration 501, loss = 0.06609346\n",
      "Iteration 502, loss = 0.06604700\n",
      "Iteration 503, loss = 0.06599147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 504, loss = 0.06594843\n",
      "Iteration 505, loss = 0.06589749\n",
      "Iteration 506, loss = 0.06583851\n",
      "Iteration 507, loss = 0.06578859\n",
      "Iteration 508, loss = 0.06573537\n",
      "Iteration 509, loss = 0.06568794\n",
      "Iteration 510, loss = 0.06563488\n",
      "Iteration 511, loss = 0.06558918\n",
      "Iteration 512, loss = 0.06553692\n",
      "Iteration 513, loss = 0.06549114\n",
      "Iteration 514, loss = 0.06543246\n",
      "Iteration 515, loss = 0.06538563\n",
      "Iteration 516, loss = 0.06533196\n",
      "Iteration 517, loss = 0.06528612\n",
      "Iteration 518, loss = 0.06523305\n",
      "Iteration 519, loss = 0.06518834\n",
      "Iteration 520, loss = 0.06513535\n",
      "Iteration 521, loss = 0.06509188\n",
      "Iteration 522, loss = 0.06503341\n",
      "Iteration 523, loss = 0.06499552\n",
      "Iteration 524, loss = 0.06494095\n",
      "Iteration 525, loss = 0.06488953\n",
      "Iteration 526, loss = 0.06484319\n",
      "Iteration 527, loss = 0.06479074\n",
      "Iteration 528, loss = 0.06474367\n",
      "Iteration 529, loss = 0.06470438\n",
      "Iteration 530, loss = 0.06464853\n",
      "Iteration 531, loss = 0.06460551\n",
      "Iteration 532, loss = 0.06456160\n",
      "Iteration 533, loss = 0.06450479\n",
      "Iteration 534, loss = 0.06446300\n",
      "Iteration 535, loss = 0.06441286\n",
      "Iteration 536, loss = 0.06436180\n",
      "Iteration 537, loss = 0.06432143\n",
      "Iteration 538, loss = 0.06427182\n",
      "Iteration 539, loss = 0.06422814\n",
      "Iteration 540, loss = 0.06417577\n",
      "Iteration 541, loss = 0.06413138\n",
      "Iteration 542, loss = 0.06408947\n",
      "Iteration 543, loss = 0.06404303\n",
      "Iteration 544, loss = 0.06399365\n",
      "Iteration 545, loss = 0.06394647\n",
      "Iteration 546, loss = 0.06390029\n",
      "Iteration 547, loss = 0.06385734\n",
      "Iteration 548, loss = 0.06380847\n",
      "Iteration 549, loss = 0.06376099\n",
      "Iteration 550, loss = 0.06372302\n",
      "Iteration 551, loss = 0.06367207\n",
      "Iteration 552, loss = 0.06362499\n",
      "Iteration 553, loss = 0.06358404\n",
      "Iteration 554, loss = 0.06353780\n",
      "Iteration 555, loss = 0.06349052\n",
      "Iteration 556, loss = 0.06344749\n",
      "Iteration 557, loss = 0.06340164\n",
      "Iteration 558, loss = 0.06335972\n",
      "Iteration 559, loss = 0.06331400\n",
      "Iteration 560, loss = 0.06327206\n",
      "Iteration 561, loss = 0.06322958\n",
      "Iteration 562, loss = 0.06319495\n",
      "Iteration 563, loss = 0.06314202\n",
      "Iteration 564, loss = 0.06309964\n",
      "Iteration 565, loss = 0.06305688\n",
      "Iteration 566, loss = 0.06301514\n",
      "Iteration 567, loss = 0.06297472\n",
      "Iteration 568, loss = 0.06293420\n",
      "Iteration 569, loss = 0.06288762\n",
      "Iteration 570, loss = 0.06284211\n",
      "Iteration 571, loss = 0.06280149\n",
      "Iteration 572, loss = 0.06275032\n",
      "Iteration 573, loss = 0.06270769\n",
      "Iteration 574, loss = 0.06266680\n",
      "Iteration 575, loss = 0.06263677\n",
      "Iteration 576, loss = 0.06257949\n",
      "Iteration 577, loss = 0.06254410\n",
      "Iteration 578, loss = 0.06249728\n",
      "Iteration 579, loss = 0.06245289\n",
      "Iteration 580, loss = 0.06241160\n",
      "Iteration 581, loss = 0.06237311\n",
      "Iteration 582, loss = 0.06232468\n",
      "Iteration 583, loss = 0.06228887\n",
      "Iteration 584, loss = 0.06225340\n",
      "Iteration 585, loss = 0.06221016\n",
      "Iteration 586, loss = 0.06217025\n",
      "Iteration 587, loss = 0.06211515\n",
      "Iteration 588, loss = 0.06207964\n",
      "Iteration 589, loss = 0.06202908\n",
      "Iteration 590, loss = 0.06200421\n",
      "Iteration 591, loss = 0.06195859\n",
      "Iteration 592, loss = 0.06191539\n",
      "Iteration 593, loss = 0.06187544\n",
      "Iteration 594, loss = 0.06183556\n",
      "Iteration 595, loss = 0.06178877\n",
      "Iteration 596, loss = 0.06175196\n",
      "Iteration 597, loss = 0.06171555\n",
      "Iteration 598, loss = 0.06166898\n",
      "Iteration 599, loss = 0.06162938\n",
      "Iteration 600, loss = 0.06160019\n",
      "Iteration 601, loss = 0.06154921\n",
      "Iteration 602, loss = 0.06150967\n",
      "Iteration 603, loss = 0.06147089\n",
      "Iteration 604, loss = 0.06143149\n",
      "Iteration 605, loss = 0.06140040\n",
      "Iteration 606, loss = 0.06134263\n",
      "Iteration 607, loss = 0.06130457\n",
      "Iteration 608, loss = 0.06126993\n",
      "Iteration 609, loss = 0.06123306\n",
      "Iteration 610, loss = 0.06119192\n",
      "Iteration 611, loss = 0.06115082\n",
      "Iteration 612, loss = 0.06110835\n",
      "Iteration 613, loss = 0.06107113\n",
      "Iteration 614, loss = 0.06103198\n",
      "Iteration 615, loss = 0.06099597\n",
      "Iteration 616, loss = 0.06095761\n",
      "Iteration 617, loss = 0.06091171\n",
      "Iteration 618, loss = 0.06087810\n",
      "Iteration 619, loss = 0.06083495\n",
      "Iteration 620, loss = 0.06080775\n",
      "Iteration 621, loss = 0.06075907\n",
      "Iteration 622, loss = 0.06072454\n",
      "Iteration 623, loss = 0.06067519\n",
      "Iteration 624, loss = 0.06064423\n",
      "Iteration 625, loss = 0.06060783\n",
      "Iteration 626, loss = 0.06056684\n",
      "Iteration 627, loss = 0.06052859\n",
      "Iteration 628, loss = 0.06049160\n",
      "Iteration 629, loss = 0.06045104\n",
      "Iteration 630, loss = 0.06042777\n",
      "Iteration 631, loss = 0.06038935\n",
      "Iteration 632, loss = 0.06033585\n",
      "Iteration 633, loss = 0.06030255\n",
      "Iteration 634, loss = 0.06026379\n",
      "Iteration 635, loss = 0.06022260\n",
      "Iteration 636, loss = 0.06019180\n",
      "Iteration 637, loss = 0.06014874\n",
      "Iteration 638, loss = 0.06011260\n",
      "Iteration 639, loss = 0.06007270\n",
      "Iteration 640, loss = 0.06004025\n",
      "Iteration 641, loss = 0.05999869\n",
      "Iteration 642, loss = 0.05996946\n",
      "Iteration 643, loss = 0.05992872\n",
      "Iteration 644, loss = 0.05989781\n",
      "Iteration 645, loss = 0.05985592\n",
      "Iteration 646, loss = 0.05981985\n",
      "Iteration 647, loss = 0.05978141\n",
      "Iteration 648, loss = 0.05974510\n",
      "Iteration 649, loss = 0.05971656\n",
      "Iteration 650, loss = 0.05967323\n",
      "Iteration 651, loss = 0.05963459\n",
      "Iteration 652, loss = 0.05961026\n",
      "Iteration 653, loss = 0.05955911\n",
      "Iteration 654, loss = 0.05953042\n",
      "Iteration 655, loss = 0.05948931\n",
      "Iteration 656, loss = 0.05945551\n",
      "Iteration 657, loss = 0.05941674\n",
      "Iteration 658, loss = 0.05938700\n",
      "Iteration 659, loss = 0.05934865\n",
      "Iteration 660, loss = 0.05931591\n",
      "Iteration 661, loss = 0.05927400\n",
      "Iteration 662, loss = 0.05923992\n",
      "Iteration 663, loss = 0.05920779\n",
      "Iteration 664, loss = 0.05917287\n",
      "Iteration 665, loss = 0.05913384\n",
      "Iteration 666, loss = 0.05909650\n",
      "Iteration 667, loss = 0.05906196\n",
      "Iteration 668, loss = 0.05902823\n",
      "Iteration 669, loss = 0.05899590\n",
      "Iteration 670, loss = 0.05895965\n",
      "Iteration 671, loss = 0.05892265\n",
      "Iteration 672, loss = 0.05888644\n",
      "Iteration 673, loss = 0.05885758\n",
      "Iteration 674, loss = 0.05882796\n",
      "Iteration 675, loss = 0.05879444\n",
      "Iteration 676, loss = 0.05874883\n",
      "Iteration 677, loss = 0.05871055\n",
      "Iteration 678, loss = 0.05867868\n",
      "Iteration 679, loss = 0.05864319\n",
      "Iteration 680, loss = 0.05861261\n",
      "Iteration 681, loss = 0.05857421\n",
      "Iteration 682, loss = 0.05854017\n",
      "Iteration 683, loss = 0.05851903\n",
      "Iteration 684, loss = 0.05847300\n",
      "Iteration 685, loss = 0.05843751\n",
      "Iteration 686, loss = 0.05840776\n",
      "Iteration 687, loss = 0.05837092\n",
      "Iteration 688, loss = 0.05834563\n",
      "Iteration 689, loss = 0.05830546\n",
      "Iteration 690, loss = 0.05827462\n",
      "Iteration 691, loss = 0.05824180\n",
      "Iteration 692, loss = 0.05821186\n",
      "Iteration 693, loss = 0.05818253\n",
      "Iteration 694, loss = 0.05814374\n",
      "Iteration 695, loss = 0.05810789\n",
      "Iteration 696, loss = 0.05808121\n",
      "Iteration 697, loss = 0.05804452\n",
      "Iteration 698, loss = 0.05801142\n",
      "Iteration 699, loss = 0.05797799\n",
      "Iteration 700, loss = 0.05795192\n",
      "Iteration 701, loss = 0.05791758\n",
      "Iteration 702, loss = 0.05789876\n",
      "Iteration 703, loss = 0.05784469\n",
      "Iteration 704, loss = 0.05781987\n",
      "Iteration 705, loss = 0.05778170\n",
      "Iteration 706, loss = 0.05775787\n",
      "Iteration 707, loss = 0.05772118\n",
      "Iteration 708, loss = 0.05769244\n",
      "Iteration 709, loss = 0.05765971\n",
      "Iteration 710, loss = 0.05762826\n",
      "Iteration 711, loss = 0.05758727\n",
      "Iteration 712, loss = 0.05756428\n",
      "Iteration 713, loss = 0.05752851\n",
      "Iteration 714, loss = 0.05749359\n",
      "Iteration 715, loss = 0.05746476\n",
      "Iteration 716, loss = 0.05743293\n",
      "Iteration 717, loss = 0.05739819\n",
      "Iteration 718, loss = 0.05736252\n",
      "Iteration 719, loss = 0.05733455\n",
      "Iteration 720, loss = 0.05729985\n",
      "Iteration 721, loss = 0.05726883\n",
      "Iteration 722, loss = 0.05724115\n",
      "Iteration 723, loss = 0.05721288\n",
      "Iteration 724, loss = 0.05717947\n",
      "Iteration 725, loss = 0.05713944\n",
      "Iteration 726, loss = 0.05711674\n",
      "Iteration 727, loss = 0.05708357\n",
      "Iteration 728, loss = 0.05705209\n",
      "Iteration 729, loss = 0.05702226\n",
      "Iteration 730, loss = 0.05698637\n",
      "Iteration 731, loss = 0.05695844\n",
      "Iteration 732, loss = 0.05692709\n",
      "Iteration 733, loss = 0.05690201\n",
      "Iteration 734, loss = 0.05686538\n",
      "Iteration 735, loss = 0.05683264\n",
      "Iteration 736, loss = 0.05680694\n",
      "Iteration 737, loss = 0.05677321\n",
      "Iteration 738, loss = 0.05673454\n",
      "Iteration 739, loss = 0.05671083\n",
      "Iteration 740, loss = 0.05667964\n",
      "Iteration 741, loss = 0.05665632\n",
      "Iteration 742, loss = 0.05661859\n",
      "Iteration 743, loss = 0.05658729\n",
      "Iteration 744, loss = 0.05655524\n",
      "Iteration 745, loss = 0.05653024\n",
      "Iteration 746, loss = 0.05650559\n",
      "Iteration 747, loss = 0.05646373\n",
      "Iteration 748, loss = 0.05643623\n",
      "Iteration 749, loss = 0.05640535\n",
      "Iteration 750, loss = 0.05637693\n",
      "Iteration 751, loss = 0.05635333\n",
      "Iteration 752, loss = 0.05632019\n",
      "Iteration 753, loss = 0.05628531\n",
      "Iteration 754, loss = 0.05625040\n",
      "Iteration 755, loss = 0.05623701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 756, loss = 0.05620137\n",
      "Iteration 757, loss = 0.05617021\n",
      "Iteration 758, loss = 0.05613368\n",
      "Iteration 759, loss = 0.05610589\n",
      "Iteration 760, loss = 0.05607586\n",
      "Iteration 761, loss = 0.05604802\n",
      "Iteration 762, loss = 0.05602053\n",
      "Iteration 763, loss = 0.05599549\n",
      "Iteration 764, loss = 0.05596030\n",
      "Iteration 765, loss = 0.05592872\n",
      "Iteration 766, loss = 0.05590461\n",
      "Iteration 767, loss = 0.05587477\n",
      "Iteration 768, loss = 0.05584256\n",
      "Iteration 769, loss = 0.05581509\n",
      "Iteration 770, loss = 0.05578303\n",
      "Iteration 771, loss = 0.05575640\n",
      "Iteration 772, loss = 0.05572220\n",
      "Iteration 773, loss = 0.05569653\n",
      "Iteration 774, loss = 0.05566939\n",
      "Iteration 775, loss = 0.05563971\n",
      "Iteration 776, loss = 0.05561437\n",
      "Iteration 777, loss = 0.05557967\n",
      "Iteration 778, loss = 0.05555443\n",
      "Iteration 779, loss = 0.05552834\n",
      "Iteration 780, loss = 0.05549901\n",
      "Iteration 781, loss = 0.05546536\n",
      "Iteration 782, loss = 0.05544085\n",
      "Iteration 783, loss = 0.05541149\n",
      "Iteration 784, loss = 0.05538901\n",
      "Iteration 785, loss = 0.05535171\n",
      "Iteration 786, loss = 0.05532015\n",
      "Iteration 787, loss = 0.05530005\n",
      "Iteration 788, loss = 0.05526534\n",
      "Iteration 789, loss = 0.05523098\n",
      "Iteration 790, loss = 0.05520755\n",
      "Iteration 791, loss = 0.05518255\n",
      "Iteration 792, loss = 0.05514714\n",
      "Iteration 793, loss = 0.05511802\n",
      "Iteration 794, loss = 0.05509841\n",
      "Iteration 795, loss = 0.05506032\n",
      "Iteration 796, loss = 0.05503511\n",
      "Iteration 797, loss = 0.05501025\n",
      "Iteration 798, loss = 0.05497822\n",
      "Iteration 799, loss = 0.05494892\n",
      "Iteration 800, loss = 0.05491816\n",
      "Iteration 801, loss = 0.05489625\n",
      "Iteration 802, loss = 0.05486264\n",
      "Iteration 803, loss = 0.05484254\n",
      "Iteration 804, loss = 0.05481695\n",
      "Iteration 805, loss = 0.05478321\n",
      "Iteration 806, loss = 0.05476301\n",
      "Iteration 807, loss = 0.05472506\n",
      "Iteration 808, loss = 0.05470041\n",
      "Iteration 809, loss = 0.05467735\n",
      "Iteration 810, loss = 0.05464524\n",
      "Iteration 811, loss = 0.05462219\n",
      "Iteration 812, loss = 0.05458585\n",
      "Iteration 813, loss = 0.05456377\n",
      "Iteration 814, loss = 0.05454523\n",
      "Iteration 815, loss = 0.05450869\n",
      "Iteration 816, loss = 0.05448113\n",
      "Iteration 817, loss = 0.05446199\n",
      "Iteration 818, loss = 0.05442338\n",
      "Iteration 819, loss = 0.05439939\n",
      "Iteration 820, loss = 0.05437410\n",
      "Iteration 821, loss = 0.05434746\n",
      "Iteration 822, loss = 0.05431859\n",
      "Iteration 823, loss = 0.05429869\n",
      "Iteration 824, loss = 0.05426766\n",
      "Iteration 825, loss = 0.05423960\n",
      "Iteration 826, loss = 0.05421114\n",
      "Iteration 827, loss = 0.05418633\n",
      "Iteration 828, loss = 0.05416181\n",
      "Iteration 829, loss = 0.05413194\n",
      "Iteration 830, loss = 0.05410602\n",
      "Iteration 831, loss = 0.05408580\n",
      "Iteration 832, loss = 0.05405761\n",
      "Iteration 833, loss = 0.05403406\n",
      "Iteration 834, loss = 0.05400531\n",
      "Iteration 835, loss = 0.05397822\n",
      "Iteration 836, loss = 0.05395153\n",
      "Iteration 837, loss = 0.05392665\n",
      "Iteration 838, loss = 0.05390596\n",
      "Iteration 839, loss = 0.05387331\n",
      "Iteration 840, loss = 0.05385168\n",
      "Iteration 841, loss = 0.05382700\n",
      "Iteration 842, loss = 0.05380509\n",
      "Iteration 843, loss = 0.05377423\n",
      "Iteration 844, loss = 0.05375538\n",
      "Iteration 845, loss = 0.05372224\n",
      "Iteration 846, loss = 0.05369205\n",
      "Iteration 847, loss = 0.05367109\n",
      "Iteration 848, loss = 0.05364047\n",
      "Iteration 849, loss = 0.05361824\n",
      "Iteration 850, loss = 0.05359254\n",
      "Iteration 851, loss = 0.05357354\n",
      "Iteration 852, loss = 0.05353948\n",
      "Iteration 853, loss = 0.05351524\n",
      "Iteration 854, loss = 0.05349711\n",
      "Iteration 855, loss = 0.05346319\n",
      "Iteration 856, loss = 0.05343875\n",
      "Iteration 857, loss = 0.05342174\n",
      "Iteration 858, loss = 0.05339788\n",
      "Iteration 859, loss = 0.05336322\n",
      "Iteration 860, loss = 0.05333833\n",
      "Iteration 861, loss = 0.05331967\n",
      "Iteration 862, loss = 0.05329392\n",
      "Iteration 863, loss = 0.05326484\n",
      "Iteration 864, loss = 0.05324112\n",
      "Iteration 865, loss = 0.05321040\n",
      "Iteration 866, loss = 0.05318597\n",
      "Iteration 867, loss = 0.05316882\n",
      "Iteration 868, loss = 0.05313626\n",
      "Iteration 869, loss = 0.05311624\n",
      "Iteration 870, loss = 0.05308921\n",
      "Iteration 871, loss = 0.05306258\n",
      "Iteration 872, loss = 0.05303477\n",
      "Iteration 873, loss = 0.05301453\n",
      "Iteration 874, loss = 0.05299021\n",
      "Iteration 875, loss = 0.05296077\n",
      "Iteration 876, loss = 0.05293751\n",
      "Iteration 877, loss = 0.05291487\n",
      "Iteration 878, loss = 0.05289035\n",
      "Iteration 879, loss = 0.05286531\n",
      "Iteration 880, loss = 0.05284131\n",
      "Iteration 881, loss = 0.05282174\n",
      "Iteration 882, loss = 0.05279352\n",
      "Iteration 883, loss = 0.05276970\n",
      "Iteration 884, loss = 0.05274616\n",
      "Iteration 885, loss = 0.05272236\n",
      "Iteration 886, loss = 0.05269577\n",
      "Iteration 887, loss = 0.05267174\n",
      "Iteration 888, loss = 0.05264686\n",
      "Iteration 889, loss = 0.05262551\n",
      "Iteration 890, loss = 0.05260659\n",
      "Iteration 891, loss = 0.05257948\n",
      "Iteration 892, loss = 0.05255354\n",
      "Iteration 893, loss = 0.05253634\n",
      "Iteration 894, loss = 0.05251220\n",
      "Iteration 895, loss = 0.05248896\n",
      "Iteration 896, loss = 0.05246191\n",
      "Iteration 897, loss = 0.05243611\n",
      "Iteration 898, loss = 0.05241294\n",
      "Iteration 899, loss = 0.05239451\n",
      "Iteration 900, loss = 0.05236456\n",
      "Iteration 901, loss = 0.05234515\n",
      "Iteration 902, loss = 0.05232858\n",
      "Iteration 903, loss = 0.05230390\n",
      "Iteration 904, loss = 0.05227570\n",
      "Iteration 905, loss = 0.05225473\n",
      "Iteration 906, loss = 0.05223718\n",
      "Iteration 907, loss = 0.05221418\n",
      "Iteration 908, loss = 0.05218225\n",
      "Iteration 909, loss = 0.05215857\n",
      "Iteration 910, loss = 0.05213867\n",
      "Iteration 911, loss = 0.05211327\n",
      "Iteration 912, loss = 0.05208653\n",
      "Iteration 913, loss = 0.05208168\n",
      "Iteration 914, loss = 0.05204669\n",
      "Iteration 915, loss = 0.05203043\n",
      "Iteration 916, loss = 0.05200305\n",
      "Iteration 917, loss = 0.05199132\n",
      "Iteration 918, loss = 0.05195226\n",
      "Iteration 919, loss = 0.05192777\n",
      "Iteration 920, loss = 0.05191387\n",
      "Iteration 921, loss = 0.05188385\n",
      "Iteration 922, loss = 0.05186442\n",
      "Iteration 923, loss = 0.05183862\n",
      "Iteration 924, loss = 0.05183009\n",
      "Iteration 925, loss = 0.05179028\n",
      "Iteration 926, loss = 0.05177047\n",
      "Iteration 927, loss = 0.05174578\n",
      "Iteration 928, loss = 0.05172935\n",
      "Iteration 929, loss = 0.05170365\n",
      "Iteration 930, loss = 0.05167814\n",
      "Iteration 931, loss = 0.05165599\n",
      "Iteration 932, loss = 0.05163531\n",
      "Iteration 933, loss = 0.05161223\n",
      "Iteration 934, loss = 0.05158885\n",
      "Iteration 935, loss = 0.05156424\n",
      "Iteration 936, loss = 0.05155238\n",
      "Iteration 937, loss = 0.05152242\n",
      "Iteration 938, loss = 0.05149768\n",
      "Iteration 939, loss = 0.05148072\n",
      "Iteration 940, loss = 0.05146377\n",
      "Iteration 941, loss = 0.05143809\n",
      "Iteration 942, loss = 0.05141537\n",
      "Iteration 943, loss = 0.05139360\n",
      "Iteration 944, loss = 0.05136732\n",
      "Iteration 945, loss = 0.05134619\n",
      "Iteration 946, loss = 0.05132524\n",
      "Iteration 947, loss = 0.05129930\n",
      "Iteration 948, loss = 0.05128834\n",
      "Iteration 949, loss = 0.05125736\n",
      "Iteration 950, loss = 0.05123450\n",
      "Iteration 951, loss = 0.05122237\n",
      "Iteration 952, loss = 0.05119321\n",
      "Iteration 953, loss = 0.05117133\n",
      "Iteration 954, loss = 0.05115763\n",
      "Iteration 955, loss = 0.05112936\n",
      "Iteration 956, loss = 0.05110764\n",
      "Iteration 957, loss = 0.05109384\n",
      "Iteration 958, loss = 0.05106460\n",
      "Iteration 959, loss = 0.05104921\n",
      "Iteration 960, loss = 0.05102876\n",
      "Iteration 961, loss = 0.05099969\n",
      "Iteration 962, loss = 0.05097886\n",
      "Iteration 963, loss = 0.05095893\n",
      "Iteration 964, loss = 0.05093985\n",
      "Iteration 965, loss = 0.05091478\n",
      "Iteration 966, loss = 0.05089744\n",
      "Iteration 967, loss = 0.05087414\n",
      "Iteration 968, loss = 0.05085114\n",
      "Iteration 969, loss = 0.05083234\n",
      "Iteration 970, loss = 0.05080706\n",
      "Iteration 971, loss = 0.05078403\n",
      "Iteration 972, loss = 0.05076465\n",
      "Iteration 973, loss = 0.05074615\n",
      "Iteration 974, loss = 0.05072746\n",
      "Iteration 975, loss = 0.05070099\n",
      "Iteration 976, loss = 0.05068312\n",
      "Iteration 977, loss = 0.05066353\n",
      "Iteration 978, loss = 0.05064389\n",
      "Iteration 979, loss = 0.05062080\n",
      "Iteration 980, loss = 0.05060492\n",
      "Iteration 981, loss = 0.05058481\n",
      "Iteration 982, loss = 0.05056122\n",
      "Iteration 983, loss = 0.05054251\n",
      "Iteration 984, loss = 0.05052460\n",
      "Iteration 985, loss = 0.05049781\n",
      "Iteration 986, loss = 0.05047825\n",
      "Iteration 987, loss = 0.05046609\n",
      "Iteration 988, loss = 0.05044091\n",
      "Iteration 989, loss = 0.05041911\n",
      "Iteration 990, loss = 0.05040128\n",
      "Iteration 991, loss = 0.05038620\n",
      "Iteration 992, loss = 0.05036000\n",
      "Iteration 993, loss = 0.05033988\n",
      "Iteration 994, loss = 0.05031580\n",
      "Iteration 995, loss = 0.05030383\n",
      "Iteration 996, loss = 0.05028430\n",
      "Iteration 997, loss = 0.05026000\n",
      "Iteration 998, loss = 0.05023843\n",
      "Iteration 999, loss = 0.05021510\n",
      "Iteration 1000, loss = 0.05020205\n",
      "Iteration 1001, loss = 0.05017654\n",
      "Iteration 1002, loss = 0.05015984\n",
      "Iteration 1003, loss = 0.05014384\n",
      "Iteration 1004, loss = 0.05012317\n",
      "Iteration 1005, loss = 0.05009739\n",
      "Iteration 1006, loss = 0.05007761\n",
      "Iteration 1007, loss = 0.05005632\n",
      "Iteration 1008, loss = 0.05004237\n",
      "Iteration 1009, loss = 0.05002047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1010, loss = 0.04999984\n",
      "Iteration 1011, loss = 0.04997959\n",
      "Iteration 1012, loss = 0.04996804\n",
      "Iteration 1013, loss = 0.04993875\n",
      "Iteration 1014, loss = 0.04993222\n",
      "Iteration 1015, loss = 0.04989939\n",
      "Iteration 1016, loss = 0.04988530\n",
      "Iteration 1017, loss = 0.04986460\n",
      "Iteration 1018, loss = 0.04984617\n",
      "Iteration 1019, loss = 0.04983016\n",
      "Iteration 1020, loss = 0.04980775\n",
      "Iteration 1021, loss = 0.04978828\n",
      "Iteration 1022, loss = 0.04976534\n",
      "Iteration 1023, loss = 0.04974833\n",
      "Iteration 1024, loss = 0.04973184\n",
      "Iteration 1025, loss = 0.04970944\n",
      "Iteration 1026, loss = 0.04969131\n",
      "Iteration 1027, loss = 0.04967643\n",
      "Iteration 1028, loss = 0.04965059\n",
      "Iteration 1029, loss = 0.04963418\n",
      "Iteration 1030, loss = 0.04961929\n",
      "Iteration 1031, loss = 0.04959708\n",
      "Iteration 1032, loss = 0.04957221\n",
      "Iteration 1033, loss = 0.04955408\n",
      "Iteration 1034, loss = 0.04953376\n",
      "Iteration 1035, loss = 0.04951434\n",
      "Iteration 1036, loss = 0.04950378\n",
      "Iteration 1037, loss = 0.04948125\n",
      "Iteration 1038, loss = 0.04945923\n",
      "Iteration 1039, loss = 0.04944275\n",
      "Iteration 1040, loss = 0.04942538\n",
      "Iteration 1041, loss = 0.04939953\n",
      "Iteration 1042, loss = 0.04938369\n",
      "Iteration 1043, loss = 0.04936581\n",
      "Iteration 1044, loss = 0.04934675\n",
      "Iteration 1045, loss = 0.04932678\n",
      "Iteration 1046, loss = 0.04931143\n",
      "Iteration 1047, loss = 0.04928739\n",
      "Iteration 1048, loss = 0.04927638\n",
      "Iteration 1049, loss = 0.04926431\n",
      "Iteration 1050, loss = 0.04924283\n",
      "Iteration 1051, loss = 0.04921715\n",
      "Iteration 1052, loss = 0.04920512\n",
      "Iteration 1053, loss = 0.04917999\n",
      "Iteration 1054, loss = 0.04916191\n",
      "Iteration 1055, loss = 0.04915431\n",
      "Iteration 1056, loss = 0.04912708\n",
      "Iteration 1057, loss = 0.04910825\n",
      "Iteration 1058, loss = 0.04909225\n",
      "Iteration 1059, loss = 0.04907888\n",
      "Iteration 1060, loss = 0.04905864\n",
      "Iteration 1061, loss = 0.04903920\n",
      "Iteration 1062, loss = 0.04901751\n",
      "Iteration 1063, loss = 0.04900102\n",
      "Iteration 1064, loss = 0.04898678\n",
      "Iteration 1065, loss = 0.04896479\n",
      "Iteration 1066, loss = 0.04895195\n",
      "Iteration 1067, loss = 0.04893373\n",
      "Iteration 1068, loss = 0.04890861\n",
      "Iteration 1069, loss = 0.04889538\n",
      "Iteration 1070, loss = 0.04887501\n",
      "Iteration 1071, loss = 0.04886574\n",
      "Iteration 1072, loss = 0.04884101\n",
      "Iteration 1073, loss = 0.04882402\n",
      "Iteration 1074, loss = 0.04880669\n",
      "Iteration 1075, loss = 0.04879068\n",
      "Iteration 1076, loss = 0.04877113\n",
      "Iteration 1077, loss = 0.04875033\n",
      "Iteration 1078, loss = 0.04873523\n",
      "Iteration 1079, loss = 0.04871590\n",
      "Iteration 1080, loss = 0.04870749\n",
      "Iteration 1081, loss = 0.04868059\n",
      "Iteration 1082, loss = 0.04866197\n",
      "Iteration 1083, loss = 0.04864736\n",
      "Iteration 1084, loss = 0.04862507\n",
      "Iteration 1085, loss = 0.04861643\n",
      "Iteration 1086, loss = 0.04859623\n",
      "Iteration 1087, loss = 0.04857497\n",
      "Iteration 1088, loss = 0.04855510\n",
      "Iteration 1089, loss = 0.04853850\n",
      "Iteration 1090, loss = 0.04852272\n",
      "Iteration 1091, loss = 0.04850557\n",
      "Iteration 1092, loss = 0.04848753\n",
      "Iteration 1093, loss = 0.04847395\n",
      "Iteration 1094, loss = 0.04845170\n",
      "Iteration 1095, loss = 0.04844182\n",
      "Iteration 1096, loss = 0.04842071\n",
      "Iteration 1097, loss = 0.04840891\n",
      "Iteration 1098, loss = 0.04838580\n",
      "Iteration 1099, loss = 0.04837698\n",
      "Iteration 1100, loss = 0.04835321\n",
      "Iteration 1101, loss = 0.04833056\n",
      "Iteration 1102, loss = 0.04831472\n",
      "Iteration 1103, loss = 0.04830223\n",
      "Iteration 1104, loss = 0.04827856\n",
      "Iteration 1105, loss = 0.04826589\n",
      "Iteration 1106, loss = 0.04825071\n",
      "Iteration 1107, loss = 0.04823047\n",
      "Iteration 1108, loss = 0.04820999\n",
      "Iteration 1109, loss = 0.04819248\n",
      "Iteration 1110, loss = 0.04817980\n",
      "Iteration 1111, loss = 0.04815950\n",
      "Iteration 1112, loss = 0.04814705\n",
      "Iteration 1113, loss = 0.04812682\n",
      "Iteration 1114, loss = 0.04811941\n",
      "Iteration 1115, loss = 0.04810094\n",
      "Iteration 1116, loss = 0.04808132\n",
      "Iteration 1117, loss = 0.04806256\n",
      "Iteration 1118, loss = 0.04804204\n",
      "Iteration 1119, loss = 0.04802471\n",
      "Iteration 1120, loss = 0.04801355\n",
      "Iteration 1121, loss = 0.04799489\n",
      "Iteration 1122, loss = 0.04797956\n",
      "Iteration 1123, loss = 0.04796165\n",
      "Iteration 1124, loss = 0.04794079\n",
      "Iteration 1125, loss = 0.04792826\n",
      "Iteration 1126, loss = 0.04790574\n",
      "Iteration 1127, loss = 0.04789337\n",
      "Iteration 1128, loss = 0.04787771\n",
      "Iteration 1129, loss = 0.04785908\n",
      "Iteration 1130, loss = 0.04784440\n",
      "Iteration 1131, loss = 0.04782744\n",
      "Iteration 1132, loss = 0.04780443\n",
      "Iteration 1133, loss = 0.04780121\n",
      "Iteration 1134, loss = 0.04777563\n",
      "Iteration 1135, loss = 0.04776158\n",
      "Iteration 1136, loss = 0.04774418\n",
      "Iteration 1137, loss = 0.04772322\n",
      "Iteration 1138, loss = 0.04771204\n",
      "Iteration 1139, loss = 0.04769648\n",
      "Iteration 1140, loss = 0.04768096\n",
      "Iteration 1141, loss = 0.04765747\n",
      "Iteration 1142, loss = 0.04764493\n",
      "Iteration 1143, loss = 0.04762440\n",
      "Iteration 1144, loss = 0.04761042\n",
      "Iteration 1145, loss = 0.04759803\n",
      "Iteration 1146, loss = 0.04757564\n",
      "Iteration 1147, loss = 0.04755896\n",
      "Iteration 1148, loss = 0.04755826\n",
      "Iteration 1149, loss = 0.04753762\n",
      "Iteration 1150, loss = 0.04751412\n",
      "Iteration 1151, loss = 0.04749703\n",
      "Iteration 1152, loss = 0.04748186\n",
      "Iteration 1153, loss = 0.04746094\n",
      "Iteration 1154, loss = 0.04744483\n",
      "Iteration 1155, loss = 0.04743258\n",
      "Iteration 1156, loss = 0.04741731\n",
      "Iteration 1157, loss = 0.04740333\n",
      "Iteration 1158, loss = 0.04738877\n",
      "Iteration 1159, loss = 0.04736786\n",
      "Iteration 1160, loss = 0.04734771\n",
      "Iteration 1161, loss = 0.04733751\n",
      "Iteration 1162, loss = 0.04732220\n",
      "Iteration 1163, loss = 0.04730265\n",
      "Iteration 1164, loss = 0.04728972\n",
      "Iteration 1165, loss = 0.04727213\n",
      "Iteration 1166, loss = 0.04725651\n",
      "Iteration 1167, loss = 0.04724260\n",
      "Iteration 1168, loss = 0.04722837\n",
      "Iteration 1169, loss = 0.04721292\n",
      "Iteration 1170, loss = 0.04719416\n",
      "Iteration 1171, loss = 0.04717693\n",
      "Iteration 1172, loss = 0.04716335\n",
      "Iteration 1173, loss = 0.04714985\n",
      "Iteration 1174, loss = 0.04713152\n",
      "Iteration 1175, loss = 0.04711890\n",
      "Iteration 1176, loss = 0.04710720\n",
      "Iteration 1177, loss = 0.04709062\n",
      "Iteration 1178, loss = 0.04706926\n",
      "Iteration 1179, loss = 0.04705095\n",
      "Iteration 1180, loss = 0.04703787\n",
      "Iteration 1181, loss = 0.04703265\n",
      "Iteration 1182, loss = 0.04700428\n",
      "Iteration 1183, loss = 0.04699132\n",
      "Iteration 1184, loss = 0.04698289\n",
      "Iteration 1185, loss = 0.04696643\n",
      "Iteration 1186, loss = 0.04695126\n",
      "Iteration 1187, loss = 0.04693065\n",
      "Iteration 1188, loss = 0.04691628\n",
      "Iteration 1189, loss = 0.04690373\n",
      "Iteration 1190, loss = 0.04688435\n",
      "Iteration 1191, loss = 0.04686537\n",
      "Iteration 1192, loss = 0.04685102\n",
      "Iteration 1193, loss = 0.04683489\n",
      "Iteration 1194, loss = 0.04682411\n",
      "Iteration 1195, loss = 0.04681483\n",
      "Iteration 1196, loss = 0.04679336\n",
      "Iteration 1197, loss = 0.04677316\n",
      "Iteration 1198, loss = 0.04676354\n",
      "Iteration 1199, loss = 0.04674572\n",
      "Iteration 1200, loss = 0.04673943\n",
      "Iteration 1201, loss = 0.04671689\n",
      "Iteration 1202, loss = 0.04669889\n",
      "Iteration 1203, loss = 0.04668533\n",
      "Iteration 1204, loss = 0.04666873\n",
      "Iteration 1205, loss = 0.04665117\n",
      "Iteration 1206, loss = 0.04664121\n",
      "Iteration 1207, loss = 0.04662581\n",
      "Iteration 1208, loss = 0.04661450\n",
      "Iteration 1209, loss = 0.04659351\n",
      "Iteration 1210, loss = 0.04657645\n",
      "Iteration 1211, loss = 0.04656533\n",
      "Iteration 1212, loss = 0.04655121\n",
      "Iteration 1213, loss = 0.04653759\n",
      "Iteration 1214, loss = 0.04652965\n",
      "Iteration 1215, loss = 0.04650575\n",
      "Iteration 1216, loss = 0.04649033\n",
      "Iteration 1217, loss = 0.04647088\n",
      "Iteration 1218, loss = 0.04646250\n",
      "Iteration 1219, loss = 0.04644542\n",
      "Iteration 1220, loss = 0.04642692\n",
      "Iteration 1221, loss = 0.04641149\n",
      "Iteration 1222, loss = 0.04639787\n",
      "Iteration 1223, loss = 0.04638591\n",
      "Iteration 1224, loss = 0.04637689\n",
      "Iteration 1225, loss = 0.04635327\n",
      "Iteration 1226, loss = 0.04633748\n",
      "Iteration 1227, loss = 0.04632477\n",
      "Iteration 1228, loss = 0.04631034\n",
      "Iteration 1229, loss = 0.04629947\n",
      "Iteration 1230, loss = 0.04628456\n",
      "Iteration 1231, loss = 0.04626677\n",
      "Iteration 1232, loss = 0.04625755\n",
      "Iteration 1233, loss = 0.04623584\n",
      "Iteration 1234, loss = 0.04622332\n",
      "Iteration 1235, loss = 0.04620597\n",
      "Iteration 1236, loss = 0.04618769\n",
      "Iteration 1237, loss = 0.04617311\n",
      "Iteration 1238, loss = 0.04616202\n",
      "Iteration 1239, loss = 0.04614756\n",
      "Iteration 1240, loss = 0.04612860\n",
      "Iteration 1241, loss = 0.04612017\n",
      "Iteration 1242, loss = 0.04610393\n",
      "Iteration 1243, loss = 0.04609417\n",
      "Iteration 1244, loss = 0.04606791\n",
      "Iteration 1245, loss = 0.04605997\n",
      "Iteration 1246, loss = 0.04604789\n",
      "Iteration 1247, loss = 0.04603065\n",
      "Iteration 1248, loss = 0.04601823\n",
      "Iteration 1249, loss = 0.04600195\n",
      "Iteration 1250, loss = 0.04598068\n",
      "Iteration 1251, loss = 0.04597217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1252, loss = 0.04595589\n",
      "Iteration 1253, loss = 0.04594159\n",
      "Iteration 1254, loss = 0.04592526\n",
      "Iteration 1255, loss = 0.04591200\n",
      "Iteration 1256, loss = 0.04590001\n",
      "Iteration 1257, loss = 0.04588076\n",
      "Iteration 1258, loss = 0.04587110\n",
      "Iteration 1259, loss = 0.04585118\n",
      "Iteration 1260, loss = 0.04584252\n",
      "Iteration 1261, loss = 0.04582268\n",
      "Iteration 1262, loss = 0.04581273\n",
      "Iteration 1263, loss = 0.04580310\n",
      "Iteration 1264, loss = 0.04578504\n",
      "Iteration 1265, loss = 0.04577342\n",
      "Iteration 1266, loss = 0.04575250\n",
      "Iteration 1267, loss = 0.04573596\n",
      "Iteration 1268, loss = 0.04572482\n",
      "Iteration 1269, loss = 0.04570722\n",
      "Iteration 1270, loss = 0.04569520\n",
      "Iteration 1271, loss = 0.04568900\n",
      "Iteration 1272, loss = 0.04566530\n",
      "Iteration 1273, loss = 0.04564728\n",
      "Iteration 1274, loss = 0.04564506\n",
      "Iteration 1275, loss = 0.04562179\n",
      "Iteration 1276, loss = 0.04561189\n",
      "Iteration 1277, loss = 0.04560114\n",
      "Iteration 1278, loss = 0.04558532\n",
      "Iteration 1279, loss = 0.04557379\n",
      "Iteration 1280, loss = 0.04555381\n",
      "Iteration 1281, loss = 0.04553645\n",
      "Iteration 1282, loss = 0.04552159\n",
      "Iteration 1283, loss = 0.04551300\n",
      "Iteration 1284, loss = 0.04549740\n",
      "Iteration 1285, loss = 0.04547954\n",
      "Iteration 1286, loss = 0.04546503\n",
      "Iteration 1287, loss = 0.04545613\n",
      "Iteration 1288, loss = 0.04543968\n",
      "Iteration 1289, loss = 0.04543436\n",
      "Iteration 1290, loss = 0.04542253\n",
      "Iteration 1291, loss = 0.04539897\n",
      "Iteration 1292, loss = 0.04538775\n",
      "Iteration 1293, loss = 0.04536854\n",
      "Iteration 1294, loss = 0.04536174\n",
      "Iteration 1295, loss = 0.04534553\n",
      "Iteration 1296, loss = 0.04532902\n",
      "Iteration 1297, loss = 0.04531874\n",
      "Iteration 1298, loss = 0.04530199\n",
      "Iteration 1299, loss = 0.04529358\n",
      "Iteration 1300, loss = 0.04527816\n",
      "Iteration 1301, loss = 0.04525968\n",
      "Iteration 1302, loss = 0.04524416\n",
      "Iteration 1303, loss = 0.04523976\n",
      "Iteration 1304, loss = 0.04521673\n",
      "Iteration 1305, loss = 0.04520517\n",
      "Iteration 1306, loss = 0.04519163\n",
      "Iteration 1307, loss = 0.04517365\n",
      "Iteration 1308, loss = 0.04516114\n",
      "Iteration 1309, loss = 0.04515004\n",
      "Iteration 1310, loss = 0.04513846\n",
      "Iteration 1311, loss = 0.04511983\n",
      "Iteration 1312, loss = 0.04510305\n",
      "Iteration 1313, loss = 0.04509185\n",
      "Iteration 1314, loss = 0.04508098\n",
      "Iteration 1315, loss = 0.04506244\n",
      "Iteration 1316, loss = 0.04506008\n",
      "Iteration 1317, loss = 0.04503320\n",
      "Iteration 1318, loss = 0.04502417\n",
      "Iteration 1319, loss = 0.04500930\n",
      "Iteration 1320, loss = 0.04499294\n",
      "Iteration 1321, loss = 0.04498132\n",
      "Iteration 1322, loss = 0.04496694\n",
      "Iteration 1323, loss = 0.04495412\n",
      "Iteration 1324, loss = 0.04494057\n",
      "Iteration 1325, loss = 0.04492696\n",
      "Iteration 1326, loss = 0.04491262\n",
      "Iteration 1327, loss = 0.04489844\n",
      "Iteration 1328, loss = 0.04488605\n",
      "Iteration 1329, loss = 0.04487504\n",
      "Iteration 1330, loss = 0.04486429\n",
      "Iteration 1331, loss = 0.04484539\n",
      "Iteration 1332, loss = 0.04483313\n",
      "Iteration 1333, loss = 0.04481515\n",
      "Iteration 1334, loss = 0.04480481\n",
      "Iteration 1335, loss = 0.04478883\n",
      "Iteration 1336, loss = 0.04478229\n",
      "Iteration 1337, loss = 0.04476705\n",
      "Iteration 1338, loss = 0.04476476\n",
      "Iteration 1339, loss = 0.04475165\n",
      "Iteration 1340, loss = 0.04472585\n",
      "Iteration 1341, loss = 0.04471541\n",
      "Iteration 1342, loss = 0.04470103\n",
      "Iteration 1343, loss = 0.04468294\n",
      "Iteration 1344, loss = 0.04467890\n",
      "Iteration 1345, loss = 0.04465910\n",
      "Iteration 1346, loss = 0.04464751\n",
      "Iteration 1347, loss = 0.04462979\n",
      "Iteration 1348, loss = 0.04461582\n",
      "Iteration 1349, loss = 0.04460630\n",
      "Iteration 1350, loss = 0.04459375\n",
      "Iteration 1351, loss = 0.04457816\n",
      "Iteration 1352, loss = 0.04457058\n",
      "Iteration 1353, loss = 0.04455312\n",
      "Iteration 1354, loss = 0.04454524\n",
      "Iteration 1355, loss = 0.04453326\n",
      "Iteration 1356, loss = 0.04451642\n",
      "Iteration 1357, loss = 0.04450709\n",
      "Iteration 1358, loss = 0.04449310\n",
      "Iteration 1359, loss = 0.04447665\n",
      "Iteration 1360, loss = 0.04445968\n",
      "Iteration 1361, loss = 0.04444804\n",
      "Iteration 1362, loss = 0.04443421\n",
      "Iteration 1363, loss = 0.04442131\n",
      "Iteration 1364, loss = 0.04441496\n",
      "Iteration 1365, loss = 0.04439978\n",
      "Iteration 1366, loss = 0.04438295\n",
      "Iteration 1367, loss = 0.04438072\n",
      "Iteration 1368, loss = 0.04436196\n",
      "Iteration 1369, loss = 0.04434644\n",
      "Iteration 1370, loss = 0.04433210\n",
      "Iteration 1371, loss = 0.04431755\n",
      "Iteration 1372, loss = 0.04430827\n",
      "Iteration 1373, loss = 0.04429317\n",
      "Iteration 1374, loss = 0.04427970\n",
      "Iteration 1375, loss = 0.04426409\n",
      "Iteration 1376, loss = 0.04425169\n",
      "Iteration 1377, loss = 0.04424324\n",
      "Iteration 1378, loss = 0.04422738\n",
      "Iteration 1379, loss = 0.04421745\n",
      "Iteration 1380, loss = 0.04420261\n",
      "Iteration 1381, loss = 0.04418754\n",
      "Iteration 1382, loss = 0.04417619\n",
      "Iteration 1383, loss = 0.04416305\n",
      "Iteration 1384, loss = 0.04415105\n",
      "Iteration 1385, loss = 0.04413748\n",
      "Iteration 1386, loss = 0.04411819\n",
      "Iteration 1387, loss = 0.04410973\n",
      "Iteration 1388, loss = 0.04409558\n",
      "Iteration 1389, loss = 0.04408498\n",
      "Iteration 1390, loss = 0.04406995\n",
      "Iteration 1391, loss = 0.04405930\n",
      "Iteration 1392, loss = 0.04404350\n",
      "Iteration 1393, loss = 0.04403167\n",
      "Iteration 1394, loss = 0.04402343\n",
      "Iteration 1395, loss = 0.04400915\n",
      "Iteration 1396, loss = 0.04399287\n",
      "Iteration 1397, loss = 0.04398149\n",
      "Iteration 1398, loss = 0.04396628\n",
      "Iteration 1399, loss = 0.04395751\n",
      "Iteration 1400, loss = 0.04394231\n",
      "Iteration 1401, loss = 0.04393054\n",
      "Iteration 1402, loss = 0.04391726\n",
      "Iteration 1403, loss = 0.04390234\n",
      "Iteration 1404, loss = 0.04389584\n",
      "Iteration 1405, loss = 0.04388367\n",
      "Iteration 1406, loss = 0.04386444\n",
      "Iteration 1407, loss = 0.04386558\n",
      "Iteration 1408, loss = 0.04384139\n",
      "Iteration 1409, loss = 0.04383382\n",
      "Iteration 1410, loss = 0.04382370\n",
      "Iteration 1411, loss = 0.04380449\n",
      "Iteration 1412, loss = 0.04378770\n",
      "Iteration 1413, loss = 0.04377751\n",
      "Iteration 1414, loss = 0.04377136\n",
      "Iteration 1415, loss = 0.04375278\n",
      "Iteration 1416, loss = 0.04374090\n",
      "Iteration 1417, loss = 0.04373027\n",
      "Iteration 1418, loss = 0.04371872\n",
      "Iteration 1419, loss = 0.04370517\n",
      "Iteration 1420, loss = 0.04369642\n",
      "Iteration 1421, loss = 0.04367833\n",
      "Iteration 1422, loss = 0.04366942\n",
      "Iteration 1423, loss = 0.04366112\n",
      "Iteration 1424, loss = 0.04364361\n",
      "Iteration 1425, loss = 0.04363528\n",
      "Iteration 1426, loss = 0.04362568\n",
      "Iteration 1427, loss = 0.04360572\n",
      "Iteration 1428, loss = 0.04359397\n",
      "Iteration 1429, loss = 0.04358463\n",
      "Iteration 1430, loss = 0.04356987\n",
      "Iteration 1431, loss = 0.04355553\n",
      "Iteration 1432, loss = 0.04354378\n",
      "Iteration 1433, loss = 0.04353565\n",
      "Iteration 1434, loss = 0.04352355\n",
      "Iteration 1435, loss = 0.04351094\n",
      "Iteration 1436, loss = 0.04349105\n",
      "Iteration 1437, loss = 0.04348416\n",
      "Iteration 1438, loss = 0.04347320\n",
      "Iteration 1439, loss = 0.04345585\n",
      "Iteration 1440, loss = 0.04345059\n",
      "Iteration 1441, loss = 0.04343473\n",
      "Iteration 1442, loss = 0.04342248\n",
      "Iteration 1443, loss = 0.04341228\n",
      "Iteration 1444, loss = 0.04340100\n",
      "Iteration 1445, loss = 0.04338848\n",
      "Iteration 1446, loss = 0.04337658\n",
      "Iteration 1447, loss = 0.04336511\n",
      "Iteration 1448, loss = 0.04335018\n",
      "Iteration 1449, loss = 0.04334561\n",
      "Iteration 1450, loss = 0.04333221\n",
      "Iteration 1451, loss = 0.04332038\n",
      "Iteration 1452, loss = 0.04330408\n",
      "Iteration 1453, loss = 0.04328885\n",
      "Iteration 1454, loss = 0.04327950\n",
      "Iteration 1455, loss = 0.04327022\n",
      "Iteration 1456, loss = 0.04326391\n",
      "Iteration 1457, loss = 0.04324603\n",
      "Iteration 1458, loss = 0.04323162\n",
      "Iteration 1459, loss = 0.04322218\n",
      "Iteration 1460, loss = 0.04320947\n",
      "Iteration 1461, loss = 0.04319876\n",
      "Iteration 1462, loss = 0.04318524\n",
      "Iteration 1463, loss = 0.04318241\n",
      "Iteration 1464, loss = 0.04316199\n",
      "Iteration 1465, loss = 0.04315593\n",
      "Iteration 1466, loss = 0.04314086\n",
      "Iteration 1467, loss = 0.04313099\n",
      "Iteration 1468, loss = 0.04311968\n",
      "Iteration 1469, loss = 0.04310774\n",
      "Iteration 1470, loss = 0.04310178\n",
      "Iteration 1471, loss = 0.04308733\n",
      "Iteration 1472, loss = 0.04307361\n",
      "Iteration 1473, loss = 0.04305957\n",
      "Iteration 1474, loss = 0.04305570\n",
      "Iteration 1475, loss = 0.04303394\n",
      "Iteration 1476, loss = 0.04302210\n",
      "Iteration 1477, loss = 0.04301454\n",
      "Iteration 1478, loss = 0.04300464\n",
      "Iteration 1479, loss = 0.04298999\n",
      "Iteration 1480, loss = 0.04298341\n",
      "Iteration 1481, loss = 0.04297561\n",
      "Iteration 1482, loss = 0.04296085\n",
      "Iteration 1483, loss = 0.04294823\n",
      "Iteration 1484, loss = 0.04293570\n",
      "Iteration 1485, loss = 0.04292772\n",
      "Iteration 1486, loss = 0.04291278\n",
      "Iteration 1487, loss = 0.04290388\n",
      "Iteration 1488, loss = 0.04288598\n",
      "Iteration 1489, loss = 0.04287873\n",
      "Iteration 1490, loss = 0.04287025\n",
      "Iteration 1491, loss = 0.04285335\n",
      "Iteration 1492, loss = 0.04284468\n",
      "Iteration 1493, loss = 0.04282995\n",
      "Iteration 1494, loss = 0.04282060\n",
      "Iteration 1495, loss = 0.04281944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1496, loss = 0.04280046\n",
      "Iteration 1497, loss = 0.04279527\n",
      "Iteration 1498, loss = 0.04277402\n",
      "Iteration 1499, loss = 0.04276522\n",
      "Iteration 1500, loss = 0.04275541\n",
      "Iteration 1501, loss = 0.04274295\n",
      "Iteration 1502, loss = 0.04273318\n",
      "Iteration 1503, loss = 0.04272482\n",
      "Iteration 1504, loss = 0.04270733\n",
      "Iteration 1505, loss = 0.04269824\n",
      "Iteration 1506, loss = 0.04268826\n",
      "Iteration 1507, loss = 0.04267998\n",
      "Iteration 1508, loss = 0.04266189\n",
      "Iteration 1509, loss = 0.04265473\n",
      "Iteration 1510, loss = 0.04264123\n",
      "Iteration 1511, loss = 0.04264016\n",
      "Iteration 1512, loss = 0.04262546\n",
      "Iteration 1513, loss = 0.04261576\n",
      "Iteration 1514, loss = 0.04260251\n",
      "Iteration 1515, loss = 0.04258953\n",
      "Iteration 1516, loss = 0.04257211\n",
      "Iteration 1517, loss = 0.04256860\n",
      "Iteration 1518, loss = 0.04255631\n",
      "Iteration 1519, loss = 0.04254641\n",
      "Iteration 1520, loss = 0.04252844\n",
      "Iteration 1521, loss = 0.04252373\n",
      "Iteration 1522, loss = 0.04252234\n",
      "Iteration 1523, loss = 0.04250093\n",
      "Iteration 1524, loss = 0.04248825\n",
      "Iteration 1525, loss = 0.04248160\n",
      "Iteration 1526, loss = 0.04248052\n",
      "Iteration 1527, loss = 0.04246231\n",
      "Iteration 1528, loss = 0.04244915\n",
      "Iteration 1529, loss = 0.04243722\n",
      "Iteration 1530, loss = 0.04242320\n",
      "Iteration 1531, loss = 0.04241442\n",
      "Iteration 1532, loss = 0.04240466\n",
      "Iteration 1533, loss = 0.04240263\n",
      "Iteration 1534, loss = 0.04238349\n",
      "Iteration 1535, loss = 0.04236983\n",
      "Iteration 1536, loss = 0.04236347\n",
      "Iteration 1537, loss = 0.04235205\n",
      "Iteration 1538, loss = 0.04234292\n",
      "Iteration 1539, loss = 0.04232462\n",
      "Iteration 1540, loss = 0.04231620\n",
      "Iteration 1541, loss = 0.04231030\n",
      "Iteration 1542, loss = 0.04229773\n",
      "Iteration 1543, loss = 0.04228756\n",
      "Iteration 1544, loss = 0.04227493\n",
      "Iteration 1545, loss = 0.04226183\n",
      "Iteration 1546, loss = 0.04225351\n",
      "Iteration 1547, loss = 0.04223947\n",
      "Iteration 1548, loss = 0.04223587\n",
      "Iteration 1549, loss = 0.04222019\n",
      "Iteration 1550, loss = 0.04220874\n",
      "Iteration 1551, loss = 0.04219921\n",
      "Iteration 1552, loss = 0.04219123\n",
      "Iteration 1553, loss = 0.04218896\n",
      "Iteration 1554, loss = 0.04217013\n",
      "Iteration 1555, loss = 0.04215586\n",
      "Iteration 1556, loss = 0.04214617\n",
      "Iteration 1557, loss = 0.04213863\n",
      "Iteration 1558, loss = 0.04212239\n",
      "Iteration 1559, loss = 0.04212600\n",
      "Iteration 1560, loss = 0.04209962\n",
      "Iteration 1561, loss = 0.04209548\n",
      "Iteration 1562, loss = 0.04208642\n",
      "Iteration 1563, loss = 0.04207032\n",
      "Iteration 1564, loss = 0.04206548\n",
      "Iteration 1565, loss = 0.04205209\n",
      "Iteration 1566, loss = 0.04203954\n",
      "Iteration 1567, loss = 0.04203014\n",
      "Iteration 1568, loss = 0.04202085\n",
      "Iteration 1569, loss = 0.04200807\n",
      "Iteration 1570, loss = 0.04199655\n",
      "Iteration 1571, loss = 0.04198845\n",
      "Iteration 1572, loss = 0.04197589\n",
      "Iteration 1573, loss = 0.04196727\n",
      "Iteration 1574, loss = 0.04195368\n",
      "Iteration 1575, loss = 0.04194709\n",
      "Iteration 1576, loss = 0.04193424\n",
      "Iteration 1577, loss = 0.04192436\n",
      "Iteration 1578, loss = 0.04191818\n",
      "Iteration 1579, loss = 0.04190436\n",
      "Iteration 1580, loss = 0.04189436\n",
      "Iteration 1581, loss = 0.04188717\n",
      "Iteration 1582, loss = 0.04187181\n",
      "Iteration 1583, loss = 0.04186278\n",
      "Iteration 1584, loss = 0.04185408\n",
      "Iteration 1585, loss = 0.04184558\n",
      "Iteration 1586, loss = 0.04184034\n",
      "Iteration 1587, loss = 0.04182517\n",
      "Iteration 1588, loss = 0.04182122\n",
      "Iteration 1589, loss = 0.04180504\n",
      "Iteration 1590, loss = 0.04179208\n",
      "Iteration 1591, loss = 0.04178290\n",
      "Iteration 1592, loss = 0.04177352\n",
      "Iteration 1593, loss = 0.04176363\n",
      "Iteration 1594, loss = 0.04174963\n",
      "Iteration 1595, loss = 0.04174335\n",
      "Iteration 1596, loss = 0.04173249\n",
      "Iteration 1597, loss = 0.04172220\n",
      "Iteration 1598, loss = 0.04170990\n",
      "Iteration 1599, loss = 0.04169948\n",
      "Iteration 1600, loss = 0.04168604\n",
      "Iteration 1601, loss = 0.04168547\n",
      "Iteration 1602, loss = 0.04167205\n",
      "Iteration 1603, loss = 0.04165628\n",
      "Iteration 1604, loss = 0.04166334\n",
      "Iteration 1605, loss = 0.04164338\n",
      "Iteration 1606, loss = 0.04162861\n",
      "Iteration 1607, loss = 0.04162643\n",
      "Iteration 1608, loss = 0.04161148\n",
      "Iteration 1609, loss = 0.04160368\n",
      "Iteration 1610, loss = 0.04159227\n",
      "Iteration 1611, loss = 0.04157807\n",
      "Iteration 1612, loss = 0.04156626\n",
      "Iteration 1613, loss = 0.04155486\n",
      "Iteration 1614, loss = 0.04155259\n",
      "Iteration 1615, loss = 0.04153405\n",
      "Iteration 1616, loss = 0.04152349\n",
      "Iteration 1617, loss = 0.04151485\n",
      "Iteration 1618, loss = 0.04150750\n",
      "Iteration 1619, loss = 0.04149312\n",
      "Iteration 1620, loss = 0.04148349\n",
      "Iteration 1621, loss = 0.04149106\n",
      "Iteration 1622, loss = 0.04146708\n",
      "Iteration 1623, loss = 0.04145445\n",
      "Iteration 1624, loss = 0.04144681\n",
      "Iteration 1625, loss = 0.04144099\n",
      "Iteration 1626, loss = 0.04145392\n",
      "Iteration 1627, loss = 0.04141684\n",
      "Iteration 1628, loss = 0.04140323\n",
      "Iteration 1629, loss = 0.04140248\n",
      "Iteration 1630, loss = 0.04139188\n",
      "Iteration 1631, loss = 0.04137763\n",
      "Iteration 1632, loss = 0.04136614\n",
      "Iteration 1633, loss = 0.04135980\n",
      "Iteration 1634, loss = 0.04134124\n",
      "Iteration 1635, loss = 0.04133371\n",
      "Iteration 1636, loss = 0.04132322\n",
      "Iteration 1637, loss = 0.04131738\n",
      "Iteration 1638, loss = 0.04130182\n",
      "Iteration 1639, loss = 0.04129369\n",
      "Iteration 1640, loss = 0.04128471\n",
      "Iteration 1641, loss = 0.04128485\n",
      "Iteration 1642, loss = 0.04126312\n",
      "Iteration 1643, loss = 0.04125401\n",
      "Iteration 1644, loss = 0.04124393\n",
      "Iteration 1645, loss = 0.04123147\n",
      "Iteration 1646, loss = 0.04122207\n",
      "Iteration 1647, loss = 0.04121655\n",
      "Iteration 1648, loss = 0.04120870\n",
      "Iteration 1649, loss = 0.04119478\n",
      "Iteration 1650, loss = 0.04118495\n",
      "Iteration 1651, loss = 0.04117119\n",
      "Iteration 1652, loss = 0.04116187\n",
      "Iteration 1653, loss = 0.04115084\n",
      "Iteration 1654, loss = 0.04114506\n",
      "Iteration 1655, loss = 0.04113148\n",
      "Iteration 1656, loss = 0.04112549\n",
      "Iteration 1657, loss = 0.04111050\n",
      "Iteration 1658, loss = 0.04110412\n",
      "Iteration 1659, loss = 0.04109169\n",
      "Iteration 1660, loss = 0.04108391\n",
      "Iteration 1661, loss = 0.04107147\n",
      "Iteration 1662, loss = 0.04106048\n",
      "Iteration 1663, loss = 0.04106005\n",
      "Iteration 1664, loss = 0.04104408\n",
      "Iteration 1665, loss = 0.04103754\n",
      "Iteration 1666, loss = 0.04102525\n",
      "Iteration 1667, loss = 0.04101475\n",
      "Iteration 1668, loss = 0.04100527\n",
      "Iteration 1669, loss = 0.04099852\n",
      "Iteration 1670, loss = 0.04098281\n",
      "Iteration 1671, loss = 0.04097076\n",
      "Iteration 1672, loss = 0.04097245\n",
      "Iteration 1673, loss = 0.04095622\n",
      "Iteration 1674, loss = 0.04094225\n",
      "Iteration 1675, loss = 0.04093315\n",
      "Iteration 1676, loss = 0.04092407\n",
      "Iteration 1677, loss = 0.04091532\n",
      "Iteration 1678, loss = 0.04090770\n",
      "Iteration 1679, loss = 0.04090385\n",
      "Iteration 1680, loss = 0.04088696\n",
      "Iteration 1681, loss = 0.04087511\n",
      "Iteration 1682, loss = 0.04086834\n",
      "Iteration 1683, loss = 0.04085727\n",
      "Iteration 1684, loss = 0.04085026\n",
      "Iteration 1685, loss = 0.04084194\n",
      "Iteration 1686, loss = 0.04082980\n",
      "Iteration 1687, loss = 0.04081734\n",
      "Iteration 1688, loss = 0.04081073\n",
      "Iteration 1689, loss = 0.04079685\n",
      "Iteration 1690, loss = 0.04078859\n",
      "Iteration 1691, loss = 0.04077846\n",
      "Iteration 1692, loss = 0.04076587\n",
      "Iteration 1693, loss = 0.04075802\n",
      "Iteration 1694, loss = 0.04075099\n",
      "Iteration 1695, loss = 0.04073803\n",
      "Iteration 1696, loss = 0.04073457\n",
      "Iteration 1697, loss = 0.04071931\n",
      "Iteration 1698, loss = 0.04071152\n",
      "Iteration 1699, loss = 0.04070633\n",
      "Iteration 1700, loss = 0.04069510\n",
      "Iteration 1701, loss = 0.04068092\n",
      "Iteration 1702, loss = 0.04067557\n",
      "Iteration 1703, loss = 0.04066572\n",
      "Iteration 1704, loss = 0.04065823\n",
      "Iteration 1705, loss = 0.04064711\n",
      "Iteration 1706, loss = 0.04063234\n",
      "Iteration 1707, loss = 0.04062755\n",
      "Iteration 1708, loss = 0.04062265\n",
      "Iteration 1709, loss = 0.04060687\n",
      "Iteration 1710, loss = 0.04060188\n",
      "Iteration 1711, loss = 0.04059260\n",
      "Iteration 1712, loss = 0.04058994\n",
      "Iteration 1713, loss = 0.04057062\n",
      "Iteration 1714, loss = 0.04056151\n",
      "Iteration 1715, loss = 0.04055631\n",
      "Iteration 1716, loss = 0.04054432\n",
      "Iteration 1717, loss = 0.04053708\n",
      "Iteration 1718, loss = 0.04052049\n",
      "Iteration 1719, loss = 0.04051317\n",
      "Iteration 1720, loss = 0.04050572\n",
      "Iteration 1721, loss = 0.04049544\n",
      "Iteration 1722, loss = 0.04048366\n",
      "Iteration 1723, loss = 0.04047100\n",
      "Iteration 1724, loss = 0.04046855\n",
      "Iteration 1725, loss = 0.04045036\n",
      "Iteration 1726, loss = 0.04044768\n",
      "Iteration 1727, loss = 0.04043658\n",
      "Iteration 1728, loss = 0.04042806\n",
      "Iteration 1729, loss = 0.04041776\n",
      "Iteration 1730, loss = 0.04041358\n",
      "Iteration 1731, loss = 0.04040022\n",
      "Iteration 1732, loss = 0.04039146\n",
      "Iteration 1733, loss = 0.04038562\n",
      "Iteration 1734, loss = 0.04037358\n",
      "Iteration 1735, loss = 0.04036504\n",
      "Iteration 1736, loss = 0.04034950\n",
      "Iteration 1737, loss = 0.04034627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1738, loss = 0.04033073\n",
      "Iteration 1739, loss = 0.04032367\n",
      "Iteration 1740, loss = 0.04031424\n",
      "Iteration 1741, loss = 0.04031280\n",
      "Iteration 1742, loss = 0.04029553\n",
      "Iteration 1743, loss = 0.04028844\n",
      "Iteration 1744, loss = 0.04027580\n",
      "Iteration 1745, loss = 0.04026881\n",
      "Iteration 1746, loss = 0.04026058\n",
      "Iteration 1747, loss = 0.04025327\n",
      "Iteration 1748, loss = 0.04024132\n",
      "Iteration 1749, loss = 0.04023054\n",
      "Iteration 1750, loss = 0.04022489\n",
      "Iteration 1751, loss = 0.04021130\n",
      "Iteration 1752, loss = 0.04019941\n",
      "Iteration 1753, loss = 0.04019052\n",
      "Iteration 1754, loss = 0.04018194\n",
      "Iteration 1755, loss = 0.04017369\n",
      "Iteration 1756, loss = 0.04016225\n",
      "Iteration 1757, loss = 0.04015519\n",
      "Iteration 1758, loss = 0.04015127\n",
      "Iteration 1759, loss = 0.04013722\n",
      "Iteration 1760, loss = 0.04012792\n",
      "Iteration 1761, loss = 0.04011562\n",
      "Iteration 1762, loss = 0.04011151\n",
      "Iteration 1763, loss = 0.04010163\n",
      "Iteration 1764, loss = 0.04008663\n",
      "Iteration 1765, loss = 0.04008738\n",
      "Iteration 1766, loss = 0.04007470\n",
      "Iteration 1767, loss = 0.04006458\n",
      "Iteration 1768, loss = 0.04005528\n",
      "Iteration 1769, loss = 0.04004943\n",
      "Iteration 1770, loss = 0.04004069\n",
      "Iteration 1771, loss = 0.04002642\n",
      "Iteration 1772, loss = 0.04002581\n",
      "Iteration 1773, loss = 0.04001222\n",
      "Iteration 1774, loss = 0.03999859\n",
      "Iteration 1775, loss = 0.03999457\n",
      "Iteration 1776, loss = 0.03997757\n",
      "Iteration 1777, loss = 0.03997488\n",
      "Iteration 1778, loss = 0.03996730\n",
      "Iteration 1779, loss = 0.03995561\n",
      "Iteration 1780, loss = 0.03994963\n",
      "Iteration 1781, loss = 0.03994128\n",
      "Iteration 1782, loss = 0.03993033\n",
      "Iteration 1783, loss = 0.03991836\n",
      "Iteration 1784, loss = 0.03991058\n",
      "Iteration 1785, loss = 0.03989930\n",
      "Iteration 1786, loss = 0.03989128\n",
      "Iteration 1787, loss = 0.03988720\n",
      "Iteration 1788, loss = 0.03987134\n",
      "Iteration 1789, loss = 0.03986153\n",
      "Iteration 1790, loss = 0.03985584\n",
      "Iteration 1791, loss = 0.03984579\n",
      "Iteration 1792, loss = 0.03984129\n",
      "Iteration 1793, loss = 0.03983402\n",
      "Iteration 1794, loss = 0.03983051\n",
      "Iteration 1795, loss = 0.03981139\n",
      "Iteration 1796, loss = 0.03979936\n",
      "Iteration 1797, loss = 0.03979577\n",
      "Iteration 1798, loss = 0.03978200\n",
      "Iteration 1799, loss = 0.03977343\n",
      "Iteration 1800, loss = 0.03976253\n",
      "Iteration 1801, loss = 0.03975858\n",
      "Iteration 1802, loss = 0.03974423\n",
      "Iteration 1803, loss = 0.03974094\n",
      "Iteration 1804, loss = 0.03973418\n",
      "Iteration 1805, loss = 0.03971751\n",
      "Iteration 1806, loss = 0.03971216\n",
      "Iteration 1807, loss = 0.03970193\n",
      "Iteration 1808, loss = 0.03969276\n",
      "Iteration 1809, loss = 0.03968363\n",
      "Iteration 1810, loss = 0.03967249\n",
      "Iteration 1811, loss = 0.03966757\n",
      "Iteration 1812, loss = 0.03965687\n",
      "Iteration 1813, loss = 0.03964776\n",
      "Iteration 1814, loss = 0.03963582\n",
      "Iteration 1815, loss = 0.03962842\n",
      "Iteration 1816, loss = 0.03962225\n",
      "Iteration 1817, loss = 0.03961115\n",
      "Iteration 1818, loss = 0.03960588\n",
      "Iteration 1819, loss = 0.03959638\n",
      "Iteration 1820, loss = 0.03958515\n",
      "Iteration 1821, loss = 0.03957772\n",
      "Iteration 1822, loss = 0.03957130\n",
      "Iteration 1823, loss = 0.03956256\n",
      "Iteration 1824, loss = 0.03955372\n",
      "Iteration 1825, loss = 0.03954335\n",
      "Iteration 1826, loss = 0.03953138\n",
      "Iteration 1827, loss = 0.03952490\n",
      "Iteration 1828, loss = 0.03951893\n",
      "Iteration 1829, loss = 0.03951771\n",
      "Iteration 1830, loss = 0.03949655\n",
      "Iteration 1831, loss = 0.03948932\n",
      "Iteration 1832, loss = 0.03948236\n",
      "Iteration 1833, loss = 0.03947235\n",
      "Iteration 1834, loss = 0.03946270\n",
      "Iteration 1835, loss = 0.03945374\n",
      "Iteration 1836, loss = 0.03944337\n",
      "Iteration 1837, loss = 0.03944065\n",
      "Iteration 1838, loss = 0.03942751\n",
      "Iteration 1839, loss = 0.03942442\n",
      "Iteration 1840, loss = 0.03941548\n",
      "Iteration 1841, loss = 0.03940203\n",
      "Iteration 1842, loss = 0.03939504\n",
      "Iteration 1843, loss = 0.03938998\n",
      "Iteration 1844, loss = 0.03937771\n",
      "Iteration 1845, loss = 0.03936619\n",
      "Iteration 1846, loss = 0.03935731\n",
      "Iteration 1847, loss = 0.03935030\n",
      "Iteration 1848, loss = 0.03934226\n",
      "Iteration 1849, loss = 0.03933058\n",
      "Iteration 1850, loss = 0.03931999\n",
      "Iteration 1851, loss = 0.03931530\n",
      "Iteration 1852, loss = 0.03930707\n",
      "Iteration 1853, loss = 0.03930155\n",
      "Iteration 1854, loss = 0.03928804\n",
      "Iteration 1855, loss = 0.03928005\n",
      "Iteration 1856, loss = 0.03927261\n",
      "Iteration 1857, loss = 0.03926718\n",
      "Iteration 1858, loss = 0.03925105\n",
      "Iteration 1859, loss = 0.03924181\n",
      "Iteration 1860, loss = 0.03923261\n",
      "Iteration 1861, loss = 0.03922457\n",
      "Iteration 1862, loss = 0.03921302\n",
      "Iteration 1863, loss = 0.03920699\n",
      "Iteration 1864, loss = 0.03919894\n",
      "Iteration 1865, loss = 0.03919399\n",
      "Iteration 1866, loss = 0.03918400\n",
      "Iteration 1867, loss = 0.03917120\n",
      "Iteration 1868, loss = 0.03916728\n",
      "Iteration 1869, loss = 0.03915551\n",
      "Iteration 1870, loss = 0.03914826\n",
      "Iteration 1871, loss = 0.03914019\n",
      "Iteration 1872, loss = 0.03912772\n",
      "Iteration 1873, loss = 0.03912173\n",
      "Iteration 1874, loss = 0.03911422\n",
      "Iteration 1875, loss = 0.03910742\n",
      "Iteration 1876, loss = 0.03910085\n",
      "Iteration 1877, loss = 0.03908542\n",
      "Iteration 1878, loss = 0.03907922\n",
      "Iteration 1879, loss = 0.03907499\n",
      "Iteration 1880, loss = 0.03906378\n",
      "Iteration 1881, loss = 0.03905050\n",
      "Iteration 1882, loss = 0.03904362\n",
      "Iteration 1883, loss = 0.03903861\n",
      "Iteration 1884, loss = 0.03902630\n",
      "Iteration 1885, loss = 0.03901989\n",
      "Iteration 1886, loss = 0.03901438\n",
      "Iteration 1887, loss = 0.03900070\n",
      "Iteration 1888, loss = 0.03899718\n",
      "Iteration 1889, loss = 0.03898757\n",
      "Iteration 1890, loss = 0.03898028\n",
      "Iteration 1891, loss = 0.03897067\n",
      "Iteration 1892, loss = 0.03896279\n",
      "Iteration 1893, loss = 0.03895690\n",
      "Iteration 1894, loss = 0.03894590\n",
      "Iteration 1895, loss = 0.03893607\n",
      "Iteration 1896, loss = 0.03892632\n",
      "Iteration 1897, loss = 0.03891593\n",
      "Iteration 1898, loss = 0.03891076\n",
      "Iteration 1899, loss = 0.03890221\n",
      "Iteration 1900, loss = 0.03889709\n",
      "Iteration 1901, loss = 0.03888486\n",
      "Iteration 1902, loss = 0.03887384\n",
      "Iteration 1903, loss = 0.03886858\n",
      "Iteration 1904, loss = 0.03885766\n",
      "Iteration 1905, loss = 0.03884858\n",
      "Iteration 1906, loss = 0.03884143\n",
      "Iteration 1907, loss = 0.03883106\n",
      "Iteration 1908, loss = 0.03882766\n",
      "Iteration 1909, loss = 0.03881715\n",
      "Iteration 1910, loss = 0.03880714\n",
      "Iteration 1911, loss = 0.03879814\n",
      "Iteration 1912, loss = 0.03878881\n",
      "Iteration 1913, loss = 0.03877786\n",
      "Iteration 1914, loss = 0.03877426\n",
      "Iteration 1915, loss = 0.03876602\n",
      "Iteration 1916, loss = 0.03876119\n",
      "Iteration 1917, loss = 0.03875141\n",
      "Iteration 1918, loss = 0.03874148\n",
      "Iteration 1919, loss = 0.03872916\n",
      "Iteration 1920, loss = 0.03872440\n",
      "Iteration 1921, loss = 0.03871495\n",
      "Iteration 1922, loss = 0.03870605\n",
      "Iteration 1923, loss = 0.03870431\n",
      "Iteration 1924, loss = 0.03869461\n",
      "Iteration 1925, loss = 0.03868215\n",
      "Iteration 1926, loss = 0.03867199\n",
      "Iteration 1927, loss = 0.03866203\n",
      "Iteration 1928, loss = 0.03865248\n",
      "Iteration 1929, loss = 0.03864663\n",
      "Iteration 1930, loss = 0.03863779\n",
      "Iteration 1931, loss = 0.03863033\n",
      "Iteration 1932, loss = 0.03861981\n",
      "Iteration 1933, loss = 0.03861345\n",
      "Iteration 1934, loss = 0.03860792\n",
      "Iteration 1935, loss = 0.03859992\n",
      "Iteration 1936, loss = 0.03859278\n",
      "Iteration 1937, loss = 0.03857860\n",
      "Iteration 1938, loss = 0.03857419\n",
      "Iteration 1939, loss = 0.03856564\n",
      "Iteration 1940, loss = 0.03855445\n",
      "Iteration 1941, loss = 0.03854897\n",
      "Iteration 1942, loss = 0.03853641\n",
      "Iteration 1943, loss = 0.03853771\n",
      "Iteration 1944, loss = 0.03852378\n",
      "Iteration 1945, loss = 0.03852110\n",
      "Iteration 1946, loss = 0.03851001\n",
      "Iteration 1947, loss = 0.03849651\n",
      "Iteration 1948, loss = 0.03849184\n",
      "Iteration 1949, loss = 0.03848520\n",
      "Iteration 1950, loss = 0.03847449\n",
      "Iteration 1951, loss = 0.03846453\n",
      "Iteration 1952, loss = 0.03845560\n",
      "Iteration 1953, loss = 0.03845112\n",
      "Iteration 1954, loss = 0.03843678\n",
      "Iteration 1955, loss = 0.03842768\n",
      "Iteration 1956, loss = 0.03842199\n",
      "Iteration 1957, loss = 0.03841179\n",
      "Iteration 1958, loss = 0.03840897\n",
      "Iteration 1959, loss = 0.03839959\n",
      "Iteration 1960, loss = 0.03839049\n",
      "Iteration 1961, loss = 0.03838193\n",
      "Iteration 1962, loss = 0.03837432\n",
      "Iteration 1963, loss = 0.03836938\n",
      "Iteration 1964, loss = 0.03835957\n",
      "Iteration 1965, loss = 0.03835181\n",
      "Iteration 1966, loss = 0.03834032\n",
      "Iteration 1967, loss = 0.03833615\n",
      "Iteration 1968, loss = 0.03832518\n",
      "Iteration 1969, loss = 0.03831427\n",
      "Iteration 1970, loss = 0.03831072\n",
      "Iteration 1971, loss = 0.03830702\n",
      "Iteration 1972, loss = 0.03829517\n",
      "Iteration 1973, loss = 0.03828551\n",
      "Iteration 1974, loss = 0.03827720\n",
      "Iteration 1975, loss = 0.03826615\n",
      "Iteration 1976, loss = 0.03826235\n",
      "Iteration 1977, loss = 0.03824635\n",
      "Iteration 1978, loss = 0.03824493\n",
      "Iteration 1979, loss = 0.03824159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1980, loss = 0.03822896\n",
      "Iteration 1981, loss = 0.03822152\n",
      "Iteration 1982, loss = 0.03821467\n",
      "Iteration 1983, loss = 0.03821088\n",
      "Iteration 1984, loss = 0.03819614\n",
      "Iteration 1985, loss = 0.03818456\n",
      "Iteration 1986, loss = 0.03817542\n",
      "Iteration 1987, loss = 0.03817060\n",
      "Iteration 1988, loss = 0.03816956\n",
      "Iteration 1989, loss = 0.03815439\n",
      "Iteration 1990, loss = 0.03814472\n",
      "Iteration 1991, loss = 0.03814053\n",
      "Iteration 1992, loss = 0.03813197\n",
      "Iteration 1993, loss = 0.03812698\n",
      "Iteration 1994, loss = 0.03811744\n",
      "Iteration 1995, loss = 0.03811023\n",
      "Iteration 1996, loss = 0.03809826\n",
      "Iteration 1997, loss = 0.03809580\n",
      "Iteration 1998, loss = 0.03808650\n",
      "Iteration 1999, loss = 0.03807435\n",
      "Iteration 2000, loss = 0.03806995\n",
      "Iteration 2001, loss = 0.03805804\n",
      "Iteration 2002, loss = 0.03805116\n",
      "Iteration 2003, loss = 0.03804304\n",
      "Iteration 2004, loss = 0.03803902\n",
      "Iteration 2005, loss = 0.03802873\n",
      "Iteration 2006, loss = 0.03802011\n",
      "Iteration 2007, loss = 0.03801626\n",
      "Iteration 2008, loss = 0.03800404\n",
      "Iteration 2009, loss = 0.03799661\n",
      "Iteration 2010, loss = 0.03799714\n",
      "Iteration 2011, loss = 0.03797913\n",
      "Iteration 2012, loss = 0.03796850\n",
      "Iteration 2013, loss = 0.03796873\n",
      "Iteration 2014, loss = 0.03795532\n",
      "Iteration 2015, loss = 0.03794824\n",
      "Iteration 2016, loss = 0.03794038\n",
      "Iteration 2017, loss = 0.03793079\n",
      "Iteration 2018, loss = 0.03792796\n",
      "Iteration 2019, loss = 0.03791693\n",
      "Iteration 2020, loss = 0.03790816\n",
      "Iteration 2021, loss = 0.03790424\n",
      "Iteration 2022, loss = 0.03789862\n",
      "Iteration 2023, loss = 0.03788922\n",
      "Iteration 2024, loss = 0.03788058\n",
      "Iteration 2025, loss = 0.03787265\n",
      "Iteration 2026, loss = 0.03786513\n",
      "Iteration 2027, loss = 0.03785401\n",
      "Iteration 2028, loss = 0.03784706\n",
      "Iteration 2029, loss = 0.03783622\n",
      "Iteration 2030, loss = 0.03783461\n",
      "Iteration 2031, loss = 0.03782936\n",
      "Iteration 2032, loss = 0.03782258\n",
      "Iteration 2033, loss = 0.03780688\n",
      "Iteration 2034, loss = 0.03780100\n",
      "Iteration 2035, loss = 0.03779741\n",
      "Iteration 2036, loss = 0.03778504\n",
      "Iteration 2037, loss = 0.03778045\n",
      "Iteration 2038, loss = 0.03777184\n",
      "Iteration 2039, loss = 0.03776224\n",
      "Iteration 2040, loss = 0.03775232\n",
      "Iteration 2041, loss = 0.03774570\n",
      "Iteration 2042, loss = 0.03774005\n",
      "Iteration 2043, loss = 0.03773025\n",
      "Iteration 2044, loss = 0.03772663\n",
      "Iteration 2045, loss = 0.03771461\n",
      "Iteration 2046, loss = 0.03770595\n",
      "Iteration 2047, loss = 0.03769592\n",
      "Iteration 2048, loss = 0.03768949\n",
      "Iteration 2049, loss = 0.03768970\n",
      "Iteration 2050, loss = 0.03767614\n",
      "Iteration 2051, loss = 0.03766543\n",
      "Iteration 2052, loss = 0.03765798\n",
      "Iteration 2053, loss = 0.03765199\n",
      "Iteration 2054, loss = 0.03764310\n",
      "Iteration 2055, loss = 0.03763335\n",
      "Iteration 2056, loss = 0.03763030\n",
      "Iteration 2057, loss = 0.03761856\n",
      "Iteration 2058, loss = 0.03761569\n",
      "Iteration 2059, loss = 0.03760719\n",
      "Iteration 2060, loss = 0.03760653\n",
      "Iteration 2061, loss = 0.03759754\n",
      "Iteration 2062, loss = 0.03758027\n",
      "Iteration 2063, loss = 0.03757616\n",
      "Iteration 2064, loss = 0.03756934\n",
      "Iteration 2065, loss = 0.03756586\n",
      "Iteration 2066, loss = 0.03755138\n",
      "Iteration 2067, loss = 0.03754336\n",
      "Iteration 2068, loss = 0.03753853\n",
      "Iteration 2069, loss = 0.03753922\n",
      "Iteration 2070, loss = 0.03751903\n",
      "Iteration 2071, loss = 0.03751921\n",
      "Iteration 2072, loss = 0.03750654\n",
      "Iteration 2073, loss = 0.03750133\n",
      "Iteration 2074, loss = 0.03749069\n",
      "Iteration 2075, loss = 0.03748016\n",
      "Iteration 2076, loss = 0.03747742\n",
      "Iteration 2077, loss = 0.03747089\n",
      "Iteration 2078, loss = 0.03746395\n",
      "Iteration 2079, loss = 0.03745550\n",
      "Iteration 2080, loss = 0.03745064\n",
      "Iteration 2081, loss = 0.03744421\n",
      "Iteration 2082, loss = 0.03742970\n",
      "Iteration 2083, loss = 0.03742230\n",
      "Iteration 2084, loss = 0.03741824\n",
      "Iteration 2085, loss = 0.03741279\n",
      "Iteration 2086, loss = 0.03740288\n",
      "Iteration 2087, loss = 0.03739356\n",
      "Iteration 2088, loss = 0.03739065\n",
      "Iteration 2089, loss = 0.03737816\n",
      "Iteration 2090, loss = 0.03737100\n",
      "Iteration 2091, loss = 0.03736512\n",
      "Iteration 2092, loss = 0.03735516\n",
      "Iteration 2093, loss = 0.03735006\n",
      "Iteration 2094, loss = 0.03734450\n",
      "Iteration 2095, loss = 0.03733566\n",
      "Iteration 2096, loss = 0.03732449\n",
      "Iteration 2097, loss = 0.03732017\n",
      "Iteration 2098, loss = 0.03731153\n",
      "Iteration 2099, loss = 0.03730850\n",
      "Iteration 2100, loss = 0.03729317\n",
      "Iteration 2101, loss = 0.03728764\n",
      "Iteration 2102, loss = 0.03727707\n",
      "Iteration 2103, loss = 0.03727397\n",
      "Iteration 2104, loss = 0.03726231\n",
      "Iteration 2105, loss = 0.03726035\n",
      "Iteration 2106, loss = 0.03725142\n",
      "Iteration 2107, loss = 0.03724081\n",
      "Iteration 2108, loss = 0.03723747\n",
      "Iteration 2109, loss = 0.03722772\n",
      "Iteration 2110, loss = 0.03721714\n",
      "Iteration 2111, loss = 0.03721530\n",
      "Iteration 2112, loss = 0.03720541\n",
      "Iteration 2113, loss = 0.03719402\n",
      "Iteration 2114, loss = 0.03719121\n",
      "Iteration 2115, loss = 0.03718104\n",
      "Iteration 2116, loss = 0.03717869\n",
      "Iteration 2117, loss = 0.03716456\n",
      "Iteration 2118, loss = 0.03715890\n",
      "Iteration 2119, loss = 0.03715419\n",
      "Iteration 2120, loss = 0.03714819\n",
      "Iteration 2121, loss = 0.03713290\n",
      "Iteration 2122, loss = 0.03713154\n",
      "Iteration 2123, loss = 0.03712818\n",
      "Iteration 2124, loss = 0.03711416\n",
      "Iteration 2125, loss = 0.03710773\n",
      "Iteration 2126, loss = 0.03710077\n",
      "Iteration 2127, loss = 0.03709192\n",
      "Iteration 2128, loss = 0.03708340\n",
      "Iteration 2129, loss = 0.03707486\n",
      "Iteration 2130, loss = 0.03706721\n",
      "Iteration 2131, loss = 0.03706193\n",
      "Iteration 2132, loss = 0.03705777\n",
      "Iteration 2133, loss = 0.03704769\n",
      "Iteration 2134, loss = 0.03703854\n",
      "Iteration 2135, loss = 0.03703276\n",
      "Iteration 2136, loss = 0.03702925\n",
      "Iteration 2137, loss = 0.03702659\n",
      "Iteration 2138, loss = 0.03700765\n",
      "Iteration 2139, loss = 0.03700662\n",
      "Iteration 2140, loss = 0.03699947\n",
      "Iteration 2141, loss = 0.03699070\n",
      "Iteration 2142, loss = 0.03698453\n",
      "Iteration 2143, loss = 0.03697422\n",
      "Iteration 2144, loss = 0.03696719\n",
      "Iteration 2145, loss = 0.03696396\n",
      "Iteration 2146, loss = 0.03695608\n",
      "Iteration 2147, loss = 0.03694604\n",
      "Iteration 2148, loss = 0.03694245\n",
      "Iteration 2149, loss = 0.03695043\n",
      "Iteration 2150, loss = 0.03692097\n",
      "Iteration 2151, loss = 0.03691365\n",
      "Iteration 2152, loss = 0.03690582\n",
      "Iteration 2153, loss = 0.03690276\n",
      "Iteration 2154, loss = 0.03689331\n",
      "Iteration 2155, loss = 0.03688563\n",
      "Iteration 2156, loss = 0.03687572\n",
      "Iteration 2157, loss = 0.03687111\n",
      "Iteration 2158, loss = 0.03686179\n",
      "Iteration 2159, loss = 0.03685215\n",
      "Iteration 2160, loss = 0.03685214\n",
      "Iteration 2161, loss = 0.03683884\n",
      "Iteration 2162, loss = 0.03683576\n",
      "Iteration 2163, loss = 0.03682743\n",
      "Iteration 2164, loss = 0.03682155\n",
      "Iteration 2165, loss = 0.03681409\n",
      "Iteration 2166, loss = 0.03680411\n",
      "Iteration 2167, loss = 0.03680001\n",
      "Iteration 2168, loss = 0.03679009\n",
      "Iteration 2169, loss = 0.03677864\n",
      "Iteration 2170, loss = 0.03677676\n",
      "Iteration 2171, loss = 0.03676800\n",
      "Iteration 2172, loss = 0.03676065\n",
      "Iteration 2173, loss = 0.03674874\n",
      "Iteration 2174, loss = 0.03674517\n",
      "Iteration 2175, loss = 0.03673795\n",
      "Iteration 2176, loss = 0.03673761\n",
      "Iteration 2177, loss = 0.03672427\n",
      "Iteration 2178, loss = 0.03671338\n",
      "Iteration 2179, loss = 0.03670920\n",
      "Iteration 2180, loss = 0.03670838\n",
      "Iteration 2181, loss = 0.03669646\n",
      "Iteration 2182, loss = 0.03668545\n",
      "Iteration 2183, loss = 0.03668107\n",
      "Iteration 2184, loss = 0.03668212\n",
      "Iteration 2185, loss = 0.03666050\n",
      "Iteration 2186, loss = 0.03665963\n",
      "Iteration 2187, loss = 0.03664623\n",
      "Iteration 2188, loss = 0.03664298\n",
      "Iteration 2189, loss = 0.03663961\n",
      "Iteration 2190, loss = 0.03662989\n",
      "Iteration 2191, loss = 0.03662540\n",
      "Iteration 2192, loss = 0.03661829\n",
      "Iteration 2193, loss = 0.03660659\n",
      "Iteration 2194, loss = 0.03660062\n",
      "Iteration 2195, loss = 0.03659231\n",
      "Iteration 2196, loss = 0.03658213\n",
      "Iteration 2197, loss = 0.03657157\n",
      "Iteration 2198, loss = 0.03657009\n",
      "Iteration 2199, loss = 0.03656337\n",
      "Iteration 2200, loss = 0.03655343\n",
      "Iteration 2201, loss = 0.03654927\n",
      "Iteration 2202, loss = 0.03654616\n",
      "Iteration 2203, loss = 0.03653045\n",
      "Iteration 2204, loss = 0.03653005\n",
      "Iteration 2205, loss = 0.03651701\n",
      "Iteration 2206, loss = 0.03651505\n",
      "Iteration 2207, loss = 0.03650456\n",
      "Iteration 2208, loss = 0.03649899\n",
      "Iteration 2209, loss = 0.03649489\n",
      "Iteration 2210, loss = 0.03647941\n",
      "Iteration 2211, loss = 0.03647592\n",
      "Iteration 2212, loss = 0.03646619\n",
      "Iteration 2213, loss = 0.03646418\n",
      "Iteration 2214, loss = 0.03645517\n",
      "Iteration 2215, loss = 0.03644697\n",
      "Iteration 2216, loss = 0.03643998\n",
      "Iteration 2217, loss = 0.03643150\n",
      "Iteration 2218, loss = 0.03642162\n",
      "Iteration 2219, loss = 0.03641875\n",
      "Iteration 2220, loss = 0.03641019\n",
      "Iteration 2221, loss = 0.03640410\n",
      "Iteration 2222, loss = 0.03640487\n",
      "Iteration 2223, loss = 0.03639218\n",
      "Iteration 2224, loss = 0.03638131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2225, loss = 0.03637830\n",
      "Iteration 2226, loss = 0.03636541\n",
      "Iteration 2227, loss = 0.03635814\n",
      "Iteration 2228, loss = 0.03635339\n",
      "Iteration 2229, loss = 0.03634695\n",
      "Iteration 2230, loss = 0.03633650\n",
      "Iteration 2231, loss = 0.03633401\n",
      "Iteration 2232, loss = 0.03632270\n",
      "Iteration 2233, loss = 0.03631991\n",
      "Iteration 2234, loss = 0.03631062\n",
      "Iteration 2235, loss = 0.03630332\n",
      "Iteration 2236, loss = 0.03629921\n",
      "Iteration 2237, loss = 0.03628783\n",
      "Iteration 2238, loss = 0.03628425\n",
      "Iteration 2239, loss = 0.03628199\n",
      "Iteration 2240, loss = 0.03626839\n",
      "Iteration 2241, loss = 0.03625672\n",
      "Iteration 2242, loss = 0.03625199\n",
      "Iteration 2243, loss = 0.03624395\n",
      "Iteration 2244, loss = 0.03623605\n",
      "Iteration 2245, loss = 0.03623417\n",
      "Iteration 2246, loss = 0.03622215\n",
      "Iteration 2247, loss = 0.03621759\n",
      "Iteration 2248, loss = 0.03621461\n",
      "Iteration 2249, loss = 0.03619808\n",
      "Iteration 2250, loss = 0.03619642\n",
      "Iteration 2251, loss = 0.03618589\n",
      "Iteration 2252, loss = 0.03617977\n",
      "Iteration 2253, loss = 0.03616815\n",
      "Iteration 2254, loss = 0.03616330\n",
      "Iteration 2255, loss = 0.03616224\n",
      "Iteration 2256, loss = 0.03614684\n",
      "Iteration 2257, loss = 0.03614213\n",
      "Iteration 2258, loss = 0.03613756\n",
      "Iteration 2259, loss = 0.03612683\n",
      "Iteration 2260, loss = 0.03612470\n",
      "Iteration 2261, loss = 0.03611326\n",
      "Iteration 2262, loss = 0.03611515\n",
      "Iteration 2263, loss = 0.03610245\n",
      "Iteration 2264, loss = 0.03609666\n",
      "Iteration 2265, loss = 0.03608549\n",
      "Iteration 2266, loss = 0.03607928\n",
      "Iteration 2267, loss = 0.03607667\n",
      "Iteration 2268, loss = 0.03606817\n",
      "Iteration 2269, loss = 0.03606216\n",
      "Iteration 2270, loss = 0.03604887\n",
      "Iteration 2271, loss = 0.03603873\n",
      "Iteration 2272, loss = 0.03603563\n",
      "Iteration 2273, loss = 0.03602773\n",
      "Iteration 2274, loss = 0.03601955\n",
      "Iteration 2275, loss = 0.03601215\n",
      "Iteration 2276, loss = 0.03600440\n",
      "Iteration 2277, loss = 0.03600044\n",
      "Iteration 2278, loss = 0.03599052\n",
      "Iteration 2279, loss = 0.03598255\n",
      "Iteration 2280, loss = 0.03598920\n",
      "Iteration 2281, loss = 0.03597211\n",
      "Iteration 2282, loss = 0.03596800\n",
      "Iteration 2283, loss = 0.03596261\n",
      "Iteration 2284, loss = 0.03595122\n",
      "Iteration 2285, loss = 0.03594804\n",
      "Iteration 2286, loss = 0.03593259\n",
      "Iteration 2287, loss = 0.03593301\n",
      "Iteration 2288, loss = 0.03592558\n",
      "Iteration 2289, loss = 0.03591897\n",
      "Iteration 2290, loss = 0.03591347\n",
      "Iteration 2291, loss = 0.03590337\n",
      "Iteration 2292, loss = 0.03589948\n",
      "Iteration 2293, loss = 0.03589057\n",
      "Iteration 2294, loss = 0.03588373\n",
      "Iteration 2295, loss = 0.03587542\n",
      "Iteration 2296, loss = 0.03586949\n",
      "Iteration 2297, loss = 0.03586278\n",
      "Iteration 2298, loss = 0.03585267\n",
      "Iteration 2299, loss = 0.03585398\n",
      "Iteration 2300, loss = 0.03583879\n",
      "Iteration 2301, loss = 0.03583492\n",
      "Iteration 2302, loss = 0.03582950\n",
      "Iteration 2303, loss = 0.03582216\n",
      "Iteration 2304, loss = 0.03581442\n",
      "Iteration 2305, loss = 0.03580737\n",
      "Iteration 2306, loss = 0.03580344\n",
      "Iteration 2307, loss = 0.03578986\n",
      "Iteration 2308, loss = 0.03578891\n",
      "Iteration 2309, loss = 0.03578161\n",
      "Iteration 2310, loss = 0.03577253\n",
      "Iteration 2311, loss = 0.03576834\n",
      "Iteration 2312, loss = 0.03576111\n",
      "Iteration 2313, loss = 0.03575041\n",
      "Iteration 2314, loss = 0.03574383\n",
      "Iteration 2315, loss = 0.03574086\n",
      "Iteration 2316, loss = 0.03573460\n",
      "Iteration 2317, loss = 0.03572706\n",
      "Iteration 2318, loss = 0.03571886\n",
      "Iteration 2319, loss = 0.03572181\n",
      "Iteration 2320, loss = 0.03571134\n",
      "Iteration 2321, loss = 0.03570022\n",
      "Iteration 2322, loss = 0.03569143\n",
      "Iteration 2323, loss = 0.03568822\n",
      "Iteration 2324, loss = 0.03567877\n",
      "Iteration 2325, loss = 0.03567047\n",
      "Iteration 2326, loss = 0.03568020\n",
      "Iteration 2327, loss = 0.03565665\n",
      "Iteration 2328, loss = 0.03565218\n",
      "Iteration 2329, loss = 0.03564655\n",
      "Iteration 2330, loss = 0.03563964\n",
      "Iteration 2331, loss = 0.03562815\n",
      "Iteration 2332, loss = 0.03562856\n",
      "Iteration 2333, loss = 0.03561780\n",
      "Iteration 2334, loss = 0.03561006\n",
      "Iteration 2335, loss = 0.03560551\n",
      "Iteration 2336, loss = 0.03559440\n",
      "Iteration 2337, loss = 0.03559022\n",
      "Iteration 2338, loss = 0.03558326\n",
      "Iteration 2339, loss = 0.03557496\n",
      "Iteration 2340, loss = 0.03556858\n",
      "Iteration 2341, loss = 0.03556421\n",
      "Iteration 2342, loss = 0.03556181\n",
      "Iteration 2343, loss = 0.03555051\n",
      "Iteration 2344, loss = 0.03554173\n",
      "Iteration 2345, loss = 0.03553379\n",
      "Iteration 2346, loss = 0.03553292\n",
      "Iteration 2347, loss = 0.03552757\n",
      "Iteration 2348, loss = 0.03552402\n",
      "Iteration 2349, loss = 0.03551383\n",
      "Iteration 2350, loss = 0.03550704\n",
      "Iteration 2351, loss = 0.03550166\n",
      "Iteration 2352, loss = 0.03548816\n",
      "Iteration 2353, loss = 0.03548641\n",
      "Iteration 2354, loss = 0.03548044\n",
      "Iteration 2355, loss = 0.03546683\n",
      "Iteration 2356, loss = 0.03546454\n",
      "Iteration 2357, loss = 0.03545665\n",
      "Iteration 2358, loss = 0.03546054\n",
      "Iteration 2359, loss = 0.03544008\n",
      "Iteration 2360, loss = 0.03543392\n",
      "Iteration 2361, loss = 0.03543369\n",
      "Iteration 2362, loss = 0.03542127\n",
      "Iteration 2363, loss = 0.03541835\n",
      "Iteration 2364, loss = 0.03540809\n",
      "Iteration 2365, loss = 0.03540655\n",
      "Iteration 2366, loss = 0.03539685\n",
      "Iteration 2367, loss = 0.03539018\n",
      "Iteration 2368, loss = 0.03538795\n",
      "Iteration 2369, loss = 0.03538158\n",
      "Iteration 2370, loss = 0.03537090\n",
      "Iteration 2371, loss = 0.03536268\n",
      "Iteration 2372, loss = 0.03536148\n",
      "Iteration 2373, loss = 0.03535664\n",
      "Iteration 2374, loss = 0.03534046\n",
      "Iteration 2375, loss = 0.03533821\n",
      "Iteration 2376, loss = 0.03533773\n",
      "Iteration 2377, loss = 0.03532880\n",
      "Iteration 2378, loss = 0.03531988\n",
      "Iteration 2379, loss = 0.03531534\n",
      "Iteration 2380, loss = 0.03530575\n",
      "Iteration 2381, loss = 0.03530094\n",
      "Iteration 2382, loss = 0.03529752\n",
      "Iteration 2383, loss = 0.03528823\n",
      "Iteration 2384, loss = 0.03527833\n",
      "Iteration 2385, loss = 0.03527757\n",
      "Iteration 2386, loss = 0.03527126\n",
      "Iteration 2387, loss = 0.03526736\n",
      "Iteration 2388, loss = 0.03525826\n",
      "Iteration 2389, loss = 0.03524646\n",
      "Iteration 2390, loss = 0.03524505\n",
      "Iteration 2391, loss = 0.03524130\n",
      "Iteration 2392, loss = 0.03523047\n",
      "Iteration 2393, loss = 0.03522545\n",
      "Iteration 2394, loss = 0.03521078\n",
      "Iteration 2395, loss = 0.03520996\n",
      "Iteration 2396, loss = 0.03520385\n",
      "Iteration 2397, loss = 0.03520664\n",
      "Iteration 2398, loss = 0.03519650\n",
      "Iteration 2399, loss = 0.03518026\n",
      "Iteration 2400, loss = 0.03517387\n",
      "Iteration 2401, loss = 0.03516796\n",
      "Iteration 2402, loss = 0.03515703\n",
      "Iteration 2403, loss = 0.03515344\n",
      "Iteration 2404, loss = 0.03514942\n",
      "Iteration 2405, loss = 0.03514075\n",
      "Iteration 2406, loss = 0.03513172\n",
      "Iteration 2407, loss = 0.03512910\n",
      "Iteration 2408, loss = 0.03512347\n",
      "Iteration 2409, loss = 0.03511478\n",
      "Iteration 2410, loss = 0.03511180\n",
      "Iteration 2411, loss = 0.03510300\n",
      "Iteration 2412, loss = 0.03509601\n",
      "Iteration 2413, loss = 0.03509122\n",
      "Iteration 2414, loss = 0.03508396\n",
      "Iteration 2415, loss = 0.03507726\n",
      "Iteration 2416, loss = 0.03506768\n",
      "Iteration 2417, loss = 0.03506767\n",
      "Iteration 2418, loss = 0.03505807\n",
      "Iteration 2419, loss = 0.03505168\n",
      "Iteration 2420, loss = 0.03504088\n",
      "Iteration 2421, loss = 0.03503802\n",
      "Iteration 2422, loss = 0.03502830\n",
      "Iteration 2423, loss = 0.03502037\n",
      "Iteration 2424, loss = 0.03501620\n",
      "Iteration 2425, loss = 0.03501256\n",
      "Iteration 2426, loss = 0.03500659\n",
      "Iteration 2427, loss = 0.03499745\n",
      "Iteration 2428, loss = 0.03498994\n",
      "Iteration 2429, loss = 0.03499160\n",
      "Iteration 2430, loss = 0.03497665\n",
      "Iteration 2431, loss = 0.03497041\n",
      "Iteration 2432, loss = 0.03497088\n",
      "Iteration 2433, loss = 0.03496003\n",
      "Iteration 2434, loss = 0.03495229\n",
      "Iteration 2435, loss = 0.03495171\n",
      "Iteration 2436, loss = 0.03494537\n",
      "Iteration 2437, loss = 0.03493193\n",
      "Iteration 2438, loss = 0.03492812\n",
      "Iteration 2439, loss = 0.03492197\n",
      "Iteration 2440, loss = 0.03491662\n",
      "Iteration 2441, loss = 0.03490830\n",
      "Iteration 2442, loss = 0.03490261\n",
      "Iteration 2443, loss = 0.03489690\n",
      "Iteration 2444, loss = 0.03488777\n",
      "Iteration 2445, loss = 0.03488896\n",
      "Iteration 2446, loss = 0.03487360\n",
      "Iteration 2447, loss = 0.03487123\n",
      "Iteration 2448, loss = 0.03486354\n",
      "Iteration 2449, loss = 0.03485367\n",
      "Iteration 2450, loss = 0.03485095\n",
      "Iteration 2451, loss = 0.03484235\n",
      "Iteration 2452, loss = 0.03484570\n",
      "Iteration 2453, loss = 0.03483606\n",
      "Iteration 2454, loss = 0.03482455\n",
      "Iteration 2455, loss = 0.03483138\n",
      "Iteration 2456, loss = 0.03481022\n",
      "Iteration 2457, loss = 0.03480566\n",
      "Iteration 2458, loss = 0.03480004\n",
      "Iteration 2459, loss = 0.03480019\n",
      "Iteration 2460, loss = 0.03479656\n",
      "Iteration 2461, loss = 0.03478687\n",
      "Iteration 2462, loss = 0.03477301\n",
      "Iteration 2463, loss = 0.03476924\n",
      "Iteration 2464, loss = 0.03475890\n",
      "Iteration 2465, loss = 0.03476103\n",
      "Iteration 2466, loss = 0.03475259\n",
      "Iteration 2467, loss = 0.03473960\n",
      "Iteration 2468, loss = 0.03473557\n",
      "Iteration 2469, loss = 0.03472912\n",
      "Iteration 2470, loss = 0.03472545\n",
      "Iteration 2471, loss = 0.03471767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2472, loss = 0.03471826\n",
      "Iteration 2473, loss = 0.03470671\n",
      "Iteration 2474, loss = 0.03470080\n",
      "Iteration 2475, loss = 0.03469927\n",
      "Iteration 2476, loss = 0.03468487\n",
      "Iteration 2477, loss = 0.03467782\n",
      "Iteration 2478, loss = 0.03467682\n",
      "Iteration 2479, loss = 0.03466739\n",
      "Iteration 2480, loss = 0.03465709\n",
      "Iteration 2481, loss = 0.03465377\n",
      "Iteration 2482, loss = 0.03464690\n",
      "Iteration 2483, loss = 0.03464531\n",
      "Iteration 2484, loss = 0.03463147\n",
      "Iteration 2485, loss = 0.03463116\n",
      "Iteration 2486, loss = 0.03462144\n",
      "Iteration 2487, loss = 0.03461795\n",
      "Iteration 2488, loss = 0.03460827\n",
      "Iteration 2489, loss = 0.03460635\n",
      "Iteration 2490, loss = 0.03460149\n",
      "Iteration 2491, loss = 0.03459047\n",
      "Iteration 2492, loss = 0.03458388\n",
      "Iteration 2493, loss = 0.03457838\n",
      "Iteration 2494, loss = 0.03457106\n",
      "Iteration 2495, loss = 0.03456366\n",
      "Iteration 2496, loss = 0.03455626\n",
      "Iteration 2497, loss = 0.03455607\n",
      "Iteration 2498, loss = 0.03454612\n",
      "Iteration 2499, loss = 0.03454468\n",
      "Iteration 2500, loss = 0.03453664\n",
      "Iteration 2501, loss = 0.03453356\n",
      "Iteration 2502, loss = 0.03452313\n",
      "Iteration 2503, loss = 0.03451502\n",
      "Iteration 2504, loss = 0.03451166\n",
      "Iteration 2505, loss = 0.03450689\n",
      "Iteration 2506, loss = 0.03450893\n",
      "Iteration 2507, loss = 0.03449537\n",
      "Iteration 2508, loss = 0.03449050\n",
      "Iteration 2509, loss = 0.03448704\n",
      "Iteration 2510, loss = 0.03447591\n",
      "Iteration 2511, loss = 0.03446636\n",
      "Iteration 2512, loss = 0.03446365\n",
      "Iteration 2513, loss = 0.03445871\n",
      "Iteration 2514, loss = 0.03445634\n",
      "Iteration 2515, loss = 0.03444484\n",
      "Iteration 2516, loss = 0.03443824\n",
      "Iteration 2517, loss = 0.03443489\n",
      "Iteration 2518, loss = 0.03442544\n",
      "Iteration 2519, loss = 0.03441870\n",
      "Iteration 2520, loss = 0.03441389\n",
      "Iteration 2521, loss = 0.03440611\n",
      "Iteration 2522, loss = 0.03439838\n",
      "Iteration 2523, loss = 0.03439831\n",
      "Iteration 2524, loss = 0.03438910\n",
      "Iteration 2525, loss = 0.03438354\n",
      "Iteration 2526, loss = 0.03437586\n",
      "Iteration 2527, loss = 0.03437080\n",
      "Iteration 2528, loss = 0.03436829\n",
      "Iteration 2529, loss = 0.03436352\n",
      "Iteration 2530, loss = 0.03434931\n",
      "Iteration 2531, loss = 0.03434642\n",
      "Iteration 2532, loss = 0.03433944\n",
      "Iteration 2533, loss = 0.03433812\n",
      "Iteration 2534, loss = 0.03433297\n",
      "Iteration 2535, loss = 0.03432219\n",
      "Iteration 2536, loss = 0.03431793\n",
      "Iteration 2537, loss = 0.03431100\n",
      "Iteration 2538, loss = 0.03431064\n",
      "Iteration 2539, loss = 0.03429755\n",
      "Iteration 2540, loss = 0.03429733\n",
      "Iteration 2541, loss = 0.03429032\n",
      "Iteration 2542, loss = 0.03428597\n",
      "Iteration 2543, loss = 0.03427869\n",
      "Iteration 2544, loss = 0.03427062\n",
      "Iteration 2545, loss = 0.03426400\n",
      "Iteration 2546, loss = 0.03425434\n",
      "Iteration 2547, loss = 0.03425085\n",
      "Iteration 2548, loss = 0.03424793\n",
      "Iteration 2549, loss = 0.03424670\n",
      "Iteration 2550, loss = 0.03423862\n",
      "Iteration 2551, loss = 0.03423198\n",
      "Iteration 2552, loss = 0.03421922\n",
      "Iteration 2553, loss = 0.03421829\n",
      "Iteration 2554, loss = 0.03420880\n",
      "Iteration 2555, loss = 0.03420863\n",
      "Iteration 2556, loss = 0.03419704\n",
      "Iteration 2557, loss = 0.03418889\n",
      "Iteration 2558, loss = 0.03418496\n",
      "Iteration 2559, loss = 0.03417863\n",
      "Iteration 2560, loss = 0.03417141\n",
      "Iteration 2561, loss = 0.03416724\n",
      "Iteration 2562, loss = 0.03416175\n",
      "Iteration 2563, loss = 0.03416234\n",
      "Iteration 2564, loss = 0.03414960\n",
      "Iteration 2565, loss = 0.03414461\n",
      "Iteration 2566, loss = 0.03413863\n",
      "Iteration 2567, loss = 0.03414104\n",
      "Iteration 2568, loss = 0.03412369\n",
      "Iteration 2569, loss = 0.03412226\n",
      "Iteration 2570, loss = 0.03411481\n",
      "Iteration 2571, loss = 0.03410894\n",
      "Iteration 2572, loss = 0.03410525\n",
      "Iteration 2573, loss = 0.03409787\n",
      "Iteration 2574, loss = 0.03409241\n",
      "Iteration 2575, loss = 0.03408369\n",
      "Iteration 2576, loss = 0.03408182\n",
      "Iteration 2577, loss = 0.03407204\n",
      "Iteration 2578, loss = 0.03407254\n",
      "Iteration 2579, loss = 0.03406493\n",
      "Iteration 2580, loss = 0.03406358\n",
      "Iteration 2581, loss = 0.03405052\n",
      "Iteration 2582, loss = 0.03404383\n",
      "Iteration 2583, loss = 0.03404058\n",
      "Iteration 2584, loss = 0.03403435\n",
      "Iteration 2585, loss = 0.03402583\n",
      "Iteration 2586, loss = 0.03401902\n",
      "Iteration 2587, loss = 0.03401687\n",
      "Iteration 2588, loss = 0.03400662\n",
      "Iteration 2589, loss = 0.03400846\n",
      "Iteration 2590, loss = 0.03400217\n",
      "Iteration 2591, loss = 0.03399161\n",
      "Iteration 2592, loss = 0.03398522\n",
      "Iteration 2593, loss = 0.03397694\n",
      "Iteration 2594, loss = 0.03397394\n",
      "Iteration 2595, loss = 0.03396822\n",
      "Iteration 2596, loss = 0.03396229\n",
      "Iteration 2597, loss = 0.03395654\n",
      "Iteration 2598, loss = 0.03395353\n",
      "Iteration 2599, loss = 0.03394148\n",
      "Iteration 2600, loss = 0.03393551\n",
      "Iteration 2601, loss = 0.03393004\n",
      "Iteration 2602, loss = 0.03392311\n",
      "Iteration 2603, loss = 0.03391966\n",
      "Iteration 2604, loss = 0.03391294\n",
      "Iteration 2605, loss = 0.03391091\n",
      "Iteration 2606, loss = 0.03389950\n",
      "Iteration 2607, loss = 0.03389855\n",
      "Iteration 2608, loss = 0.03389108\n",
      "Iteration 2609, loss = 0.03388683\n",
      "Iteration 2610, loss = 0.03387784\n",
      "Iteration 2611, loss = 0.03386977\n",
      "Iteration 2612, loss = 0.03386996\n",
      "Iteration 2613, loss = 0.03386697\n",
      "Iteration 2614, loss = 0.03386018\n",
      "Iteration 2615, loss = 0.03384723\n",
      "Iteration 2616, loss = 0.03384191\n",
      "Iteration 2617, loss = 0.03383470\n",
      "Iteration 2618, loss = 0.03383417\n",
      "Iteration 2619, loss = 0.03382439\n",
      "Iteration 2620, loss = 0.03381898\n",
      "Iteration 2621, loss = 0.03381281\n",
      "Iteration 2622, loss = 0.03380114\n",
      "Iteration 2623, loss = 0.03380401\n",
      "Iteration 2624, loss = 0.03379045\n",
      "Iteration 2625, loss = 0.03379249\n",
      "Iteration 2626, loss = 0.03378285\n",
      "Iteration 2627, loss = 0.03377869\n",
      "Iteration 2628, loss = 0.03378200\n",
      "Iteration 2629, loss = 0.03375906\n",
      "Iteration 2630, loss = 0.03376059\n",
      "Iteration 2631, loss = 0.03375526\n",
      "Iteration 2632, loss = 0.03374950\n",
      "Iteration 2633, loss = 0.03374516\n",
      "Iteration 2634, loss = 0.03374381\n",
      "Iteration 2635, loss = 0.03373251\n",
      "Iteration 2636, loss = 0.03372947\n",
      "Iteration 2637, loss = 0.03372072\n",
      "Iteration 2638, loss = 0.03371959\n",
      "Iteration 2639, loss = 0.03370756\n",
      "Iteration 2640, loss = 0.03370314\n",
      "Iteration 2641, loss = 0.03370136\n",
      "Iteration 2642, loss = 0.03368882\n",
      "Iteration 2643, loss = 0.03368725\n",
      "Iteration 2644, loss = 0.03368112\n",
      "Iteration 2645, loss = 0.03367600\n",
      "Iteration 2646, loss = 0.03366372\n",
      "Iteration 2647, loss = 0.03366656\n",
      "Iteration 2648, loss = 0.03365317\n",
      "Iteration 2649, loss = 0.03365502\n",
      "Iteration 2650, loss = 0.03364435\n",
      "Iteration 2651, loss = 0.03363629\n",
      "Iteration 2652, loss = 0.03363447\n",
      "Iteration 2653, loss = 0.03363267\n",
      "Iteration 2654, loss = 0.03362216\n",
      "Iteration 2655, loss = 0.03361359\n",
      "Iteration 2656, loss = 0.03361068\n",
      "Iteration 2657, loss = 0.03360969\n",
      "Iteration 2658, loss = 0.03359828\n",
      "Iteration 2659, loss = 0.03359972\n",
      "Iteration 2660, loss = 0.03358570\n",
      "Iteration 2661, loss = 0.03358220\n",
      "Iteration 2662, loss = 0.03357512\n",
      "Iteration 2663, loss = 0.03357513\n",
      "Iteration 2664, loss = 0.03356713\n",
      "Iteration 2665, loss = 0.03356290\n",
      "Iteration 2666, loss = 0.03355319\n",
      "Iteration 2667, loss = 0.03354946\n",
      "Iteration 2668, loss = 0.03354152\n",
      "Iteration 2669, loss = 0.03354456\n",
      "Iteration 2670, loss = 0.03352831\n",
      "Iteration 2671, loss = 0.03353459\n",
      "Iteration 2672, loss = 0.03351365\n",
      "Iteration 2673, loss = 0.03351101\n",
      "Iteration 2674, loss = 0.03350834\n",
      "Iteration 2675, loss = 0.03350312\n",
      "Iteration 2676, loss = 0.03349592\n",
      "Iteration 2677, loss = 0.03348833\n",
      "Iteration 2678, loss = 0.03348665\n",
      "Iteration 2679, loss = 0.03348236\n",
      "Iteration 2680, loss = 0.03347016\n",
      "Iteration 2681, loss = 0.03346278\n",
      "Iteration 2682, loss = 0.03346308\n",
      "Iteration 2683, loss = 0.03345955\n",
      "Iteration 2684, loss = 0.03344453\n",
      "Iteration 2685, loss = 0.03344710\n",
      "Iteration 2686, loss = 0.03344489\n",
      "Iteration 2687, loss = 0.03342660\n",
      "Iteration 2688, loss = 0.03342281\n",
      "Iteration 2689, loss = 0.03342551\n",
      "Iteration 2690, loss = 0.03341501\n",
      "Iteration 2691, loss = 0.03340633\n",
      "Iteration 2692, loss = 0.03340754\n",
      "Iteration 2693, loss = 0.03339754\n",
      "Iteration 2694, loss = 0.03338941\n",
      "Iteration 2695, loss = 0.03338431\n",
      "Iteration 2696, loss = 0.03337951\n",
      "Iteration 2697, loss = 0.03337070\n",
      "Iteration 2698, loss = 0.03336786\n",
      "Iteration 2699, loss = 0.03336339\n",
      "Iteration 2700, loss = 0.03335407\n",
      "Iteration 2701, loss = 0.03335266\n",
      "Iteration 2702, loss = 0.03334626\n",
      "Iteration 2703, loss = 0.03334443\n",
      "Iteration 2704, loss = 0.03334288\n",
      "Iteration 2705, loss = 0.03332870\n",
      "Iteration 2706, loss = 0.03332187\n",
      "Iteration 2707, loss = 0.03331735\n",
      "Iteration 2708, loss = 0.03331932\n",
      "Iteration 2709, loss = 0.03330777\n",
      "Iteration 2710, loss = 0.03329720\n",
      "Iteration 2711, loss = 0.03329446\n",
      "Iteration 2712, loss = 0.03328895\n",
      "Iteration 2713, loss = 0.03327960\n",
      "Iteration 2714, loss = 0.03327515\n",
      "Iteration 2715, loss = 0.03326773\n",
      "Iteration 2716, loss = 0.03326254\n",
      "Iteration 2717, loss = 0.03326092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2718, loss = 0.03324950\n",
      "Iteration 2719, loss = 0.03324413\n",
      "Iteration 2720, loss = 0.03325269\n",
      "Iteration 2721, loss = 0.03323618\n",
      "Iteration 2722, loss = 0.03323066\n",
      "Iteration 2723, loss = 0.03322666\n",
      "Iteration 2724, loss = 0.03322552\n",
      "Iteration 2725, loss = 0.03321297\n",
      "Iteration 2726, loss = 0.03320610\n",
      "Iteration 2727, loss = 0.03320681\n",
      "Iteration 2728, loss = 0.03319502\n",
      "Iteration 2729, loss = 0.03318956\n",
      "Iteration 2730, loss = 0.03318440\n",
      "Iteration 2731, loss = 0.03317734\n",
      "Iteration 2732, loss = 0.03317137\n",
      "Iteration 2733, loss = 0.03316737\n",
      "Iteration 2734, loss = 0.03316571\n",
      "Iteration 2735, loss = 0.03315744\n",
      "Iteration 2736, loss = 0.03314923\n",
      "Iteration 2737, loss = 0.03314456\n",
      "Iteration 2738, loss = 0.03314276\n",
      "Iteration 2739, loss = 0.03313505\n",
      "Iteration 2740, loss = 0.03312562\n",
      "Iteration 2741, loss = 0.03312179\n",
      "Iteration 2742, loss = 0.03311531\n",
      "Iteration 2743, loss = 0.03311162\n",
      "Iteration 2744, loss = 0.03310233\n",
      "Iteration 2745, loss = 0.03310541\n",
      "Iteration 2746, loss = 0.03309374\n",
      "Iteration 2747, loss = 0.03308797\n",
      "Iteration 2748, loss = 0.03308480\n",
      "Iteration 2749, loss = 0.03307307\n",
      "Iteration 2750, loss = 0.03306967\n",
      "Iteration 2751, loss = 0.03306629\n",
      "Iteration 2752, loss = 0.03306058\n",
      "Iteration 2753, loss = 0.03305598\n",
      "Iteration 2754, loss = 0.03304638\n",
      "Iteration 2755, loss = 0.03304184\n",
      "Iteration 2756, loss = 0.03303276\n",
      "Iteration 2757, loss = 0.03303268\n",
      "Iteration 2758, loss = 0.03302893\n",
      "Iteration 2759, loss = 0.03302440\n",
      "Iteration 2760, loss = 0.03301114\n",
      "Iteration 2761, loss = 0.03300712\n",
      "Iteration 2762, loss = 0.03300311\n",
      "Iteration 2763, loss = 0.03299521\n",
      "Iteration 2764, loss = 0.03299219\n",
      "Iteration 2765, loss = 0.03298698\n",
      "Iteration 2766, loss = 0.03297832\n",
      "Iteration 2767, loss = 0.03297168\n",
      "Iteration 2768, loss = 0.03296598\n",
      "Iteration 2769, loss = 0.03295785\n",
      "Iteration 2770, loss = 0.03295408\n",
      "Iteration 2771, loss = 0.03295115\n",
      "Iteration 2772, loss = 0.03295226\n",
      "Iteration 2773, loss = 0.03293738\n",
      "Iteration 2774, loss = 0.03293774\n",
      "Iteration 2775, loss = 0.03293812\n",
      "Iteration 2776, loss = 0.03292804\n",
      "Iteration 2777, loss = 0.03291493\n",
      "Iteration 2778, loss = 0.03291104\n",
      "Iteration 2779, loss = 0.03290452\n",
      "Iteration 2780, loss = 0.03289306\n",
      "Iteration 2781, loss = 0.03289175\n",
      "Iteration 2782, loss = 0.03289263\n",
      "Iteration 2783, loss = 0.03288025\n",
      "Iteration 2784, loss = 0.03288365\n",
      "Iteration 2785, loss = 0.03286813\n",
      "Iteration 2786, loss = 0.03286306\n",
      "Iteration 2787, loss = 0.03285811\n",
      "Iteration 2788, loss = 0.03285499\n",
      "Iteration 2789, loss = 0.03284858\n",
      "Iteration 2790, loss = 0.03284360\n",
      "Iteration 2791, loss = 0.03283852\n",
      "Iteration 2792, loss = 0.03282594\n",
      "Iteration 2793, loss = 0.03282834\n",
      "Iteration 2794, loss = 0.03282463\n",
      "Iteration 2795, loss = 0.03281212\n",
      "Iteration 2796, loss = 0.03281290\n",
      "Iteration 2797, loss = 0.03280172\n",
      "Iteration 2798, loss = 0.03280010\n",
      "Iteration 2799, loss = 0.03278826\n",
      "Iteration 2800, loss = 0.03278336\n",
      "Iteration 2801, loss = 0.03277766\n",
      "Iteration 2802, loss = 0.03277219\n",
      "Iteration 2803, loss = 0.03276646\n",
      "Iteration 2804, loss = 0.03275849\n",
      "Iteration 2805, loss = 0.03275769\n",
      "Iteration 2806, loss = 0.03275218\n",
      "Iteration 2807, loss = 0.03274407\n",
      "Iteration 2808, loss = 0.03274106\n",
      "Iteration 2809, loss = 0.03272967\n",
      "Iteration 2810, loss = 0.03272928\n",
      "Iteration 2811, loss = 0.03272126\n",
      "Iteration 2812, loss = 0.03271469\n",
      "Iteration 2813, loss = 0.03271307\n",
      "Iteration 2814, loss = 0.03270190\n",
      "Iteration 2815, loss = 0.03270508\n",
      "Iteration 2816, loss = 0.03270179\n",
      "Iteration 2817, loss = 0.03269340\n",
      "Iteration 2818, loss = 0.03268580\n",
      "Iteration 2819, loss = 0.03268042\n",
      "Iteration 2820, loss = 0.03267362\n",
      "Iteration 2821, loss = 0.03266857\n",
      "Iteration 2822, loss = 0.03266070\n",
      "Iteration 2823, loss = 0.03265682\n",
      "Iteration 2824, loss = 0.03264797\n",
      "Iteration 2825, loss = 0.03264287\n",
      "Iteration 2826, loss = 0.03264005\n",
      "Iteration 2827, loss = 0.03263578\n",
      "Iteration 2828, loss = 0.03263010\n",
      "Iteration 2829, loss = 0.03262210\n",
      "Iteration 2830, loss = 0.03261654\n",
      "Iteration 2831, loss = 0.03262048\n",
      "Iteration 2832, loss = 0.03260896\n",
      "Iteration 2833, loss = 0.03260554\n",
      "Iteration 2834, loss = 0.03259236\n",
      "Iteration 2835, loss = 0.03259003\n",
      "Iteration 2836, loss = 0.03258762\n",
      "Iteration 2837, loss = 0.03257446\n",
      "Iteration 2838, loss = 0.03257289\n",
      "Iteration 2839, loss = 0.03257015\n",
      "Iteration 2840, loss = 0.03256194\n",
      "Iteration 2841, loss = 0.03256353\n",
      "Iteration 2842, loss = 0.03254709\n",
      "Iteration 2843, loss = 0.03254128\n",
      "Iteration 2844, loss = 0.03253928\n",
      "Iteration 2845, loss = 0.03253849\n",
      "Iteration 2846, loss = 0.03253127\n",
      "Iteration 2847, loss = 0.03252719\n",
      "Iteration 2848, loss = 0.03251929\n",
      "Iteration 2849, loss = 0.03251005\n",
      "Iteration 2850, loss = 0.03250660\n",
      "Iteration 2851, loss = 0.03250045\n",
      "Iteration 2852, loss = 0.03249141\n",
      "Iteration 2853, loss = 0.03249253\n",
      "Iteration 2854, loss = 0.03247809\n",
      "Iteration 2855, loss = 0.03247416\n",
      "Iteration 2856, loss = 0.03247518\n",
      "Iteration 2857, loss = 0.03246765\n",
      "Iteration 2858, loss = 0.03246652\n",
      "Iteration 2859, loss = 0.03245325\n",
      "Iteration 2860, loss = 0.03245194\n",
      "Iteration 2861, loss = 0.03244416\n",
      "Iteration 2862, loss = 0.03243906\n",
      "Iteration 2863, loss = 0.03243092\n",
      "Iteration 2864, loss = 0.03242957\n",
      "Iteration 2865, loss = 0.03241985\n",
      "Iteration 2866, loss = 0.03241534\n",
      "Iteration 2867, loss = 0.03241133\n",
      "Iteration 2868, loss = 0.03240543\n",
      "Iteration 2869, loss = 0.03240107\n",
      "Iteration 2870, loss = 0.03239139\n",
      "Iteration 2871, loss = 0.03239204\n",
      "Iteration 2872, loss = 0.03238290\n",
      "Iteration 2873, loss = 0.03238084\n",
      "Iteration 2874, loss = 0.03237092\n",
      "Iteration 2875, loss = 0.03236980\n",
      "Iteration 2876, loss = 0.03236411\n",
      "Iteration 2877, loss = 0.03235087\n",
      "Iteration 2878, loss = 0.03234896\n",
      "Iteration 2879, loss = 0.03234759\n",
      "Iteration 2880, loss = 0.03233823\n",
      "Iteration 2881, loss = 0.03233240\n",
      "Iteration 2882, loss = 0.03232789\n",
      "Iteration 2883, loss = 0.03232226\n",
      "Iteration 2884, loss = 0.03231784\n",
      "Iteration 2885, loss = 0.03231180\n",
      "Iteration 2886, loss = 0.03230378\n",
      "Iteration 2887, loss = 0.03229941\n",
      "Iteration 2888, loss = 0.03229987\n",
      "Iteration 2889, loss = 0.03228630\n",
      "Iteration 2890, loss = 0.03228270\n",
      "Iteration 2891, loss = 0.03227760\n",
      "Iteration 2892, loss = 0.03227443\n",
      "Iteration 2893, loss = 0.03226496\n",
      "Iteration 2894, loss = 0.03226745\n",
      "Iteration 2895, loss = 0.03225712\n",
      "Iteration 2896, loss = 0.03224500\n",
      "Iteration 2897, loss = 0.03224315\n",
      "Iteration 2898, loss = 0.03223758\n",
      "Iteration 2899, loss = 0.03223285\n",
      "Iteration 2900, loss = 0.03222985\n",
      "Iteration 2901, loss = 0.03222449\n",
      "Iteration 2902, loss = 0.03220920\n",
      "Iteration 2903, loss = 0.03221277\n",
      "Iteration 2904, loss = 0.03220071\n",
      "Iteration 2905, loss = 0.03220150\n",
      "Iteration 2906, loss = 0.03219081\n",
      "Iteration 2907, loss = 0.03218945\n",
      "Iteration 2908, loss = 0.03217964\n",
      "Iteration 2909, loss = 0.03217748\n",
      "Iteration 2910, loss = 0.03217184\n",
      "Iteration 2911, loss = 0.03217070\n",
      "Iteration 2912, loss = 0.03216449\n",
      "Iteration 2913, loss = 0.03215881\n",
      "Iteration 2914, loss = 0.03214587\n",
      "Iteration 2915, loss = 0.03214390\n",
      "Iteration 2916, loss = 0.03214034\n",
      "Iteration 2917, loss = 0.03213124\n",
      "Iteration 2918, loss = 0.03212774\n",
      "Iteration 2919, loss = 0.03212308\n",
      "Iteration 2920, loss = 0.03211342\n",
      "Iteration 2921, loss = 0.03211055\n",
      "Iteration 2922, loss = 0.03210582\n",
      "Iteration 2923, loss = 0.03210615\n",
      "Iteration 2924, loss = 0.03209480\n",
      "Iteration 2925, loss = 0.03209198\n",
      "Iteration 2926, loss = 0.03208200\n",
      "Iteration 2927, loss = 0.03207827\n",
      "Iteration 2928, loss = 0.03207765\n",
      "Iteration 2929, loss = 0.03206448\n",
      "Iteration 2930, loss = 0.03206897\n",
      "Iteration 2931, loss = 0.03205705\n",
      "Iteration 2932, loss = 0.03206195\n",
      "Iteration 2933, loss = 0.03204631\n",
      "Iteration 2934, loss = 0.03204591\n",
      "Iteration 2935, loss = 0.03203521\n",
      "Iteration 2936, loss = 0.03203333\n",
      "Iteration 2937, loss = 0.03202622\n",
      "Iteration 2938, loss = 0.03202203\n",
      "Iteration 2939, loss = 0.03201831\n",
      "Iteration 2940, loss = 0.03200781\n",
      "Iteration 2941, loss = 0.03200262\n",
      "Iteration 2942, loss = 0.03199506\n",
      "Iteration 2943, loss = 0.03199380\n",
      "Iteration 2944, loss = 0.03198502\n",
      "Iteration 2945, loss = 0.03199412\n",
      "Iteration 2946, loss = 0.03197559\n",
      "Iteration 2947, loss = 0.03196756\n",
      "Iteration 2948, loss = 0.03197294\n",
      "Iteration 2949, loss = 0.03196054\n",
      "Iteration 2950, loss = 0.03195347\n",
      "Iteration 2951, loss = 0.03194922\n",
      "Iteration 2952, loss = 0.03194150\n",
      "Iteration 2953, loss = 0.03193938\n",
      "Iteration 2954, loss = 0.03193212\n",
      "Iteration 2955, loss = 0.03192839\n",
      "Iteration 2956, loss = 0.03192484\n",
      "Iteration 2957, loss = 0.03191685\n",
      "Iteration 2958, loss = 0.03191081\n",
      "Iteration 2959, loss = 0.03190180\n",
      "Iteration 2960, loss = 0.03190109\n",
      "Iteration 2961, loss = 0.03190104\n",
      "Iteration 2962, loss = 0.03189028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2963, loss = 0.03188339\n",
      "Iteration 2964, loss = 0.03188039\n",
      "Iteration 2965, loss = 0.03187159\n",
      "Iteration 2966, loss = 0.03187169\n",
      "Iteration 2967, loss = 0.03185843\n",
      "Iteration 2968, loss = 0.03186005\n",
      "Iteration 2969, loss = 0.03185640\n",
      "Iteration 2970, loss = 0.03185085\n",
      "Iteration 2971, loss = 0.03184544\n",
      "Iteration 2972, loss = 0.03183325\n",
      "Iteration 2973, loss = 0.03183050\n",
      "Iteration 2974, loss = 0.03182504\n",
      "Iteration 2975, loss = 0.03182225\n",
      "Iteration 2976, loss = 0.03181221\n",
      "Iteration 2977, loss = 0.03180729\n",
      "Iteration 2978, loss = 0.03180361\n",
      "Iteration 2979, loss = 0.03180471\n",
      "Iteration 2980, loss = 0.03179682\n",
      "Iteration 2981, loss = 0.03178786\n",
      "Iteration 2982, loss = 0.03178376\n",
      "Iteration 2983, loss = 0.03177752\n",
      "Iteration 2984, loss = 0.03177658\n",
      "Iteration 2985, loss = 0.03177137\n",
      "Iteration 2986, loss = 0.03175952\n",
      "Iteration 2987, loss = 0.03175845\n",
      "Iteration 2988, loss = 0.03175000\n",
      "Iteration 2989, loss = 0.03174382\n",
      "Iteration 2990, loss = 0.03173760\n",
      "Iteration 2991, loss = 0.03173136\n",
      "Iteration 2992, loss = 0.03172937\n",
      "Iteration 2993, loss = 0.03172684\n",
      "Iteration 2994, loss = 0.03171388\n",
      "Iteration 2995, loss = 0.03171172\n",
      "Iteration 2996, loss = 0.03170614\n",
      "Iteration 2997, loss = 0.03170117\n",
      "Iteration 2998, loss = 0.03169578\n",
      "Iteration 2999, loss = 0.03169183\n",
      "Iteration 3000, loss = 0.03168909\n",
      "Iteration 3001, loss = 0.03168077\n",
      "Iteration 3002, loss = 0.03167260\n",
      "Iteration 3003, loss = 0.03167008\n",
      "Iteration 3004, loss = 0.03166300\n",
      "Iteration 3005, loss = 0.03165752\n",
      "Iteration 3006, loss = 0.03165348\n",
      "Iteration 3007, loss = 0.03164641\n",
      "Iteration 3008, loss = 0.03164429\n",
      "Iteration 3009, loss = 0.03163844\n",
      "Iteration 3010, loss = 0.03162911\n",
      "Iteration 3011, loss = 0.03162433\n",
      "Iteration 3012, loss = 0.03161910\n",
      "Iteration 3013, loss = 0.03161410\n",
      "Iteration 3014, loss = 0.03161039\n",
      "Iteration 3015, loss = 0.03160348\n",
      "Iteration 3016, loss = 0.03159921\n",
      "Iteration 3017, loss = 0.03159501\n",
      "Iteration 3018, loss = 0.03158991\n",
      "Iteration 3019, loss = 0.03159094\n",
      "Iteration 3020, loss = 0.03158205\n",
      "Iteration 3021, loss = 0.03157466\n",
      "Iteration 3022, loss = 0.03156898\n",
      "Iteration 3023, loss = 0.03156379\n",
      "Iteration 3024, loss = 0.03155722\n",
      "Iteration 3025, loss = 0.03155084\n",
      "Iteration 3026, loss = 0.03154638\n",
      "Iteration 3027, loss = 0.03154519\n",
      "Iteration 3028, loss = 0.03153180\n",
      "Iteration 3029, loss = 0.03152821\n",
      "Iteration 3030, loss = 0.03152468\n",
      "Iteration 3031, loss = 0.03151880\n",
      "Iteration 3032, loss = 0.03151603\n",
      "Iteration 3033, loss = 0.03150909\n",
      "Iteration 3034, loss = 0.03150260\n",
      "Iteration 3035, loss = 0.03150604\n",
      "Iteration 3036, loss = 0.03149675\n",
      "Iteration 3037, loss = 0.03148806\n",
      "Iteration 3038, loss = 0.03148302\n",
      "Iteration 3039, loss = 0.03147867\n",
      "Iteration 3040, loss = 0.03147550\n",
      "Iteration 3041, loss = 0.03146980\n",
      "Iteration 3042, loss = 0.03146099\n",
      "Iteration 3043, loss = 0.03145362\n",
      "Iteration 3044, loss = 0.03145340\n",
      "Iteration 3045, loss = 0.03144672\n",
      "Iteration 3046, loss = 0.03143375\n",
      "Iteration 3047, loss = 0.03143652\n",
      "Iteration 3048, loss = 0.03142955\n",
      "Iteration 3049, loss = 0.03141921\n",
      "Iteration 3050, loss = 0.03141856\n",
      "Iteration 3051, loss = 0.03141307\n",
      "Iteration 3052, loss = 0.03140997\n",
      "Iteration 3053, loss = 0.03140154\n",
      "Iteration 3054, loss = 0.03139386\n",
      "Iteration 3055, loss = 0.03139282\n",
      "Iteration 3056, loss = 0.03139668\n",
      "Iteration 3057, loss = 0.03138614\n",
      "Iteration 3058, loss = 0.03137833\n",
      "Iteration 3059, loss = 0.03137596\n",
      "Iteration 3060, loss = 0.03136953\n",
      "Iteration 3061, loss = 0.03136393\n",
      "Iteration 3062, loss = 0.03136339\n",
      "Iteration 3063, loss = 0.03134801\n",
      "Iteration 3064, loss = 0.03134537\n",
      "Iteration 3065, loss = 0.03134373\n",
      "Iteration 3066, loss = 0.03133563\n",
      "Iteration 3067, loss = 0.03133230\n",
      "Iteration 3068, loss = 0.03132440\n",
      "Iteration 3069, loss = 0.03132829\n",
      "Iteration 3070, loss = 0.03131669\n",
      "Iteration 3071, loss = 0.03131421\n",
      "Iteration 3072, loss = 0.03130915\n",
      "Iteration 3073, loss = 0.03131071\n",
      "Iteration 3074, loss = 0.03130027\n",
      "Iteration 3075, loss = 0.03129160\n",
      "Iteration 3076, loss = 0.03128716\n",
      "Iteration 3077, loss = 0.03128463\n",
      "Iteration 3078, loss = 0.03127680\n",
      "Iteration 3079, loss = 0.03126749\n",
      "Iteration 3080, loss = 0.03126833\n",
      "Iteration 3081, loss = 0.03126097\n",
      "Iteration 3082, loss = 0.03125315\n",
      "Iteration 3083, loss = 0.03124548\n",
      "Iteration 3084, loss = 0.03124226\n",
      "Iteration 3085, loss = 0.03123531\n",
      "Iteration 3086, loss = 0.03123337\n",
      "Iteration 3087, loss = 0.03122580\n",
      "Iteration 3088, loss = 0.03122734\n",
      "Iteration 3089, loss = 0.03121379\n",
      "Iteration 3090, loss = 0.03120997\n",
      "Iteration 3091, loss = 0.03120861\n",
      "Iteration 3092, loss = 0.03120225\n",
      "Iteration 3093, loss = 0.03119663\n",
      "Iteration 3094, loss = 0.03119217\n",
      "Iteration 3095, loss = 0.03118433\n",
      "Iteration 3096, loss = 0.03117817\n",
      "Iteration 3097, loss = 0.03117132\n",
      "Iteration 3098, loss = 0.03117150\n",
      "Iteration 3099, loss = 0.03117241\n",
      "Iteration 3100, loss = 0.03116210\n",
      "Iteration 3101, loss = 0.03116005\n",
      "Iteration 3102, loss = 0.03114814\n",
      "Iteration 3103, loss = 0.03115626\n",
      "Iteration 3104, loss = 0.03114169\n",
      "Iteration 3105, loss = 0.03113216\n",
      "Iteration 3106, loss = 0.03113231\n",
      "Iteration 3107, loss = 0.03112484\n",
      "Iteration 3108, loss = 0.03111983\n",
      "Iteration 3109, loss = 0.03111790\n",
      "Iteration 3110, loss = 0.03111083\n",
      "Iteration 3111, loss = 0.03110720\n",
      "Iteration 3112, loss = 0.03109512\n",
      "Iteration 3113, loss = 0.03109463\n",
      "Iteration 3114, loss = 0.03108911\n",
      "Iteration 3115, loss = 0.03108768\n",
      "Iteration 3116, loss = 0.03108656\n",
      "Iteration 3117, loss = 0.03107156\n",
      "Iteration 3118, loss = 0.03106688\n",
      "Iteration 3119, loss = 0.03106260\n",
      "Iteration 3120, loss = 0.03107030\n",
      "Iteration 3121, loss = 0.03105188\n",
      "Iteration 3122, loss = 0.03104958\n",
      "Iteration 3123, loss = 0.03104580\n",
      "Iteration 3124, loss = 0.03104437\n",
      "Iteration 3125, loss = 0.03103586\n",
      "Iteration 3126, loss = 0.03102708\n",
      "Iteration 3127, loss = 0.03102630\n",
      "Iteration 3128, loss = 0.03101627\n",
      "Iteration 3129, loss = 0.03101565\n",
      "Iteration 3130, loss = 0.03100854\n",
      "Iteration 3131, loss = 0.03099950\n",
      "Iteration 3132, loss = 0.03099713\n",
      "Iteration 3133, loss = 0.03099009\n",
      "Iteration 3134, loss = 0.03098734\n",
      "Iteration 3135, loss = 0.03098330\n",
      "Iteration 3136, loss = 0.03097866\n",
      "Iteration 3137, loss = 0.03098422\n",
      "Iteration 3138, loss = 0.03096782\n",
      "Iteration 3139, loss = 0.03096319\n",
      "Iteration 3140, loss = 0.03095382\n",
      "Iteration 3141, loss = 0.03094778\n",
      "Iteration 3142, loss = 0.03094536\n",
      "Iteration 3143, loss = 0.03094135\n",
      "Iteration 3144, loss = 0.03093694\n",
      "Iteration 3145, loss = 0.03093106\n",
      "Iteration 3146, loss = 0.03092299\n",
      "Iteration 3147, loss = 0.03091919\n",
      "Iteration 3148, loss = 0.03091654\n",
      "Iteration 3149, loss = 0.03091004\n",
      "Iteration 3150, loss = 0.03090398\n",
      "Iteration 3151, loss = 0.03090005\n",
      "Iteration 3152, loss = 0.03089941\n",
      "Iteration 3153, loss = 0.03089408\n",
      "Iteration 3154, loss = 0.03088337\n",
      "Iteration 3155, loss = 0.03087719\n",
      "Iteration 3156, loss = 0.03087848\n",
      "Iteration 3157, loss = 0.03087124\n",
      "Iteration 3158, loss = 0.03086895\n",
      "Iteration 3159, loss = 0.03086121\n",
      "Iteration 3160, loss = 0.03085385\n",
      "Iteration 3161, loss = 0.03085047\n",
      "Iteration 3162, loss = 0.03084648\n",
      "Iteration 3163, loss = 0.03084153\n",
      "Iteration 3164, loss = 0.03084434\n",
      "Iteration 3165, loss = 0.03082903\n",
      "Iteration 3166, loss = 0.03083010\n",
      "Iteration 3167, loss = 0.03082154\n",
      "Iteration 3168, loss = 0.03081527\n",
      "Iteration 3169, loss = 0.03080800\n",
      "Iteration 3170, loss = 0.03080547\n",
      "Iteration 3171, loss = 0.03079964\n",
      "Iteration 3172, loss = 0.03078981\n",
      "Iteration 3173, loss = 0.03078812\n",
      "Iteration 3174, loss = 0.03078449\n",
      "Iteration 3175, loss = 0.03078664\n",
      "Iteration 3176, loss = 0.03077543\n",
      "Iteration 3177, loss = 0.03076495\n",
      "Iteration 3178, loss = 0.03076466\n",
      "Iteration 3179, loss = 0.03075604\n",
      "Iteration 3180, loss = 0.03074635\n",
      "Iteration 3181, loss = 0.03074793\n",
      "Iteration 3182, loss = 0.03073936\n",
      "Iteration 3183, loss = 0.03073503\n",
      "Iteration 3184, loss = 0.03074484\n",
      "Iteration 3185, loss = 0.03072762\n",
      "Iteration 3186, loss = 0.03072405\n",
      "Iteration 3187, loss = 0.03071826\n",
      "Iteration 3188, loss = 0.03071316\n",
      "Iteration 3189, loss = 0.03070504\n",
      "Iteration 3190, loss = 0.03070259\n",
      "Iteration 3191, loss = 0.03069600\n",
      "Iteration 3192, loss = 0.03069204\n",
      "Iteration 3193, loss = 0.03068803\n",
      "Iteration 3194, loss = 0.03068259\n",
      "Iteration 3195, loss = 0.03067520\n",
      "Iteration 3196, loss = 0.03067980\n",
      "Iteration 3197, loss = 0.03067123\n",
      "Iteration 3198, loss = 0.03066399\n",
      "Iteration 3199, loss = 0.03065965\n",
      "Iteration 3200, loss = 0.03065453\n",
      "Iteration 3201, loss = 0.03064861\n",
      "Iteration 3202, loss = 0.03064586\n",
      "Iteration 3203, loss = 0.03063585\n",
      "Iteration 3204, loss = 0.03063174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3205, loss = 0.03062907\n",
      "Iteration 3206, loss = 0.03062262\n",
      "Iteration 3207, loss = 0.03061341\n",
      "Iteration 3208, loss = 0.03061152\n",
      "Iteration 3209, loss = 0.03061108\n",
      "Iteration 3210, loss = 0.03060235\n",
      "Iteration 3211, loss = 0.03059278\n",
      "Iteration 3212, loss = 0.03059555\n",
      "Iteration 3213, loss = 0.03058453\n",
      "Iteration 3214, loss = 0.03057750\n",
      "Iteration 3215, loss = 0.03057662\n",
      "Iteration 3216, loss = 0.03056805\n",
      "Iteration 3217, loss = 0.03057478\n",
      "Iteration 3218, loss = 0.03056123\n",
      "Iteration 3219, loss = 0.03055731\n",
      "Iteration 3220, loss = 0.03054975\n",
      "Iteration 3221, loss = 0.03054556\n",
      "Iteration 3222, loss = 0.03053945\n",
      "Iteration 3223, loss = 0.03053614\n",
      "Iteration 3224, loss = 0.03053326\n",
      "Iteration 3225, loss = 0.03052816\n",
      "Iteration 3226, loss = 0.03052328\n",
      "Iteration 3227, loss = 0.03051490\n",
      "Iteration 3228, loss = 0.03051070\n",
      "Iteration 3229, loss = 0.03050525\n",
      "Iteration 3230, loss = 0.03050196\n",
      "Iteration 3231, loss = 0.03049503\n",
      "Iteration 3232, loss = 0.03049710\n",
      "Iteration 3233, loss = 0.03048683\n",
      "Iteration 3234, loss = 0.03048623\n",
      "Iteration 3235, loss = 0.03047472\n",
      "Iteration 3236, loss = 0.03046973\n",
      "Iteration 3237, loss = 0.03046784\n",
      "Iteration 3238, loss = 0.03046294\n",
      "Iteration 3239, loss = 0.03045609\n",
      "Iteration 3240, loss = 0.03045037\n",
      "Iteration 3241, loss = 0.03044711\n",
      "Iteration 3242, loss = 0.03044224\n",
      "Iteration 3243, loss = 0.03043594\n",
      "Iteration 3244, loss = 0.03042925\n",
      "Iteration 3245, loss = 0.03043215\n",
      "Iteration 3246, loss = 0.03042155\n",
      "Iteration 3247, loss = 0.03041650\n",
      "Iteration 3248, loss = 0.03040998\n",
      "Iteration 3249, loss = 0.03041035\n",
      "Iteration 3250, loss = 0.03040214\n",
      "Iteration 3251, loss = 0.03040141\n",
      "Iteration 3252, loss = 0.03039232\n",
      "Iteration 3253, loss = 0.03039153\n",
      "Iteration 3254, loss = 0.03037822\n",
      "Iteration 3255, loss = 0.03037436\n",
      "Iteration 3256, loss = 0.03037045\n",
      "Iteration 3257, loss = 0.03036456\n",
      "Iteration 3258, loss = 0.03036151\n",
      "Iteration 3259, loss = 0.03035767\n",
      "Iteration 3260, loss = 0.03035308\n",
      "Iteration 3261, loss = 0.03034477\n",
      "Iteration 3262, loss = 0.03034975\n",
      "Iteration 3263, loss = 0.03034795\n",
      "Iteration 3264, loss = 0.03033997\n",
      "Iteration 3265, loss = 0.03032343\n",
      "Iteration 3266, loss = 0.03032545\n",
      "Iteration 3267, loss = 0.03031759\n",
      "Iteration 3268, loss = 0.03030749\n",
      "Iteration 3269, loss = 0.03031309\n",
      "Iteration 3270, loss = 0.03030539\n",
      "Iteration 3271, loss = 0.03029942\n",
      "Iteration 3272, loss = 0.03029976\n",
      "Iteration 3273, loss = 0.03029429\n",
      "Iteration 3274, loss = 0.03028029\n",
      "Iteration 3275, loss = 0.03027880\n",
      "Iteration 3276, loss = 0.03027785\n",
      "Iteration 3277, loss = 0.03026401\n",
      "Iteration 3278, loss = 0.03026115\n",
      "Iteration 3279, loss = 0.03025776\n",
      "Iteration 3280, loss = 0.03025411\n",
      "Iteration 3281, loss = 0.03025330\n",
      "Iteration 3282, loss = 0.03024227\n",
      "Iteration 3283, loss = 0.03023576\n",
      "Iteration 3284, loss = 0.03023044\n",
      "Iteration 3285, loss = 0.03022841\n",
      "Iteration 3286, loss = 0.03022515\n",
      "Iteration 3287, loss = 0.03021808\n",
      "Iteration 3288, loss = 0.03020875\n",
      "Iteration 3289, loss = 0.03020686\n",
      "Iteration 3290, loss = 0.03020186\n",
      "Iteration 3291, loss = 0.03019411\n",
      "Iteration 3292, loss = 0.03018888\n",
      "Iteration 3293, loss = 0.03018952\n",
      "Iteration 3294, loss = 0.03017931\n",
      "Iteration 3295, loss = 0.03017696\n",
      "Iteration 3296, loss = 0.03017159\n",
      "Iteration 3297, loss = 0.03016933\n",
      "Iteration 3298, loss = 0.03016435\n",
      "Iteration 3299, loss = 0.03015595\n",
      "Iteration 3300, loss = 0.03015176\n",
      "Iteration 3301, loss = 0.03014324\n",
      "Iteration 3302, loss = 0.03014026\n",
      "Iteration 3303, loss = 0.03013428\n",
      "Iteration 3304, loss = 0.03013480\n",
      "Iteration 3305, loss = 0.03012568\n",
      "Iteration 3306, loss = 0.03012700\n",
      "Iteration 3307, loss = 0.03012050\n",
      "Iteration 3308, loss = 0.03011616\n",
      "Iteration 3309, loss = 0.03010895\n",
      "Iteration 3310, loss = 0.03010354\n",
      "Iteration 3311, loss = 0.03009352\n",
      "Iteration 3312, loss = 0.03009713\n",
      "Iteration 3313, loss = 0.03008885\n",
      "Iteration 3314, loss = 0.03008313\n",
      "Iteration 3315, loss = 0.03008188\n",
      "Iteration 3316, loss = 0.03007441\n",
      "Iteration 3317, loss = 0.03006569\n",
      "Iteration 3318, loss = 0.03006245\n",
      "Iteration 3319, loss = 0.03005940\n",
      "Iteration 3320, loss = 0.03005230\n",
      "Iteration 3321, loss = 0.03005370\n",
      "Iteration 3322, loss = 0.03004883\n",
      "Iteration 3323, loss = 0.03004191\n",
      "Iteration 3324, loss = 0.03003824\n",
      "Iteration 3325, loss = 0.03002808\n",
      "Iteration 3326, loss = 0.03002749\n",
      "Iteration 3327, loss = 0.03002096\n",
      "Iteration 3328, loss = 0.03001537\n",
      "Iteration 3329, loss = 0.03001153\n",
      "Iteration 3330, loss = 0.03000410\n",
      "Iteration 3331, loss = 0.02999936\n",
      "Iteration 3332, loss = 0.02999628\n",
      "Iteration 3333, loss = 0.02999288\n",
      "Iteration 3334, loss = 0.02998741\n",
      "Iteration 3335, loss = 0.02998523\n",
      "Iteration 3336, loss = 0.02997541\n",
      "Iteration 3337, loss = 0.02997146\n",
      "Iteration 3338, loss = 0.02996349\n",
      "Iteration 3339, loss = 0.02997232\n",
      "Iteration 3340, loss = 0.02995718\n",
      "Iteration 3341, loss = 0.02995290\n",
      "Iteration 3342, loss = 0.02995162\n",
      "Iteration 3343, loss = 0.02994498\n",
      "Iteration 3344, loss = 0.02993839\n",
      "Iteration 3345, loss = 0.02992993\n",
      "Iteration 3346, loss = 0.02992915\n",
      "Iteration 3347, loss = 0.02992635\n",
      "Iteration 3348, loss = 0.02991670\n",
      "Iteration 3349, loss = 0.02991618\n",
      "Iteration 3350, loss = 0.02990622\n",
      "Iteration 3351, loss = 0.02990003\n",
      "Iteration 3352, loss = 0.02990070\n",
      "Iteration 3353, loss = 0.02989353\n",
      "Iteration 3354, loss = 0.02989360\n",
      "Iteration 3355, loss = 0.02988063\n",
      "Iteration 3356, loss = 0.02988409\n",
      "Iteration 3357, loss = 0.02988420\n",
      "Iteration 3358, loss = 0.02986705\n",
      "Iteration 3359, loss = 0.02986416\n",
      "Iteration 3360, loss = 0.02985887\n",
      "Iteration 3361, loss = 0.02985279\n",
      "Iteration 3362, loss = 0.02985593\n",
      "Iteration 3363, loss = 0.02984750\n",
      "Iteration 3364, loss = 0.02983902\n",
      "Iteration 3365, loss = 0.02983729\n",
      "Iteration 3366, loss = 0.02983427\n",
      "Iteration 3367, loss = 0.02982063\n",
      "Iteration 3368, loss = 0.02981902\n",
      "Iteration 3369, loss = 0.02981576\n",
      "Iteration 3370, loss = 0.02981613\n",
      "Iteration 3371, loss = 0.02980716\n",
      "Iteration 3372, loss = 0.02980293\n",
      "Iteration 3373, loss = 0.02980114\n",
      "Iteration 3374, loss = 0.02979771\n",
      "Iteration 3375, loss = 0.02978691\n",
      "Iteration 3376, loss = 0.02978269\n",
      "Iteration 3377, loss = 0.02977645\n",
      "Iteration 3378, loss = 0.02977096\n",
      "Iteration 3379, loss = 0.02976624\n",
      "Iteration 3380, loss = 0.02976578\n",
      "Iteration 3381, loss = 0.02975898\n",
      "Iteration 3382, loss = 0.02975420\n",
      "Iteration 3383, loss = 0.02974762\n",
      "Iteration 3384, loss = 0.02974899\n",
      "Iteration 3385, loss = 0.02973819\n",
      "Iteration 3386, loss = 0.02973393\n",
      "Iteration 3387, loss = 0.02973021\n",
      "Iteration 3388, loss = 0.02972149\n",
      "Iteration 3389, loss = 0.02971901\n",
      "Iteration 3390, loss = 0.02971933\n",
      "Iteration 3391, loss = 0.02971591\n",
      "Iteration 3392, loss = 0.02971116\n",
      "Iteration 3393, loss = 0.02970265\n",
      "Iteration 3394, loss = 0.02969568\n",
      "Iteration 3395, loss = 0.02968895\n",
      "Iteration 3396, loss = 0.02968687\n",
      "Iteration 3397, loss = 0.02969039\n",
      "Iteration 3398, loss = 0.02968017\n",
      "Iteration 3399, loss = 0.02967188\n",
      "Iteration 3400, loss = 0.02967278\n",
      "Iteration 3401, loss = 0.02966481\n",
      "Iteration 3402, loss = 0.02965916\n",
      "Iteration 3403, loss = 0.02965440\n",
      "Iteration 3404, loss = 0.02964911\n",
      "Iteration 3405, loss = 0.02964672\n",
      "Iteration 3406, loss = 0.02963871\n",
      "Iteration 3407, loss = 0.02964416\n",
      "Iteration 3408, loss = 0.02963651\n",
      "Iteration 3409, loss = 0.02962516\n",
      "Iteration 3410, loss = 0.02962369\n",
      "Iteration 3411, loss = 0.02962013\n",
      "Iteration 3412, loss = 0.02960918\n",
      "Iteration 3413, loss = 0.02961519\n",
      "Iteration 3414, loss = 0.02960516\n",
      "Iteration 3415, loss = 0.02959752\n",
      "Iteration 3416, loss = 0.02959553\n",
      "Iteration 3417, loss = 0.02959353\n",
      "Iteration 3418, loss = 0.02958137\n",
      "Iteration 3419, loss = 0.02957697\n",
      "Iteration 3420, loss = 0.02957900\n",
      "Iteration 3421, loss = 0.02957161\n",
      "Iteration 3422, loss = 0.02956152\n",
      "Iteration 3423, loss = 0.02955885\n",
      "Iteration 3424, loss = 0.02955715\n",
      "Iteration 3425, loss = 0.02955548\n",
      "Iteration 3426, loss = 0.02955206\n",
      "Iteration 3427, loss = 0.02954232\n",
      "Iteration 3428, loss = 0.02954525\n",
      "Iteration 3429, loss = 0.02953623\n",
      "Iteration 3430, loss = 0.02952634\n",
      "Iteration 3431, loss = 0.02952281\n",
      "Iteration 3432, loss = 0.02952065\n",
      "Iteration 3433, loss = 0.02951733\n",
      "Iteration 3434, loss = 0.02951105\n",
      "Iteration 3435, loss = 0.02950814\n",
      "Iteration 3436, loss = 0.02950265\n",
      "Iteration 3437, loss = 0.02949173\n",
      "Iteration 3438, loss = 0.02950005\n",
      "Iteration 3439, loss = 0.02948772\n",
      "Iteration 3440, loss = 0.02948785\n",
      "Iteration 3441, loss = 0.02947937\n",
      "Iteration 3442, loss = 0.02947502\n",
      "Iteration 3443, loss = 0.02947105\n",
      "Iteration 3444, loss = 0.02946191\n",
      "Iteration 3445, loss = 0.02945931\n",
      "Iteration 3446, loss = 0.02946025\n",
      "Iteration 3447, loss = 0.02944644\n",
      "Iteration 3448, loss = 0.02944360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3449, loss = 0.02944014\n",
      "Iteration 3450, loss = 0.02943689\n",
      "Iteration 3451, loss = 0.02943918\n",
      "Iteration 3452, loss = 0.02942035\n",
      "Iteration 3453, loss = 0.02942867\n",
      "Iteration 3454, loss = 0.02941723\n",
      "Iteration 3455, loss = 0.02941220\n",
      "Iteration 3456, loss = 0.02941169\n",
      "Iteration 3457, loss = 0.02939829\n",
      "Iteration 3458, loss = 0.02939671\n",
      "Iteration 3459, loss = 0.02939354\n",
      "Iteration 3460, loss = 0.02938756\n",
      "Iteration 3461, loss = 0.02938290\n",
      "Iteration 3462, loss = 0.02937774\n",
      "Iteration 3463, loss = 0.02937379\n",
      "Iteration 3464, loss = 0.02936701\n",
      "Iteration 3465, loss = 0.02936652\n",
      "Iteration 3466, loss = 0.02936095\n",
      "Iteration 3467, loss = 0.02934865\n",
      "Iteration 3468, loss = 0.02934686\n",
      "Iteration 3469, loss = 0.02934925\n",
      "Iteration 3470, loss = 0.02934401\n",
      "Iteration 3471, loss = 0.02933664\n",
      "Iteration 3472, loss = 0.02934105\n",
      "Iteration 3473, loss = 0.02932568\n",
      "Iteration 3474, loss = 0.02932098\n",
      "Iteration 3475, loss = 0.02931812\n",
      "Iteration 3476, loss = 0.02931276\n",
      "Iteration 3477, loss = 0.02931029\n",
      "Iteration 3478, loss = 0.02930343\n",
      "Iteration 3479, loss = 0.02929581\n",
      "Iteration 3480, loss = 0.02929298\n",
      "Iteration 3481, loss = 0.02929162\n",
      "Iteration 3482, loss = 0.02928143\n",
      "Iteration 3483, loss = 0.02928051\n",
      "Iteration 3484, loss = 0.02927157\n",
      "Iteration 3485, loss = 0.02926761\n",
      "Iteration 3486, loss = 0.02926481\n",
      "Iteration 3487, loss = 0.02926042\n",
      "Iteration 3488, loss = 0.02925379\n",
      "Iteration 3489, loss = 0.02925053\n",
      "Iteration 3490, loss = 0.02925106\n",
      "Iteration 3491, loss = 0.02924156\n",
      "Iteration 3492, loss = 0.02923804\n",
      "Iteration 3493, loss = 0.02923705\n",
      "Iteration 3494, loss = 0.02922842\n",
      "Iteration 3495, loss = 0.02923061\n",
      "Iteration 3496, loss = 0.02922450\n",
      "Iteration 3497, loss = 0.02921405\n",
      "Iteration 3498, loss = 0.02920849\n",
      "Iteration 3499, loss = 0.02920657\n",
      "Iteration 3500, loss = 0.02919824\n",
      "Iteration 3501, loss = 0.02919549\n",
      "Iteration 3502, loss = 0.02919384\n",
      "Iteration 3503, loss = 0.02918958\n",
      "Iteration 3504, loss = 0.02918365\n",
      "Iteration 3505, loss = 0.02917912\n",
      "Iteration 3506, loss = 0.02917378\n",
      "Iteration 3507, loss = 0.02916541\n",
      "Iteration 3508, loss = 0.02917079\n",
      "Iteration 3509, loss = 0.02916042\n",
      "Iteration 3510, loss = 0.02915797\n",
      "Iteration 3511, loss = 0.02915162\n",
      "Iteration 3512, loss = 0.02914386\n",
      "Iteration 3513, loss = 0.02914266\n",
      "Iteration 3514, loss = 0.02913556\n",
      "Iteration 3515, loss = 0.02913136\n",
      "Iteration 3516, loss = 0.02912413\n",
      "Iteration 3517, loss = 0.02912669\n",
      "Iteration 3518, loss = 0.02911840\n",
      "Iteration 3519, loss = 0.02911523\n",
      "Iteration 3520, loss = 0.02911075\n",
      "Iteration 3521, loss = 0.02910137\n",
      "Iteration 3522, loss = 0.02910507\n",
      "Iteration 3523, loss = 0.02909471\n",
      "Iteration 3524, loss = 0.02909044\n",
      "Iteration 3525, loss = 0.02908615\n",
      "Iteration 3526, loss = 0.02908983\n",
      "Iteration 3527, loss = 0.02907648\n",
      "Iteration 3528, loss = 0.02907579\n",
      "Iteration 3529, loss = 0.02906822\n",
      "Iteration 3530, loss = 0.02906414\n",
      "Iteration 3531, loss = 0.02907031\n",
      "Iteration 3532, loss = 0.02905734\n",
      "Iteration 3533, loss = 0.02905167\n",
      "Iteration 3534, loss = 0.02904422\n",
      "Iteration 3535, loss = 0.02903891\n",
      "Iteration 3536, loss = 0.02903854\n",
      "Iteration 3537, loss = 0.02902795\n",
      "Iteration 3538, loss = 0.02902957\n",
      "Iteration 3539, loss = 0.02902123\n",
      "Iteration 3540, loss = 0.02901551\n",
      "Iteration 3541, loss = 0.02901083\n",
      "Iteration 3542, loss = 0.02900596\n",
      "Iteration 3543, loss = 0.02900265\n",
      "Iteration 3544, loss = 0.02900225\n",
      "Iteration 3545, loss = 0.02899467\n",
      "Iteration 3546, loss = 0.02898780\n",
      "Iteration 3547, loss = 0.02898413\n",
      "Iteration 3548, loss = 0.02898876\n",
      "Iteration 3549, loss = 0.02897387\n",
      "Iteration 3550, loss = 0.02897544\n",
      "Iteration 3551, loss = 0.02896494\n",
      "Iteration 3552, loss = 0.02896767\n",
      "Iteration 3553, loss = 0.02895617\n",
      "Iteration 3554, loss = 0.02895216\n",
      "Iteration 3555, loss = 0.02894754\n",
      "Iteration 3556, loss = 0.02894554\n",
      "Iteration 3557, loss = 0.02893815\n",
      "Iteration 3558, loss = 0.02893505\n",
      "Iteration 3559, loss = 0.02893992\n",
      "Iteration 3560, loss = 0.02892435\n",
      "Iteration 3561, loss = 0.02891799\n",
      "Iteration 3562, loss = 0.02891513\n",
      "Iteration 3563, loss = 0.02891824\n",
      "Iteration 3564, loss = 0.02890997\n",
      "Iteration 3565, loss = 0.02890878\n",
      "Iteration 3566, loss = 0.02889901\n",
      "Iteration 3567, loss = 0.02889200\n",
      "Iteration 3568, loss = 0.02889118\n",
      "Iteration 3569, loss = 0.02888612\n",
      "Iteration 3570, loss = 0.02887815\n",
      "Iteration 3571, loss = 0.02887281\n",
      "Iteration 3572, loss = 0.02887131\n",
      "Iteration 3573, loss = 0.02886424\n",
      "Iteration 3574, loss = 0.02886455\n",
      "Iteration 3575, loss = 0.02886470\n",
      "Iteration 3576, loss = 0.02885080\n",
      "Iteration 3577, loss = 0.02884721\n",
      "Iteration 3578, loss = 0.02884602\n",
      "Iteration 3579, loss = 0.02883643\n",
      "Iteration 3580, loss = 0.02884750\n",
      "Iteration 3581, loss = 0.02883287\n",
      "Iteration 3582, loss = 0.02883305\n",
      "Iteration 3583, loss = 0.02882077\n",
      "Iteration 3584, loss = 0.02881709\n",
      "Iteration 3585, loss = 0.02880999\n",
      "Iteration 3586, loss = 0.02880585\n",
      "Iteration 3587, loss = 0.02881029\n",
      "Iteration 3588, loss = 0.02879776\n",
      "Iteration 3589, loss = 0.02879575\n",
      "Iteration 3590, loss = 0.02878911\n",
      "Iteration 3591, loss = 0.02878472\n",
      "Iteration 3592, loss = 0.02878356\n",
      "Iteration 3593, loss = 0.02878020\n",
      "Iteration 3594, loss = 0.02876963\n",
      "Iteration 3595, loss = 0.02876720\n",
      "Iteration 3596, loss = 0.02876323\n",
      "Iteration 3597, loss = 0.02875652\n",
      "Iteration 3598, loss = 0.02875159\n",
      "Iteration 3599, loss = 0.02874768\n",
      "Iteration 3600, loss = 0.02874849\n",
      "Iteration 3601, loss = 0.02874324\n",
      "Iteration 3602, loss = 0.02873309\n",
      "Iteration 3603, loss = 0.02872786\n",
      "Iteration 3604, loss = 0.02873522\n",
      "Iteration 3605, loss = 0.02872788\n",
      "Iteration 3606, loss = 0.02872812\n",
      "Iteration 3607, loss = 0.02871633\n",
      "Iteration 3608, loss = 0.02871229\n",
      "Iteration 3609, loss = 0.02870411\n",
      "Iteration 3610, loss = 0.02869861\n",
      "Iteration 3611, loss = 0.02869253\n",
      "Iteration 3612, loss = 0.02869245\n",
      "Iteration 3613, loss = 0.02868678\n",
      "Iteration 3614, loss = 0.02868233\n",
      "Iteration 3615, loss = 0.02868170\n",
      "Iteration 3616, loss = 0.02867978\n",
      "Iteration 3617, loss = 0.02867458\n",
      "Iteration 3618, loss = 0.02866776\n",
      "Iteration 3619, loss = 0.02866389\n",
      "Iteration 3620, loss = 0.02865455\n",
      "Iteration 3621, loss = 0.02864981\n",
      "Iteration 3622, loss = 0.02864364\n",
      "Iteration 3623, loss = 0.02864050\n",
      "Iteration 3624, loss = 0.02863981\n",
      "Iteration 3625, loss = 0.02863740\n",
      "Iteration 3626, loss = 0.02862873\n",
      "Iteration 3627, loss = 0.02862577\n",
      "Iteration 3628, loss = 0.02861825\n",
      "Iteration 3629, loss = 0.02861329\n",
      "Iteration 3630, loss = 0.02861261\n",
      "Iteration 3631, loss = 0.02860902\n",
      "Iteration 3632, loss = 0.02860503\n",
      "Iteration 3633, loss = 0.02859397\n",
      "Iteration 3634, loss = 0.02858997\n",
      "Iteration 3635, loss = 0.02859284\n",
      "Iteration 3636, loss = 0.02859117\n",
      "Iteration 3637, loss = 0.02858142\n",
      "Iteration 3638, loss = 0.02857427\n",
      "Iteration 3639, loss = 0.02857674\n",
      "Iteration 3640, loss = 0.02856611\n",
      "Iteration 3641, loss = 0.02856293\n",
      "Iteration 3642, loss = 0.02856040\n",
      "Iteration 3643, loss = 0.02855356\n",
      "Iteration 3644, loss = 0.02854857\n",
      "Iteration 3645, loss = 0.02854740\n",
      "Iteration 3646, loss = 0.02854627\n",
      "Iteration 3647, loss = 0.02853228\n",
      "Iteration 3648, loss = 0.02853093\n",
      "Iteration 3649, loss = 0.02853577\n",
      "Iteration 3650, loss = 0.02852909\n",
      "Iteration 3651, loss = 0.02852012\n",
      "Iteration 3652, loss = 0.02851283\n",
      "Iteration 3653, loss = 0.02850769\n",
      "Iteration 3654, loss = 0.02850478\n",
      "Iteration 3655, loss = 0.02850041\n",
      "Iteration 3656, loss = 0.02849966\n",
      "Iteration 3657, loss = 0.02849463\n",
      "Iteration 3658, loss = 0.02849372\n",
      "Iteration 3659, loss = 0.02847999\n",
      "Iteration 3660, loss = 0.02847720\n",
      "Iteration 3661, loss = 0.02847230\n",
      "Iteration 3662, loss = 0.02847442\n",
      "Iteration 3663, loss = 0.02847065\n",
      "Iteration 3664, loss = 0.02846423\n",
      "Iteration 3665, loss = 0.02845817\n",
      "Iteration 3666, loss = 0.02845769\n",
      "Iteration 3667, loss = 0.02844898\n",
      "Iteration 3668, loss = 0.02844336\n",
      "Iteration 3669, loss = 0.02843894\n",
      "Iteration 3670, loss = 0.02843676\n",
      "Iteration 3671, loss = 0.02843267\n",
      "Iteration 3672, loss = 0.02842370\n",
      "Iteration 3673, loss = 0.02842347\n",
      "Iteration 3674, loss = 0.02841921\n",
      "Iteration 3675, loss = 0.02841182\n",
      "Iteration 3676, loss = 0.02841155\n",
      "Iteration 3677, loss = 0.02840888\n",
      "Iteration 3678, loss = 0.02839708\n",
      "Iteration 3679, loss = 0.02839503\n",
      "Iteration 3680, loss = 0.02839301\n",
      "Iteration 3681, loss = 0.02838934\n",
      "Iteration 3682, loss = 0.02839273\n",
      "Iteration 3683, loss = 0.02837907\n",
      "Iteration 3684, loss = 0.02837452\n",
      "Iteration 3685, loss = 0.02837625\n",
      "Iteration 3686, loss = 0.02836261\n",
      "Iteration 3687, loss = 0.02836255\n",
      "Iteration 3688, loss = 0.02835540\n",
      "Iteration 3689, loss = 0.02835826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3690, loss = 0.02834813\n",
      "Iteration 3691, loss = 0.02834301\n",
      "Iteration 3692, loss = 0.02833619\n",
      "Iteration 3693, loss = 0.02833577\n",
      "Iteration 3694, loss = 0.02833607\n",
      "Iteration 3695, loss = 0.02833021\n",
      "Iteration 3696, loss = 0.02832656\n",
      "Iteration 3697, loss = 0.02832322\n",
      "Iteration 3698, loss = 0.02831570\n",
      "Iteration 3699, loss = 0.02830890\n",
      "Iteration 3700, loss = 0.02830435\n",
      "Iteration 3701, loss = 0.02830154\n",
      "Iteration 3702, loss = 0.02829528\n",
      "Iteration 3703, loss = 0.02829043\n",
      "Iteration 3704, loss = 0.02829252\n",
      "Iteration 3705, loss = 0.02828161\n",
      "Iteration 3706, loss = 0.02828584\n",
      "Iteration 3707, loss = 0.02827307\n",
      "Iteration 3708, loss = 0.02827215\n",
      "Iteration 3709, loss = 0.02826902\n",
      "Iteration 3710, loss = 0.02826261\n",
      "Iteration 3711, loss = 0.02826360\n",
      "Iteration 3712, loss = 0.02825470\n",
      "Iteration 3713, loss = 0.02825169\n",
      "Iteration 3714, loss = 0.02824394\n",
      "Iteration 3715, loss = 0.02824083\n",
      "Iteration 3716, loss = 0.02823522\n",
      "Iteration 3717, loss = 0.02823058\n",
      "Iteration 3718, loss = 0.02823018\n",
      "Iteration 3719, loss = 0.02822358\n",
      "Iteration 3720, loss = 0.02822092\n",
      "Iteration 3721, loss = 0.02821399\n",
      "Iteration 3722, loss = 0.02821618\n",
      "Iteration 3723, loss = 0.02820942\n",
      "Iteration 3724, loss = 0.02821472\n",
      "Iteration 3725, loss = 0.02820251\n",
      "Iteration 3726, loss = 0.02819577\n",
      "Iteration 3727, loss = 0.02820142\n",
      "Iteration 3728, loss = 0.02818461\n",
      "Iteration 3729, loss = 0.02818223\n",
      "Iteration 3730, loss = 0.02818629\n",
      "Iteration 3731, loss = 0.02816918\n",
      "Iteration 3732, loss = 0.02816972\n",
      "Iteration 3733, loss = 0.02816421\n",
      "Iteration 3734, loss = 0.02816355\n",
      "Iteration 3735, loss = 0.02815871\n",
      "Iteration 3736, loss = 0.02814928\n",
      "Iteration 3737, loss = 0.02814466\n",
      "Iteration 3738, loss = 0.02814365\n",
      "Iteration 3739, loss = 0.02813768\n",
      "Iteration 3740, loss = 0.02813671\n",
      "Iteration 3741, loss = 0.02812698\n",
      "Iteration 3742, loss = 0.02813064\n",
      "Iteration 3743, loss = 0.02812683\n",
      "Iteration 3744, loss = 0.02811699\n",
      "Iteration 3745, loss = 0.02811451\n",
      "Iteration 3746, loss = 0.02810833\n",
      "Iteration 3747, loss = 0.02810908\n",
      "Iteration 3748, loss = 0.02809513\n",
      "Iteration 3749, loss = 0.02810448\n",
      "Iteration 3750, loss = 0.02809671\n",
      "Iteration 3751, loss = 0.02809213\n",
      "Iteration 3752, loss = 0.02808564\n",
      "Iteration 3753, loss = 0.02808044\n",
      "Iteration 3754, loss = 0.02807351\n",
      "Iteration 3755, loss = 0.02807162\n",
      "Iteration 3756, loss = 0.02806709\n",
      "Iteration 3757, loss = 0.02805986\n",
      "Iteration 3758, loss = 0.02805814\n",
      "Iteration 3759, loss = 0.02805586\n",
      "Iteration 3760, loss = 0.02805169\n",
      "Iteration 3761, loss = 0.02804634\n",
      "Iteration 3762, loss = 0.02804553\n",
      "Iteration 3763, loss = 0.02803952\n",
      "Iteration 3764, loss = 0.02803076\n",
      "Iteration 3765, loss = 0.02802946\n",
      "Iteration 3766, loss = 0.02802576\n",
      "Iteration 3767, loss = 0.02802352\n",
      "Iteration 3768, loss = 0.02801662\n",
      "Iteration 3769, loss = 0.02801042\n",
      "Iteration 3770, loss = 0.02800378\n",
      "Iteration 3771, loss = 0.02800581\n",
      "Iteration 3772, loss = 0.02799549\n",
      "Iteration 3773, loss = 0.02799415\n",
      "Iteration 3774, loss = 0.02799253\n",
      "Iteration 3775, loss = 0.02798057\n",
      "Iteration 3776, loss = 0.02798448\n",
      "Iteration 3777, loss = 0.02797889\n",
      "Iteration 3778, loss = 0.02797148\n",
      "Iteration 3779, loss = 0.02797149\n",
      "Iteration 3780, loss = 0.02796702\n",
      "Iteration 3781, loss = 0.02796613\n",
      "Iteration 3782, loss = 0.02795869\n",
      "Iteration 3783, loss = 0.02795288\n",
      "Iteration 3784, loss = 0.02794622\n",
      "Iteration 3785, loss = 0.02794002\n",
      "Iteration 3786, loss = 0.02793902\n",
      "Iteration 3787, loss = 0.02793461\n",
      "Iteration 3788, loss = 0.02793214\n",
      "Iteration 3789, loss = 0.02793023\n",
      "Iteration 3790, loss = 0.02792223\n",
      "Iteration 3791, loss = 0.02791969\n",
      "Iteration 3792, loss = 0.02791517\n",
      "Iteration 3793, loss = 0.02790985\n",
      "Iteration 3794, loss = 0.02790512\n",
      "Iteration 3795, loss = 0.02789996\n",
      "Iteration 3796, loss = 0.02789350\n",
      "Iteration 3797, loss = 0.02789226\n",
      "Iteration 3798, loss = 0.02789299\n",
      "Iteration 3799, loss = 0.02788720\n",
      "Iteration 3800, loss = 0.02788343\n",
      "Iteration 3801, loss = 0.02787686\n",
      "Iteration 3802, loss = 0.02787292\n",
      "Iteration 3803, loss = 0.02786609\n",
      "Iteration 3804, loss = 0.02787240\n",
      "Iteration 3805, loss = 0.02786031\n",
      "Iteration 3806, loss = 0.02784730\n",
      "Iteration 3807, loss = 0.02784889\n",
      "Iteration 3808, loss = 0.02784590\n",
      "Iteration 3809, loss = 0.02784103\n",
      "Iteration 3810, loss = 0.02783840\n",
      "Iteration 3811, loss = 0.02783522\n",
      "Iteration 3812, loss = 0.02783141\n",
      "Iteration 3813, loss = 0.02782662\n",
      "Iteration 3814, loss = 0.02782369\n",
      "Iteration 3815, loss = 0.02781983\n",
      "Iteration 3816, loss = 0.02781435\n",
      "Iteration 3817, loss = 0.02781392\n",
      "Iteration 3818, loss = 0.02780523\n",
      "Iteration 3819, loss = 0.02780091\n",
      "Iteration 3820, loss = 0.02779222\n",
      "Iteration 3821, loss = 0.02779477\n",
      "Iteration 3822, loss = 0.02778882\n",
      "Iteration 3823, loss = 0.02777810\n",
      "Iteration 3824, loss = 0.02777836\n",
      "Iteration 3825, loss = 0.02777508\n",
      "Iteration 3826, loss = 0.02777712\n",
      "Iteration 3827, loss = 0.02776677\n",
      "Iteration 3828, loss = 0.02775935\n",
      "Iteration 3829, loss = 0.02775824\n",
      "Iteration 3830, loss = 0.02776003\n",
      "Iteration 3831, loss = 0.02774990\n",
      "Iteration 3832, loss = 0.02775236\n",
      "Iteration 3833, loss = 0.02774005\n",
      "Iteration 3834, loss = 0.02773781\n",
      "Iteration 3835, loss = 0.02773433\n",
      "Iteration 3836, loss = 0.02773588\n",
      "Iteration 3837, loss = 0.02772461\n",
      "Iteration 3838, loss = 0.02772307\n",
      "Iteration 3839, loss = 0.02772122\n",
      "Iteration 3840, loss = 0.02771320\n",
      "Iteration 3841, loss = 0.02771171\n",
      "Iteration 3842, loss = 0.02770752\n",
      "Iteration 3843, loss = 0.02769825\n",
      "Iteration 3844, loss = 0.02769588\n",
      "Iteration 3845, loss = 0.02768779\n",
      "Iteration 3846, loss = 0.02769237\n",
      "Iteration 3847, loss = 0.02768783\n",
      "Iteration 3848, loss = 0.02767845\n",
      "Iteration 3849, loss = 0.02768378\n",
      "Iteration 3850, loss = 0.02766915\n",
      "Iteration 3851, loss = 0.02766447\n",
      "Iteration 3852, loss = 0.02766820\n",
      "Iteration 3853, loss = 0.02766056\n",
      "Iteration 3854, loss = 0.02765599\n",
      "Iteration 3855, loss = 0.02764837\n",
      "Iteration 3856, loss = 0.02764603\n",
      "Iteration 3857, loss = 0.02764605\n",
      "Iteration 3858, loss = 0.02763625\n",
      "Iteration 3859, loss = 0.02762963\n",
      "Iteration 3860, loss = 0.02763269\n",
      "Iteration 3861, loss = 0.02762507\n",
      "Iteration 3862, loss = 0.02762060\n",
      "Iteration 3863, loss = 0.02761778\n",
      "Iteration 3864, loss = 0.02761499\n",
      "Iteration 3865, loss = 0.02760808\n",
      "Iteration 3866, loss = 0.02760444\n",
      "Iteration 3867, loss = 0.02759992\n",
      "Iteration 3868, loss = 0.02760229\n",
      "Iteration 3869, loss = 0.02759664\n",
      "Iteration 3870, loss = 0.02758635\n",
      "Iteration 3871, loss = 0.02758389\n",
      "Iteration 3872, loss = 0.02758167\n",
      "Iteration 3873, loss = 0.02757625\n",
      "Iteration 3874, loss = 0.02756999\n",
      "Iteration 3875, loss = 0.02756693\n",
      "Iteration 3876, loss = 0.02757160\n",
      "Iteration 3877, loss = 0.02755847\n",
      "Iteration 3878, loss = 0.02756650\n",
      "Iteration 3879, loss = 0.02755287\n",
      "Iteration 3880, loss = 0.02755036\n",
      "Iteration 3881, loss = 0.02754753\n",
      "Iteration 3882, loss = 0.02754480\n",
      "Iteration 3883, loss = 0.02753631\n",
      "Iteration 3884, loss = 0.02753075\n",
      "Iteration 3885, loss = 0.02752400\n",
      "Iteration 3886, loss = 0.02752696\n",
      "Iteration 3887, loss = 0.02752061\n",
      "Iteration 3888, loss = 0.02751412\n",
      "Iteration 3889, loss = 0.02751159\n",
      "Iteration 3890, loss = 0.02750864\n",
      "Iteration 3891, loss = 0.02750121\n",
      "Iteration 3892, loss = 0.02750003\n",
      "Iteration 3893, loss = 0.02749861\n",
      "Iteration 3894, loss = 0.02748945\n",
      "Iteration 3895, loss = 0.02748338\n",
      "Iteration 3896, loss = 0.02748173\n",
      "Iteration 3897, loss = 0.02747554\n",
      "Iteration 3898, loss = 0.02747195\n",
      "Iteration 3899, loss = 0.02746506\n",
      "Iteration 3900, loss = 0.02746351\n",
      "Iteration 3901, loss = 0.02746587\n",
      "Iteration 3902, loss = 0.02745779\n",
      "Iteration 3903, loss = 0.02745018\n",
      "Iteration 3904, loss = 0.02745172\n",
      "Iteration 3905, loss = 0.02744318\n",
      "Iteration 3906, loss = 0.02744147\n",
      "Iteration 3907, loss = 0.02744242\n",
      "Iteration 3908, loss = 0.02743202\n",
      "Iteration 3909, loss = 0.02742978\n",
      "Iteration 3910, loss = 0.02743097\n",
      "Iteration 3911, loss = 0.02742036\n",
      "Iteration 3912, loss = 0.02741717\n",
      "Iteration 3913, loss = 0.02741186\n",
      "Iteration 3914, loss = 0.02741906\n",
      "Iteration 3915, loss = 0.02740027\n",
      "Iteration 3916, loss = 0.02739577\n",
      "Iteration 3917, loss = 0.02739724\n",
      "Iteration 3918, loss = 0.02738969\n",
      "Iteration 3919, loss = 0.02738470\n",
      "Iteration 3920, loss = 0.02738724\n",
      "Iteration 3921, loss = 0.02737917\n",
      "Iteration 3922, loss = 0.02738252\n",
      "Iteration 3923, loss = 0.02737143\n",
      "Iteration 3924, loss = 0.02737011\n",
      "Iteration 3925, loss = 0.02736073\n",
      "Iteration 3926, loss = 0.02736382\n",
      "Iteration 3927, loss = 0.02735634\n",
      "Iteration 3928, loss = 0.02734657\n",
      "Iteration 3929, loss = 0.02734294\n",
      "Iteration 3930, loss = 0.02734028\n",
      "Iteration 3931, loss = 0.02734835\n",
      "Iteration 3932, loss = 0.02733355\n",
      "Iteration 3933, loss = 0.02732953\n",
      "Iteration 3934, loss = 0.02732489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3935, loss = 0.02732368\n",
      "Iteration 3936, loss = 0.02731554\n",
      "Iteration 3937, loss = 0.02731688\n",
      "Iteration 3938, loss = 0.02730717\n",
      "Iteration 3939, loss = 0.02730369\n",
      "Iteration 3940, loss = 0.02730170\n",
      "Iteration 3941, loss = 0.02729398\n",
      "Iteration 3942, loss = 0.02729316\n",
      "Iteration 3943, loss = 0.02728832\n",
      "Iteration 3944, loss = 0.02728518\n",
      "Iteration 3945, loss = 0.02727580\n",
      "Iteration 3946, loss = 0.02727944\n",
      "Iteration 3947, loss = 0.02727200\n",
      "Iteration 3948, loss = 0.02727043\n",
      "Iteration 3949, loss = 0.02725693\n",
      "Iteration 3950, loss = 0.02725709\n",
      "Iteration 3951, loss = 0.02725927\n",
      "Iteration 3952, loss = 0.02725276\n",
      "Iteration 3953, loss = 0.02724406\n",
      "Iteration 3954, loss = 0.02724690\n",
      "Iteration 3955, loss = 0.02723604\n",
      "Iteration 3956, loss = 0.02723862\n",
      "Iteration 3957, loss = 0.02722849\n",
      "Iteration 3958, loss = 0.02722721\n",
      "Iteration 3959, loss = 0.02722456\n",
      "Iteration 3960, loss = 0.02721752\n",
      "Iteration 3961, loss = 0.02721031\n",
      "Iteration 3962, loss = 0.02721335\n",
      "Iteration 3963, loss = 0.02720607\n",
      "Iteration 3964, loss = 0.02720294\n",
      "Iteration 3965, loss = 0.02719711\n",
      "Iteration 3966, loss = 0.02719536\n",
      "Iteration 3967, loss = 0.02719224\n",
      "Iteration 3968, loss = 0.02718852\n",
      "Iteration 3969, loss = 0.02719109\n",
      "Iteration 3970, loss = 0.02717951\n",
      "Iteration 3971, loss = 0.02717371\n",
      "Iteration 3972, loss = 0.02717120\n",
      "Iteration 3973, loss = 0.02716784\n",
      "Iteration 3974, loss = 0.02716122\n",
      "Iteration 3975, loss = 0.02715722\n",
      "Iteration 3976, loss = 0.02715541\n",
      "Iteration 3977, loss = 0.02714812\n",
      "Iteration 3978, loss = 0.02715146\n",
      "Iteration 3979, loss = 0.02714610\n",
      "Iteration 3980, loss = 0.02713592\n",
      "Iteration 3981, loss = 0.02713424\n",
      "Iteration 3982, loss = 0.02713273\n",
      "Iteration 3983, loss = 0.02712352\n",
      "Iteration 3984, loss = 0.02712356\n",
      "Iteration 3985, loss = 0.02711693\n",
      "Iteration 3986, loss = 0.02712184\n",
      "Iteration 3987, loss = 0.02711102\n",
      "Iteration 3988, loss = 0.02710508\n",
      "Iteration 3989, loss = 0.02710323\n",
      "Iteration 3990, loss = 0.02709527\n",
      "Iteration 3991, loss = 0.02709475\n",
      "Iteration 3992, loss = 0.02708853\n",
      "Iteration 3993, loss = 0.02709696\n",
      "Iteration 3994, loss = 0.02707602\n",
      "Iteration 3995, loss = 0.02707914\n",
      "Iteration 3996, loss = 0.02707573\n",
      "Iteration 3997, loss = 0.02706643\n",
      "Iteration 3998, loss = 0.02706200\n",
      "Iteration 3999, loss = 0.02706390\n",
      "Iteration 4000, loss = 0.02706556\n",
      "Iteration 4001, loss = 0.02704943\n",
      "Iteration 4002, loss = 0.02704608\n",
      "Iteration 4003, loss = 0.02704274\n",
      "Iteration 4004, loss = 0.02705012\n",
      "Iteration 4005, loss = 0.02703709\n",
      "Iteration 4006, loss = 0.02703154\n",
      "Iteration 4007, loss = 0.02703008\n",
      "Iteration 4008, loss = 0.02702271\n",
      "Iteration 4009, loss = 0.02702131\n",
      "Iteration 4010, loss = 0.02701747\n",
      "Iteration 4011, loss = 0.02701410\n",
      "Iteration 4012, loss = 0.02701641\n",
      "Iteration 4013, loss = 0.02700470\n",
      "Iteration 4014, loss = 0.02700593\n",
      "Iteration 4015, loss = 0.02699416\n",
      "Iteration 4016, loss = 0.02699838\n",
      "Iteration 4017, loss = 0.02699187\n",
      "Iteration 4018, loss = 0.02698602\n",
      "Iteration 4019, loss = 0.02698188\n",
      "Iteration 4020, loss = 0.02697307\n",
      "Iteration 4021, loss = 0.02697194\n",
      "Iteration 4022, loss = 0.02696812\n",
      "Iteration 4023, loss = 0.02696691\n",
      "Iteration 4024, loss = 0.02695950\n",
      "Iteration 4025, loss = 0.02695853\n",
      "Iteration 4026, loss = 0.02695594\n",
      "Iteration 4027, loss = 0.02695276\n",
      "Iteration 4028, loss = 0.02694070\n",
      "Iteration 4029, loss = 0.02694570\n",
      "Iteration 4030, loss = 0.02693410\n",
      "Iteration 4031, loss = 0.02693280\n",
      "Iteration 4032, loss = 0.02692873\n",
      "Iteration 4033, loss = 0.02692490\n",
      "Iteration 4034, loss = 0.02691764\n",
      "Iteration 4035, loss = 0.02691677\n",
      "Iteration 4036, loss = 0.02691669\n",
      "Iteration 4037, loss = 0.02690703\n",
      "Iteration 4038, loss = 0.02690989\n",
      "Iteration 4039, loss = 0.02690087\n",
      "Iteration 4040, loss = 0.02689580\n",
      "Iteration 4041, loss = 0.02689423\n",
      "Iteration 4042, loss = 0.02689149\n",
      "Iteration 4043, loss = 0.02688333\n",
      "Iteration 4044, loss = 0.02688922\n",
      "Iteration 4045, loss = 0.02688404\n",
      "Iteration 4046, loss = 0.02686915\n",
      "Iteration 4047, loss = 0.02687065\n",
      "Iteration 4048, loss = 0.02686076\n",
      "Iteration 4049, loss = 0.02686058\n",
      "Iteration 4050, loss = 0.02685897\n",
      "Iteration 4051, loss = 0.02685721\n",
      "Iteration 4052, loss = 0.02685127\n",
      "Iteration 4053, loss = 0.02684595\n",
      "Iteration 4054, loss = 0.02684023\n",
      "Iteration 4055, loss = 0.02683038\n",
      "Iteration 4056, loss = 0.02683776\n",
      "Iteration 4057, loss = 0.02682624\n",
      "Iteration 4058, loss = 0.02682708\n",
      "Iteration 4059, loss = 0.02682283\n",
      "Iteration 4060, loss = 0.02681738\n",
      "Iteration 4061, loss = 0.02681411\n",
      "Iteration 4062, loss = 0.02680825\n",
      "Iteration 4063, loss = 0.02680765\n",
      "Iteration 4064, loss = 0.02679927\n",
      "Iteration 4065, loss = 0.02679665\n",
      "Iteration 4066, loss = 0.02679261\n",
      "Iteration 4067, loss = 0.02679318\n",
      "Iteration 4068, loss = 0.02678868\n",
      "Iteration 4069, loss = 0.02677644\n",
      "Iteration 4070, loss = 0.02677993\n",
      "Iteration 4071, loss = 0.02677315\n",
      "Iteration 4072, loss = 0.02676949\n",
      "Iteration 4073, loss = 0.02676678\n",
      "Iteration 4074, loss = 0.02676088\n",
      "Iteration 4075, loss = 0.02676175\n",
      "Iteration 4076, loss = 0.02675135\n",
      "Iteration 4077, loss = 0.02674830\n",
      "Iteration 4078, loss = 0.02674507\n",
      "Iteration 4079, loss = 0.02673830\n",
      "Iteration 4080, loss = 0.02673917\n",
      "Iteration 4081, loss = 0.02673204\n",
      "Iteration 4082, loss = 0.02672530\n",
      "Iteration 4083, loss = 0.02672458\n",
      "Iteration 4084, loss = 0.02672126\n",
      "Iteration 4085, loss = 0.02671280\n",
      "Iteration 4086, loss = 0.02671123\n",
      "Iteration 4087, loss = 0.02671023\n",
      "Iteration 4088, loss = 0.02670371\n",
      "Iteration 4089, loss = 0.02670145\n",
      "Iteration 4090, loss = 0.02669648\n",
      "Iteration 4091, loss = 0.02669157\n",
      "Iteration 4092, loss = 0.02668660\n",
      "Iteration 4093, loss = 0.02668790\n",
      "Iteration 4094, loss = 0.02668183\n",
      "Iteration 4095, loss = 0.02667939\n",
      "Iteration 4096, loss = 0.02667274\n",
      "Iteration 4097, loss = 0.02666720\n",
      "Iteration 4098, loss = 0.02666916\n",
      "Iteration 4099, loss = 0.02665895\n",
      "Iteration 4100, loss = 0.02666423\n",
      "Iteration 4101, loss = 0.02666216\n",
      "Iteration 4102, loss = 0.02664940\n",
      "Iteration 4103, loss = 0.02664790\n",
      "Iteration 4104, loss = 0.02664489\n",
      "Iteration 4105, loss = 0.02664131\n",
      "Iteration 4106, loss = 0.02663692\n",
      "Iteration 4107, loss = 0.02663041\n",
      "Iteration 4108, loss = 0.02662679\n",
      "Iteration 4109, loss = 0.02661986\n",
      "Iteration 4110, loss = 0.02661419\n",
      "Iteration 4111, loss = 0.02661316\n",
      "Iteration 4112, loss = 0.02661335\n",
      "Iteration 4113, loss = 0.02660444\n",
      "Iteration 4114, loss = 0.02659938\n",
      "Iteration 4115, loss = 0.02659782\n",
      "Iteration 4116, loss = 0.02659487\n",
      "Iteration 4117, loss = 0.02659107\n",
      "Iteration 4118, loss = 0.02659927\n",
      "Iteration 4119, loss = 0.02659702\n",
      "Iteration 4120, loss = 0.02657632\n",
      "Iteration 4121, loss = 0.02657470\n",
      "Iteration 4122, loss = 0.02657715\n",
      "Iteration 4123, loss = 0.02656860\n",
      "Iteration 4124, loss = 0.02656610\n",
      "Iteration 4125, loss = 0.02656113\n",
      "Iteration 4126, loss = 0.02655772\n",
      "Iteration 4127, loss = 0.02655279\n",
      "Iteration 4128, loss = 0.02654887\n",
      "Iteration 4129, loss = 0.02654311\n",
      "Iteration 4130, loss = 0.02654352\n",
      "Iteration 4131, loss = 0.02653379\n",
      "Iteration 4132, loss = 0.02653274\n",
      "Iteration 4133, loss = 0.02652901\n",
      "Iteration 4134, loss = 0.02652118\n",
      "Iteration 4135, loss = 0.02651761\n",
      "Iteration 4136, loss = 0.02651925\n",
      "Iteration 4137, loss = 0.02651291\n",
      "Iteration 4138, loss = 0.02651202\n",
      "Iteration 4139, loss = 0.02650293\n",
      "Iteration 4140, loss = 0.02649955\n",
      "Iteration 4141, loss = 0.02650005\n",
      "Iteration 4142, loss = 0.02649366\n",
      "Iteration 4143, loss = 0.02648664\n",
      "Iteration 4144, loss = 0.02648409\n",
      "Iteration 4145, loss = 0.02648240\n",
      "Iteration 4146, loss = 0.02647647\n",
      "Iteration 4147, loss = 0.02647234\n",
      "Iteration 4148, loss = 0.02646997\n",
      "Iteration 4149, loss = 0.02646456\n",
      "Iteration 4150, loss = 0.02646049\n",
      "Iteration 4151, loss = 0.02645942\n",
      "Iteration 4152, loss = 0.02645477\n",
      "Iteration 4153, loss = 0.02645092\n",
      "Iteration 4154, loss = 0.02644847\n",
      "Iteration 4155, loss = 0.02644307\n",
      "Iteration 4156, loss = 0.02644226\n",
      "Iteration 4157, loss = 0.02643618\n",
      "Iteration 4158, loss = 0.02643168\n",
      "Iteration 4159, loss = 0.02642294\n",
      "Iteration 4160, loss = 0.02642550\n",
      "Iteration 4161, loss = 0.02641514\n",
      "Iteration 4162, loss = 0.02641303\n",
      "Iteration 4163, loss = 0.02641310\n",
      "Iteration 4164, loss = 0.02640843\n",
      "Iteration 4165, loss = 0.02640728\n",
      "Iteration 4166, loss = 0.02640028\n",
      "Iteration 4167, loss = 0.02639724\n",
      "Iteration 4168, loss = 0.02639774\n",
      "Iteration 4169, loss = 0.02639609\n",
      "Iteration 4170, loss = 0.02638642\n",
      "Iteration 4171, loss = 0.02638197\n",
      "Iteration 4172, loss = 0.02637588\n",
      "Iteration 4173, loss = 0.02637703\n",
      "Iteration 4174, loss = 0.02637303\n",
      "Iteration 4175, loss = 0.02636706\n",
      "Iteration 4176, loss = 0.02636155\n",
      "Iteration 4177, loss = 0.02635986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4178, loss = 0.02635151\n",
      "Iteration 4179, loss = 0.02635023\n",
      "Iteration 4180, loss = 0.02634448\n",
      "Iteration 4181, loss = 0.02634136\n",
      "Iteration 4182, loss = 0.02633583\n",
      "Iteration 4183, loss = 0.02633596\n",
      "Iteration 4184, loss = 0.02632813\n",
      "Iteration 4185, loss = 0.02632834\n",
      "Iteration 4186, loss = 0.02631976\n",
      "Iteration 4187, loss = 0.02631896\n",
      "Iteration 4188, loss = 0.02632677\n",
      "Iteration 4189, loss = 0.02631507\n",
      "Iteration 4190, loss = 0.02631782\n",
      "Iteration 4191, loss = 0.02629883\n",
      "Iteration 4192, loss = 0.02629654\n",
      "Iteration 4193, loss = 0.02629509\n",
      "Iteration 4194, loss = 0.02629852\n",
      "Iteration 4195, loss = 0.02629548\n",
      "Iteration 4196, loss = 0.02628998\n",
      "Iteration 4197, loss = 0.02628543\n",
      "Iteration 4198, loss = 0.02627861\n",
      "Iteration 4199, loss = 0.02628101\n",
      "Iteration 4200, loss = 0.02626716\n",
      "Iteration 4201, loss = 0.02626458\n",
      "Iteration 4202, loss = 0.02625760\n",
      "Iteration 4203, loss = 0.02626728\n",
      "Iteration 4204, loss = 0.02625655\n",
      "Iteration 4205, loss = 0.02625585\n",
      "Iteration 4206, loss = 0.02625329\n",
      "Iteration 4207, loss = 0.02624233\n",
      "Iteration 4208, loss = 0.02623957\n",
      "Iteration 4209, loss = 0.02623495\n",
      "Iteration 4210, loss = 0.02622634\n",
      "Iteration 4211, loss = 0.02623038\n",
      "Iteration 4212, loss = 0.02622640\n",
      "Iteration 4213, loss = 0.02621861\n",
      "Iteration 4214, loss = 0.02621237\n",
      "Iteration 4215, loss = 0.02621098\n",
      "Iteration 4216, loss = 0.02621073\n",
      "Iteration 4217, loss = 0.02620203\n",
      "Iteration 4218, loss = 0.02620002\n",
      "Iteration 4219, loss = 0.02619921\n",
      "Iteration 4220, loss = 0.02619067\n",
      "Iteration 4221, loss = 0.02620037\n",
      "Iteration 4222, loss = 0.02618560\n",
      "Iteration 4223, loss = 0.02618049\n",
      "Iteration 4224, loss = 0.02617919\n",
      "Iteration 4225, loss = 0.02617516\n",
      "Iteration 4226, loss = 0.02617807\n",
      "Iteration 4227, loss = 0.02616722\n",
      "Iteration 4228, loss = 0.02616385\n",
      "Iteration 4229, loss = 0.02616371\n",
      "Iteration 4230, loss = 0.02615468\n",
      "Iteration 4231, loss = 0.02614408\n",
      "Iteration 4232, loss = 0.02614896\n",
      "Iteration 4233, loss = 0.02614847\n",
      "Iteration 4234, loss = 0.02614216\n",
      "Iteration 4235, loss = 0.02613635\n",
      "Iteration 4236, loss = 0.02613869\n",
      "Iteration 4237, loss = 0.02612245\n",
      "Iteration 4238, loss = 0.02612162\n",
      "Iteration 4239, loss = 0.02611870\n",
      "Iteration 4240, loss = 0.02612553\n",
      "Iteration 4241, loss = 0.02611331\n",
      "Iteration 4242, loss = 0.02611201\n",
      "Iteration 4243, loss = 0.02610840\n",
      "Iteration 4244, loss = 0.02609774\n",
      "Iteration 4245, loss = 0.02609661\n",
      "Iteration 4246, loss = 0.02610473\n",
      "Iteration 4247, loss = 0.02609162\n",
      "Iteration 4248, loss = 0.02608855\n",
      "Iteration 4249, loss = 0.02608661\n",
      "Iteration 4250, loss = 0.02608266\n",
      "Iteration 4251, loss = 0.02607153\n",
      "Iteration 4252, loss = 0.02607375\n",
      "Iteration 4253, loss = 0.02606746\n",
      "Iteration 4254, loss = 0.02606418\n",
      "Iteration 4255, loss = 0.02606385\n",
      "Iteration 4256, loss = 0.02606074\n",
      "Iteration 4257, loss = 0.02605083\n",
      "Iteration 4258, loss = 0.02604777\n",
      "Iteration 4259, loss = 0.02604354\n",
      "Iteration 4260, loss = 0.02604063\n",
      "Iteration 4261, loss = 0.02604246\n",
      "Iteration 4262, loss = 0.02603148\n",
      "Iteration 4263, loss = 0.02603347\n",
      "Iteration 4264, loss = 0.02602495\n",
      "Iteration 4265, loss = 0.02601968\n",
      "Iteration 4266, loss = 0.02601623\n",
      "Iteration 4267, loss = 0.02601622\n",
      "Iteration 4268, loss = 0.02601167\n",
      "Iteration 4269, loss = 0.02600431\n",
      "Iteration 4270, loss = 0.02600290\n",
      "Iteration 4271, loss = 0.02600599\n",
      "Iteration 4272, loss = 0.02599982\n",
      "Iteration 4273, loss = 0.02599426\n",
      "Iteration 4274, loss = 0.02599580\n",
      "Iteration 4275, loss = 0.02598100\n",
      "Iteration 4276, loss = 0.02598481\n",
      "Iteration 4277, loss = 0.02598066\n",
      "Iteration 4278, loss = 0.02597512\n",
      "Iteration 4279, loss = 0.02597422\n",
      "Iteration 4280, loss = 0.02596702\n",
      "Iteration 4281, loss = 0.02596311\n",
      "Iteration 4282, loss = 0.02597467\n",
      "Iteration 4283, loss = 0.02595510\n",
      "Iteration 4284, loss = 0.02595875\n",
      "Iteration 4285, loss = 0.02594862\n",
      "Iteration 4286, loss = 0.02595115\n",
      "Iteration 4287, loss = 0.02593974\n",
      "Iteration 4288, loss = 0.02594032\n",
      "Iteration 4289, loss = 0.02593774\n",
      "Iteration 4290, loss = 0.02593475\n",
      "Iteration 4291, loss = 0.02593382\n",
      "Iteration 4292, loss = 0.02591868\n",
      "Iteration 4293, loss = 0.02591778\n",
      "Iteration 4294, loss = 0.02591653\n",
      "Iteration 4295, loss = 0.02591326\n",
      "Iteration 4296, loss = 0.02590253\n",
      "Iteration 4297, loss = 0.02590276\n",
      "Iteration 4298, loss = 0.02590466\n",
      "Iteration 4299, loss = 0.02589418\n",
      "Iteration 4300, loss = 0.02589594\n",
      "Iteration 4301, loss = 0.02589726\n",
      "Iteration 4302, loss = 0.02588966\n",
      "Iteration 4303, loss = 0.02588182\n",
      "Iteration 4304, loss = 0.02587470\n",
      "Iteration 4305, loss = 0.02587969\n",
      "Iteration 4306, loss = 0.02587393\n",
      "Iteration 4307, loss = 0.02587505\n",
      "Iteration 4308, loss = 0.02586047\n",
      "Iteration 4309, loss = 0.02585909\n",
      "Iteration 4310, loss = 0.02585815\n",
      "Iteration 4311, loss = 0.02585435\n",
      "Iteration 4312, loss = 0.02584849\n",
      "Iteration 4313, loss = 0.02584621\n",
      "Iteration 4314, loss = 0.02584413\n",
      "Iteration 4315, loss = 0.02584481\n",
      "Iteration 4316, loss = 0.02584096\n",
      "Iteration 4317, loss = 0.02583753\n",
      "Iteration 4318, loss = 0.02582797\n",
      "Iteration 4319, loss = 0.02582155\n",
      "Iteration 4320, loss = 0.02581667\n",
      "Iteration 4321, loss = 0.02581566\n",
      "Iteration 4322, loss = 0.02581314\n",
      "Iteration 4323, loss = 0.02580772\n",
      "Iteration 4324, loss = 0.02580162\n",
      "Iteration 4325, loss = 0.02580205\n",
      "Iteration 4326, loss = 0.02580267\n",
      "Iteration 4327, loss = 0.02579643\n",
      "Iteration 4328, loss = 0.02578953\n",
      "Iteration 4329, loss = 0.02578758\n",
      "Iteration 4330, loss = 0.02578239\n",
      "Iteration 4331, loss = 0.02578047\n",
      "Iteration 4332, loss = 0.02578298\n",
      "Iteration 4333, loss = 0.02577251\n",
      "Iteration 4334, loss = 0.02576996\n",
      "Iteration 4335, loss = 0.02576609\n",
      "Iteration 4336, loss = 0.02576811\n",
      "Iteration 4337, loss = 0.02575846\n",
      "Iteration 4338, loss = 0.02575431\n",
      "Iteration 4339, loss = 0.02575120\n",
      "Iteration 4340, loss = 0.02574962\n",
      "Iteration 4341, loss = 0.02574734\n",
      "Iteration 4342, loss = 0.02573681\n",
      "Iteration 4343, loss = 0.02573613\n",
      "Iteration 4344, loss = 0.02572935\n",
      "Iteration 4345, loss = 0.02573210\n",
      "Iteration 4346, loss = 0.02572970\n",
      "Iteration 4347, loss = 0.02572339\n",
      "Iteration 4348, loss = 0.02572054\n",
      "Iteration 4349, loss = 0.02571299\n",
      "Iteration 4350, loss = 0.02571265\n",
      "Iteration 4351, loss = 0.02570586\n",
      "Iteration 4352, loss = 0.02570168\n",
      "Iteration 4353, loss = 0.02570299\n",
      "Iteration 4354, loss = 0.02570411\n",
      "Iteration 4355, loss = 0.02569310\n",
      "Iteration 4356, loss = 0.02568473\n",
      "Iteration 4357, loss = 0.02568606\n",
      "Iteration 4358, loss = 0.02568508\n",
      "Iteration 4359, loss = 0.02568238\n",
      "Iteration 4360, loss = 0.02567701\n",
      "Iteration 4361, loss = 0.02566843\n",
      "Iteration 4362, loss = 0.02567011\n",
      "Iteration 4363, loss = 0.02566274\n",
      "Iteration 4364, loss = 0.02565620\n",
      "Iteration 4365, loss = 0.02565834\n",
      "Iteration 4366, loss = 0.02565360\n",
      "Iteration 4367, loss = 0.02565015\n",
      "Iteration 4368, loss = 0.02564302\n",
      "Iteration 4369, loss = 0.02564152\n",
      "Iteration 4370, loss = 0.02563940\n",
      "Iteration 4371, loss = 0.02563612\n",
      "Iteration 4372, loss = 0.02563415\n",
      "Iteration 4373, loss = 0.02563294\n",
      "Iteration 4374, loss = 0.02562982\n",
      "Iteration 4375, loss = 0.02563131\n",
      "Iteration 4376, loss = 0.02562068\n",
      "Iteration 4377, loss = 0.02561750\n",
      "Iteration 4378, loss = 0.02560973\n",
      "Iteration 4379, loss = 0.02560655\n",
      "Iteration 4380, loss = 0.02560498\n",
      "Iteration 4381, loss = 0.02559310\n",
      "Iteration 4382, loss = 0.02559739\n",
      "Iteration 4383, loss = 0.02559170\n",
      "Iteration 4384, loss = 0.02558886\n",
      "Iteration 4385, loss = 0.02558802\n",
      "Iteration 4386, loss = 0.02558036\n",
      "Iteration 4387, loss = 0.02557309\n",
      "Iteration 4388, loss = 0.02557594\n",
      "Iteration 4389, loss = 0.02556822\n",
      "Iteration 4390, loss = 0.02557091\n",
      "Iteration 4391, loss = 0.02556572\n",
      "Iteration 4392, loss = 0.02555823\n",
      "Iteration 4393, loss = 0.02555717\n",
      "Iteration 4394, loss = 0.02555307\n",
      "Iteration 4395, loss = 0.02554709\n",
      "Iteration 4396, loss = 0.02554223\n",
      "Iteration 4397, loss = 0.02553830\n",
      "Iteration 4398, loss = 0.02553841\n",
      "Iteration 4399, loss = 0.02553400\n",
      "Iteration 4400, loss = 0.02552663\n",
      "Iteration 4401, loss = 0.02552355\n",
      "Iteration 4402, loss = 0.02552625\n",
      "Iteration 4403, loss = 0.02552209\n",
      "Iteration 4404, loss = 0.02551678\n",
      "Iteration 4405, loss = 0.02551615\n",
      "Iteration 4406, loss = 0.02550958\n",
      "Iteration 4407, loss = 0.02550506\n",
      "Iteration 4408, loss = 0.02549837\n",
      "Iteration 4409, loss = 0.02550457\n",
      "Iteration 4410, loss = 0.02549563\n",
      "Iteration 4411, loss = 0.02549637\n",
      "Iteration 4412, loss = 0.02548697\n",
      "Iteration 4413, loss = 0.02548734\n",
      "Iteration 4414, loss = 0.02548220\n",
      "Iteration 4415, loss = 0.02547752\n",
      "Iteration 4416, loss = 0.02547044\n",
      "Iteration 4417, loss = 0.02547025\n",
      "Iteration 4418, loss = 0.02546986\n",
      "Iteration 4419, loss = 0.02546234\n",
      "Iteration 4420, loss = 0.02546138\n",
      "Iteration 4421, loss = 0.02545776\n",
      "Iteration 4422, loss = 0.02544846\n",
      "Iteration 4423, loss = 0.02545166\n",
      "Iteration 4424, loss = 0.02544332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4425, loss = 0.02544083\n",
      "Iteration 4426, loss = 0.02543492\n",
      "Iteration 4427, loss = 0.02543319\n",
      "Iteration 4428, loss = 0.02542828\n",
      "Iteration 4429, loss = 0.02543124\n",
      "Iteration 4430, loss = 0.02542222\n",
      "Iteration 4431, loss = 0.02541678\n",
      "Iteration 4432, loss = 0.02541447\n",
      "Iteration 4433, loss = 0.02541103\n",
      "Iteration 4434, loss = 0.02540666\n",
      "Iteration 4435, loss = 0.02540600\n",
      "Iteration 4436, loss = 0.02540411\n",
      "Iteration 4437, loss = 0.02539796\n",
      "Iteration 4438, loss = 0.02540133\n",
      "Iteration 4439, loss = 0.02538914\n",
      "Iteration 4440, loss = 0.02538953\n",
      "Iteration 4441, loss = 0.02538064\n",
      "Iteration 4442, loss = 0.02537617\n",
      "Iteration 4443, loss = 0.02537794\n",
      "Iteration 4444, loss = 0.02537247\n",
      "Iteration 4445, loss = 0.02537245\n",
      "Iteration 4446, loss = 0.02536624\n",
      "Iteration 4447, loss = 0.02536495\n",
      "Iteration 4448, loss = 0.02536373\n",
      "Iteration 4449, loss = 0.02535755\n",
      "Iteration 4450, loss = 0.02535245\n",
      "Iteration 4451, loss = 0.02534543\n",
      "Iteration 4452, loss = 0.02534770\n",
      "Iteration 4453, loss = 0.02534131\n",
      "Iteration 4454, loss = 0.02534269\n",
      "Iteration 4455, loss = 0.02534375\n",
      "Iteration 4456, loss = 0.02533246\n",
      "Iteration 4457, loss = 0.02532644\n",
      "Iteration 4458, loss = 0.02532464\n",
      "Iteration 4459, loss = 0.02532876\n",
      "Iteration 4460, loss = 0.02532167\n",
      "Iteration 4461, loss = 0.02531494\n",
      "Iteration 4462, loss = 0.02530686\n",
      "Iteration 4463, loss = 0.02530646\n",
      "Iteration 4464, loss = 0.02530432\n",
      "Iteration 4465, loss = 0.02529630\n",
      "Iteration 4466, loss = 0.02529973\n",
      "Iteration 4467, loss = 0.02529078\n",
      "Iteration 4468, loss = 0.02528744\n",
      "Iteration 4469, loss = 0.02528663\n",
      "Iteration 4470, loss = 0.02528427\n",
      "Iteration 4471, loss = 0.02528149\n",
      "Iteration 4472, loss = 0.02527477\n",
      "Iteration 4473, loss = 0.02527121\n",
      "Iteration 4474, loss = 0.02526829\n",
      "Iteration 4475, loss = 0.02526594\n",
      "Iteration 4476, loss = 0.02526571\n",
      "Iteration 4477, loss = 0.02525942\n",
      "Iteration 4478, loss = 0.02525437\n",
      "Iteration 4479, loss = 0.02525285\n",
      "Iteration 4480, loss = 0.02524115\n",
      "Iteration 4481, loss = 0.02524277\n",
      "Iteration 4482, loss = 0.02524484\n",
      "Iteration 4483, loss = 0.02523584\n",
      "Iteration 4484, loss = 0.02523219\n",
      "Iteration 4485, loss = 0.02523192\n",
      "Iteration 4486, loss = 0.02522797\n",
      "Iteration 4487, loss = 0.02522231\n",
      "Iteration 4488, loss = 0.02521318\n",
      "Iteration 4489, loss = 0.02521574\n",
      "Iteration 4490, loss = 0.02520972\n",
      "Iteration 4491, loss = 0.02520729\n",
      "Iteration 4492, loss = 0.02520632\n",
      "Iteration 4493, loss = 0.02519887\n",
      "Iteration 4494, loss = 0.02520159\n",
      "Iteration 4495, loss = 0.02519403\n",
      "Iteration 4496, loss = 0.02519108\n",
      "Iteration 4497, loss = 0.02518425\n",
      "Iteration 4498, loss = 0.02517908\n",
      "Iteration 4499, loss = 0.02517568\n",
      "Iteration 4500, loss = 0.02517101\n",
      "Iteration 4501, loss = 0.02517722\n",
      "Iteration 4502, loss = 0.02517195\n",
      "Iteration 4503, loss = 0.02516370\n",
      "Iteration 4504, loss = 0.02517136\n",
      "Iteration 4505, loss = 0.02516525\n",
      "Iteration 4506, loss = 0.02515004\n",
      "Iteration 4507, loss = 0.02514419\n",
      "Iteration 4508, loss = 0.02514283\n",
      "Iteration 4509, loss = 0.02514503\n",
      "Iteration 4510, loss = 0.02514103\n",
      "Iteration 4511, loss = 0.02513357\n",
      "Iteration 4512, loss = 0.02513324\n",
      "Iteration 4513, loss = 0.02513414\n",
      "Iteration 4514, loss = 0.02512589\n",
      "Iteration 4515, loss = 0.02511951\n",
      "Iteration 4516, loss = 0.02511772\n",
      "Iteration 4517, loss = 0.02511151\n",
      "Iteration 4518, loss = 0.02510893\n",
      "Iteration 4519, loss = 0.02511035\n",
      "Iteration 4520, loss = 0.02510420\n",
      "Iteration 4521, loss = 0.02509967\n",
      "Iteration 4522, loss = 0.02509197\n",
      "Iteration 4523, loss = 0.02509231\n",
      "Iteration 4524, loss = 0.02509002\n",
      "Iteration 4525, loss = 0.02509316\n",
      "Iteration 4526, loss = 0.02507932\n",
      "Iteration 4527, loss = 0.02507765\n",
      "Iteration 4528, loss = 0.02507680\n",
      "Iteration 4529, loss = 0.02507025\n",
      "Iteration 4530, loss = 0.02506335\n",
      "Iteration 4531, loss = 0.02506511\n",
      "Iteration 4532, loss = 0.02505855\n",
      "Iteration 4533, loss = 0.02505373\n",
      "Iteration 4534, loss = 0.02505116\n",
      "Iteration 4535, loss = 0.02504623\n",
      "Iteration 4536, loss = 0.02504537\n",
      "Iteration 4537, loss = 0.02504555\n",
      "Iteration 4538, loss = 0.02504156\n",
      "Iteration 4539, loss = 0.02503861\n",
      "Iteration 4540, loss = 0.02503433\n",
      "Iteration 4541, loss = 0.02503593\n",
      "Iteration 4542, loss = 0.02502791\n",
      "Iteration 4543, loss = 0.02501743\n",
      "Iteration 4544, loss = 0.02501953\n",
      "Iteration 4545, loss = 0.02501554\n",
      "Iteration 4546, loss = 0.02501134\n",
      "Iteration 4547, loss = 0.02501543\n",
      "Iteration 4548, loss = 0.02501060\n",
      "Iteration 4549, loss = 0.02500572\n",
      "Iteration 4550, loss = 0.02499631\n",
      "Iteration 4551, loss = 0.02498977\n",
      "Iteration 4552, loss = 0.02498908\n",
      "Iteration 4553, loss = 0.02498689\n",
      "Iteration 4554, loss = 0.02498816\n",
      "Iteration 4555, loss = 0.02498190\n",
      "Iteration 4556, loss = 0.02498430\n",
      "Iteration 4557, loss = 0.02497319\n",
      "Iteration 4558, loss = 0.02497142\n",
      "Iteration 4559, loss = 0.02497063\n",
      "Iteration 4560, loss = 0.02496892\n",
      "Iteration 4561, loss = 0.02495969\n",
      "Iteration 4562, loss = 0.02495657\n",
      "Iteration 4563, loss = 0.02495276\n",
      "Iteration 4564, loss = 0.02494783\n",
      "Iteration 4565, loss = 0.02495365\n",
      "Iteration 4566, loss = 0.02494683\n",
      "Iteration 4567, loss = 0.02493602\n",
      "Iteration 4568, loss = 0.02493406\n",
      "Iteration 4569, loss = 0.02492872\n",
      "Iteration 4570, loss = 0.02492678\n",
      "Iteration 4571, loss = 0.02491945\n",
      "Iteration 4572, loss = 0.02492515\n",
      "Iteration 4573, loss = 0.02491728\n",
      "Iteration 4574, loss = 0.02490983\n",
      "Iteration 4575, loss = 0.02491136\n",
      "Iteration 4576, loss = 0.02490809\n",
      "Iteration 4577, loss = 0.02490612\n",
      "Iteration 4578, loss = 0.02490040\n",
      "Iteration 4579, loss = 0.02490507\n",
      "Iteration 4580, loss = 0.02489495\n",
      "Iteration 4581, loss = 0.02488784\n",
      "Iteration 4582, loss = 0.02488861\n",
      "Iteration 4583, loss = 0.02488561\n",
      "Iteration 4584, loss = 0.02488644\n",
      "Iteration 4585, loss = 0.02487528\n",
      "Iteration 4586, loss = 0.02487985\n",
      "Iteration 4587, loss = 0.02487223\n",
      "Iteration 4588, loss = 0.02486427\n",
      "Iteration 4589, loss = 0.02486388\n",
      "Iteration 4590, loss = 0.02485444\n",
      "Iteration 4591, loss = 0.02485391\n",
      "Iteration 4592, loss = 0.02485587\n",
      "Iteration 4593, loss = 0.02484522\n",
      "Iteration 4594, loss = 0.02484347\n",
      "Iteration 4595, loss = 0.02484129\n",
      "Iteration 4596, loss = 0.02483762\n",
      "Iteration 4597, loss = 0.02483547\n",
      "Iteration 4598, loss = 0.02483633\n",
      "Iteration 4599, loss = 0.02482664\n",
      "Iteration 4600, loss = 0.02482398\n",
      "Iteration 4601, loss = 0.02482313\n",
      "Iteration 4602, loss = 0.02482170\n",
      "Iteration 4603, loss = 0.02481598\n",
      "Iteration 4604, loss = 0.02481593\n",
      "Iteration 4605, loss = 0.02480604\n",
      "Iteration 4606, loss = 0.02480755\n",
      "Iteration 4607, loss = 0.02480577\n",
      "Iteration 4608, loss = 0.02480084\n",
      "Iteration 4609, loss = 0.02479446\n",
      "Iteration 4610, loss = 0.02478533\n",
      "Iteration 4611, loss = 0.02479604\n",
      "Iteration 4612, loss = 0.02478650\n",
      "Iteration 4613, loss = 0.02477828\n",
      "Iteration 4614, loss = 0.02477433\n",
      "Iteration 4615, loss = 0.02478090\n",
      "Iteration 4616, loss = 0.02477214\n",
      "Iteration 4617, loss = 0.02476913\n",
      "Iteration 4618, loss = 0.02476511\n",
      "Iteration 4619, loss = 0.02476110\n",
      "Iteration 4620, loss = 0.02476622\n",
      "Iteration 4621, loss = 0.02475198\n",
      "Iteration 4622, loss = 0.02475552\n",
      "Iteration 4623, loss = 0.02475468\n",
      "Iteration 4624, loss = 0.02474015\n",
      "Iteration 4625, loss = 0.02474716\n",
      "Iteration 4626, loss = 0.02473856\n",
      "Iteration 4627, loss = 0.02473325\n",
      "Iteration 4628, loss = 0.02472835\n",
      "Iteration 4629, loss = 0.02473243\n",
      "Iteration 4630, loss = 0.02472440\n",
      "Iteration 4631, loss = 0.02472065\n",
      "Iteration 4632, loss = 0.02471806\n",
      "Iteration 4633, loss = 0.02471676\n",
      "Iteration 4634, loss = 0.02471094\n",
      "Iteration 4635, loss = 0.02470572\n",
      "Iteration 4636, loss = 0.02470519\n",
      "Iteration 4637, loss = 0.02470263\n",
      "Iteration 4638, loss = 0.02469766\n",
      "Iteration 4639, loss = 0.02469144\n",
      "Iteration 4640, loss = 0.02469657\n",
      "Iteration 4641, loss = 0.02468692\n",
      "Iteration 4642, loss = 0.02467938\n",
      "Iteration 4643, loss = 0.02467861\n",
      "Iteration 4644, loss = 0.02467571\n",
      "Iteration 4645, loss = 0.02466838\n",
      "Iteration 4646, loss = 0.02466674\n",
      "Iteration 4647, loss = 0.02466663\n",
      "Iteration 4648, loss = 0.02466494\n",
      "Iteration 4649, loss = 0.02465782\n",
      "Iteration 4650, loss = 0.02465216\n",
      "Iteration 4651, loss = 0.02464867\n",
      "Iteration 4652, loss = 0.02465066\n",
      "Iteration 4653, loss = 0.02464052\n",
      "Iteration 4654, loss = 0.02463647\n",
      "Iteration 4655, loss = 0.02463610\n",
      "Iteration 4656, loss = 0.02463403\n",
      "Iteration 4657, loss = 0.02463181\n",
      "Iteration 4658, loss = 0.02462700\n",
      "Iteration 4659, loss = 0.02463183\n",
      "Iteration 4660, loss = 0.02461850\n",
      "Iteration 4661, loss = 0.02461574\n",
      "Iteration 4662, loss = 0.02462771\n",
      "Iteration 4663, loss = 0.02461323\n",
      "Iteration 4664, loss = 0.02460394\n",
      "Iteration 4665, loss = 0.02459841\n",
      "Iteration 4666, loss = 0.02460190\n",
      "Iteration 4667, loss = 0.02460166\n",
      "Iteration 4668, loss = 0.02458968\n",
      "Iteration 4669, loss = 0.02458723\n",
      "Iteration 4670, loss = 0.02459201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4671, loss = 0.02458964\n",
      "Iteration 4672, loss = 0.02458015\n",
      "Iteration 4673, loss = 0.02457502\n",
      "Iteration 4674, loss = 0.02457243\n",
      "Iteration 4675, loss = 0.02456849\n",
      "Iteration 4676, loss = 0.02456684\n",
      "Iteration 4677, loss = 0.02455832\n",
      "Iteration 4678, loss = 0.02456205\n",
      "Iteration 4679, loss = 0.02455144\n",
      "Iteration 4680, loss = 0.02455212\n",
      "Iteration 4681, loss = 0.02454853\n",
      "Iteration 4682, loss = 0.02453957\n",
      "Iteration 4683, loss = 0.02454130\n",
      "Iteration 4684, loss = 0.02453360\n",
      "Iteration 4685, loss = 0.02453538\n",
      "Iteration 4686, loss = 0.02453538\n",
      "Iteration 4687, loss = 0.02452741\n",
      "Iteration 4688, loss = 0.02452741\n",
      "Iteration 4689, loss = 0.02451820\n",
      "Iteration 4690, loss = 0.02451970\n",
      "Iteration 4691, loss = 0.02451302\n",
      "Iteration 4692, loss = 0.02451034\n",
      "Iteration 4693, loss = 0.02450599\n",
      "Iteration 4694, loss = 0.02450302\n",
      "Iteration 4695, loss = 0.02450454\n",
      "Iteration 4696, loss = 0.02449728\n",
      "Iteration 4697, loss = 0.02449480\n",
      "Iteration 4698, loss = 0.02449327\n",
      "Iteration 4699, loss = 0.02448795\n",
      "Iteration 4700, loss = 0.02448067\n",
      "Iteration 4701, loss = 0.02447972\n",
      "Iteration 4702, loss = 0.02447600\n",
      "Iteration 4703, loss = 0.02448059\n",
      "Iteration 4704, loss = 0.02447727\n",
      "Iteration 4705, loss = 0.02447297\n",
      "Iteration 4706, loss = 0.02446046\n",
      "Iteration 4707, loss = 0.02447573\n",
      "Iteration 4708, loss = 0.02445600\n",
      "Iteration 4709, loss = 0.02445363\n",
      "Iteration 4710, loss = 0.02445569\n",
      "Iteration 4711, loss = 0.02445358\n",
      "Iteration 4712, loss = 0.02443711\n",
      "Iteration 4713, loss = 0.02444606\n",
      "Iteration 4714, loss = 0.02443649\n",
      "Iteration 4715, loss = 0.02443433\n",
      "Iteration 4716, loss = 0.02443294\n",
      "Iteration 4717, loss = 0.02442843\n",
      "Iteration 4718, loss = 0.02441545\n",
      "Iteration 4719, loss = 0.02441830\n",
      "Iteration 4720, loss = 0.02441974\n",
      "Iteration 4721, loss = 0.02441216\n",
      "Iteration 4722, loss = 0.02440809\n",
      "Iteration 4723, loss = 0.02440561\n",
      "Iteration 4724, loss = 0.02439936\n",
      "Iteration 4725, loss = 0.02439756\n",
      "Iteration 4726, loss = 0.02440179\n",
      "Iteration 4727, loss = 0.02438915\n",
      "Iteration 4728, loss = 0.02439374\n",
      "Iteration 4729, loss = 0.02438366\n",
      "Iteration 4730, loss = 0.02438065\n",
      "Iteration 4731, loss = 0.02438456\n",
      "Iteration 4732, loss = 0.02437471\n",
      "Iteration 4733, loss = 0.02437358\n",
      "Iteration 4734, loss = 0.02436736\n",
      "Iteration 4735, loss = 0.02436965\n",
      "Iteration 4736, loss = 0.02435922\n",
      "Iteration 4737, loss = 0.02435287\n",
      "Iteration 4738, loss = 0.02435650\n",
      "Iteration 4739, loss = 0.02435240\n",
      "Iteration 4740, loss = 0.02434308\n",
      "Iteration 4741, loss = 0.02434719\n",
      "Iteration 4742, loss = 0.02434599\n",
      "Iteration 4743, loss = 0.02434170\n",
      "Iteration 4744, loss = 0.02433288\n",
      "Iteration 4745, loss = 0.02434165\n",
      "Iteration 4746, loss = 0.02433329\n",
      "Iteration 4747, loss = 0.02432265\n",
      "Iteration 4748, loss = 0.02432053\n",
      "Iteration 4749, loss = 0.02431588\n",
      "Iteration 4750, loss = 0.02431132\n",
      "Iteration 4751, loss = 0.02431710\n",
      "Iteration 4752, loss = 0.02430762\n",
      "Iteration 4753, loss = 0.02430472\n",
      "Iteration 4754, loss = 0.02431053\n",
      "Iteration 4755, loss = 0.02429756\n",
      "Iteration 4756, loss = 0.02429680\n",
      "Iteration 4757, loss = 0.02429583\n",
      "Iteration 4758, loss = 0.02428585\n",
      "Iteration 4759, loss = 0.02428967\n",
      "Iteration 4760, loss = 0.02428567\n",
      "Iteration 4761, loss = 0.02427695\n",
      "Iteration 4762, loss = 0.02427079\n",
      "Iteration 4763, loss = 0.02427389\n",
      "Iteration 4764, loss = 0.02427625\n",
      "Iteration 4765, loss = 0.02426049\n",
      "Iteration 4766, loss = 0.02425952\n",
      "Iteration 4767, loss = 0.02425619\n",
      "Iteration 4768, loss = 0.02425352\n",
      "Iteration 4769, loss = 0.02424735\n",
      "Iteration 4770, loss = 0.02424713\n",
      "Iteration 4771, loss = 0.02424334\n",
      "Iteration 4772, loss = 0.02423989\n",
      "Iteration 4773, loss = 0.02423881\n",
      "Iteration 4774, loss = 0.02422970\n",
      "Iteration 4775, loss = 0.02423281\n",
      "Iteration 4776, loss = 0.02422413\n",
      "Iteration 4777, loss = 0.02421996\n",
      "Iteration 4778, loss = 0.02421636\n",
      "Iteration 4779, loss = 0.02421661\n",
      "Iteration 4780, loss = 0.02421931\n",
      "Iteration 4781, loss = 0.02421172\n",
      "Iteration 4782, loss = 0.02421069\n",
      "Iteration 4783, loss = 0.02420551\n",
      "Iteration 4784, loss = 0.02419648\n",
      "Iteration 4785, loss = 0.02419436\n",
      "Iteration 4786, loss = 0.02419341\n",
      "Iteration 4787, loss = 0.02419010\n",
      "Iteration 4788, loss = 0.02418434\n",
      "Iteration 4789, loss = 0.02418842\n",
      "Iteration 4790, loss = 0.02418273\n",
      "Iteration 4791, loss = 0.02417191\n",
      "Iteration 4792, loss = 0.02417001\n",
      "Iteration 4793, loss = 0.02417446\n",
      "Iteration 4794, loss = 0.02416759\n",
      "Iteration 4795, loss = 0.02416613\n",
      "Iteration 4796, loss = 0.02416021\n",
      "Iteration 4797, loss = 0.02415602\n",
      "Iteration 4798, loss = 0.02415555\n",
      "Iteration 4799, loss = 0.02414861\n",
      "Iteration 4800, loss = 0.02414968\n",
      "Iteration 4801, loss = 0.02414297\n",
      "Iteration 4802, loss = 0.02413871\n",
      "Iteration 4803, loss = 0.02413697\n",
      "Iteration 4804, loss = 0.02413304\n",
      "Iteration 4805, loss = 0.02412500\n",
      "Iteration 4806, loss = 0.02412725\n",
      "Iteration 4807, loss = 0.02411936\n",
      "Iteration 4808, loss = 0.02412036\n",
      "Iteration 4809, loss = 0.02411775\n",
      "Iteration 4810, loss = 0.02411126\n",
      "Iteration 4811, loss = 0.02410924\n",
      "Iteration 4812, loss = 0.02410835\n",
      "Iteration 4813, loss = 0.02410225\n",
      "Iteration 4814, loss = 0.02410072\n",
      "Iteration 4815, loss = 0.02410040\n",
      "Iteration 4816, loss = 0.02408965\n",
      "Iteration 4817, loss = 0.02409403\n",
      "Iteration 4818, loss = 0.02408230\n",
      "Iteration 4819, loss = 0.02408339\n",
      "Iteration 4820, loss = 0.02407798\n",
      "Iteration 4821, loss = 0.02408364\n",
      "Iteration 4822, loss = 0.02407190\n",
      "Iteration 4823, loss = 0.02407549\n",
      "Iteration 4824, loss = 0.02406184\n",
      "Iteration 4825, loss = 0.02406566\n",
      "Iteration 4826, loss = 0.02405960\n",
      "Iteration 4827, loss = 0.02405534\n",
      "Iteration 4828, loss = 0.02405343\n",
      "Iteration 4829, loss = 0.02404935\n",
      "Iteration 4830, loss = 0.02404751\n",
      "Iteration 4831, loss = 0.02404228\n",
      "Iteration 4832, loss = 0.02404176\n",
      "Iteration 4833, loss = 0.02403472\n",
      "Iteration 4834, loss = 0.02403227\n",
      "Iteration 4835, loss = 0.02402842\n",
      "Iteration 4836, loss = 0.02402392\n",
      "Iteration 4837, loss = 0.02402939\n",
      "Iteration 4838, loss = 0.02401973\n",
      "Iteration 4839, loss = 0.02401648\n",
      "Iteration 4840, loss = 0.02401493\n",
      "Iteration 4841, loss = 0.02401487\n",
      "Iteration 4842, loss = 0.02401177\n",
      "Iteration 4843, loss = 0.02400433\n",
      "Iteration 4844, loss = 0.02399887\n",
      "Iteration 4845, loss = 0.02400484\n",
      "Iteration 4846, loss = 0.02399437\n",
      "Iteration 4847, loss = 0.02399179\n",
      "Iteration 4848, loss = 0.02398585\n",
      "Iteration 4849, loss = 0.02398488\n",
      "Iteration 4850, loss = 0.02398028\n",
      "Iteration 4851, loss = 0.02397418\n",
      "Iteration 4852, loss = 0.02397346\n",
      "Iteration 4853, loss = 0.02396939\n",
      "Iteration 4854, loss = 0.02396757\n",
      "Iteration 4855, loss = 0.02396346\n",
      "Iteration 4856, loss = 0.02396432\n",
      "Iteration 4857, loss = 0.02395944\n",
      "Iteration 4858, loss = 0.02395615\n",
      "Iteration 4859, loss = 0.02395236\n",
      "Iteration 4860, loss = 0.02395191\n",
      "Iteration 4861, loss = 0.02394416\n",
      "Iteration 4862, loss = 0.02394767\n",
      "Iteration 4863, loss = 0.02393741\n",
      "Iteration 4864, loss = 0.02393319\n",
      "Iteration 4865, loss = 0.02392687\n",
      "Iteration 4866, loss = 0.02392629\n",
      "Iteration 4867, loss = 0.02392613\n",
      "Iteration 4868, loss = 0.02392509\n",
      "Iteration 4869, loss = 0.02391651\n",
      "Iteration 4870, loss = 0.02391309\n",
      "Iteration 4871, loss = 0.02391407\n",
      "Iteration 4872, loss = 0.02390803\n",
      "Iteration 4873, loss = 0.02390953\n",
      "Iteration 4874, loss = 0.02389787\n",
      "Iteration 4875, loss = 0.02389591\n",
      "Iteration 4876, loss = 0.02389142\n",
      "Iteration 4877, loss = 0.02388885\n",
      "Iteration 4878, loss = 0.02388885\n",
      "Iteration 4879, loss = 0.02388895\n",
      "Iteration 4880, loss = 0.02388519\n",
      "Iteration 4881, loss = 0.02387859\n",
      "Iteration 4882, loss = 0.02387783\n",
      "Iteration 4883, loss = 0.02387041\n",
      "Iteration 4884, loss = 0.02386647\n",
      "Iteration 4885, loss = 0.02386666\n",
      "Iteration 4886, loss = 0.02387914\n",
      "Iteration 4887, loss = 0.02385739\n",
      "Iteration 4888, loss = 0.02385360\n",
      "Iteration 4889, loss = 0.02385862\n",
      "Iteration 4890, loss = 0.02385389\n",
      "Iteration 4891, loss = 0.02384924\n",
      "Iteration 4892, loss = 0.02384695\n",
      "Iteration 4893, loss = 0.02383745\n",
      "Iteration 4894, loss = 0.02383695\n",
      "Iteration 4895, loss = 0.02383143\n",
      "Iteration 4896, loss = 0.02382938\n",
      "Iteration 4897, loss = 0.02382977\n",
      "Iteration 4898, loss = 0.02382422\n",
      "Iteration 4899, loss = 0.02382014\n",
      "Iteration 4900, loss = 0.02381581\n",
      "Iteration 4901, loss = 0.02381173\n",
      "Iteration 4902, loss = 0.02381309\n",
      "Iteration 4903, loss = 0.02380628\n",
      "Iteration 4904, loss = 0.02380293\n",
      "Iteration 4905, loss = 0.02380246\n",
      "Iteration 4906, loss = 0.02379597\n",
      "Iteration 4907, loss = 0.02379233\n",
      "Iteration 4908, loss = 0.02379666\n",
      "Iteration 4909, loss = 0.02378476\n",
      "Iteration 4910, loss = 0.02378137\n",
      "Iteration 4911, loss = 0.02377869\n",
      "Iteration 4912, loss = 0.02377786\n",
      "Iteration 4913, loss = 0.02377737\n",
      "Iteration 4914, loss = 0.02377105\n",
      "Iteration 4915, loss = 0.02376784\n",
      "Iteration 4916, loss = 0.02376078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4917, loss = 0.02375766\n",
      "Iteration 4918, loss = 0.02375946\n",
      "Iteration 4919, loss = 0.02376369\n",
      "Iteration 4920, loss = 0.02375268\n",
      "Iteration 4921, loss = 0.02374463\n",
      "Iteration 4922, loss = 0.02374481\n",
      "Iteration 4923, loss = 0.02374264\n",
      "Iteration 4924, loss = 0.02373825\n",
      "Iteration 4925, loss = 0.02373325\n",
      "Iteration 4926, loss = 0.02372977\n",
      "Iteration 4927, loss = 0.02372794\n",
      "Iteration 4928, loss = 0.02373124\n",
      "Iteration 4929, loss = 0.02372042\n",
      "Iteration 4930, loss = 0.02372270\n",
      "Iteration 4931, loss = 0.02371837\n",
      "Iteration 4932, loss = 0.02371740\n",
      "Iteration 4933, loss = 0.02370910\n",
      "Iteration 4934, loss = 0.02370970\n",
      "Iteration 4935, loss = 0.02370294\n",
      "Iteration 4936, loss = 0.02370090\n",
      "Iteration 4937, loss = 0.02369936\n",
      "Iteration 4938, loss = 0.02369715\n",
      "Iteration 4939, loss = 0.02368550\n",
      "Iteration 4940, loss = 0.02368884\n",
      "Iteration 4941, loss = 0.02368363\n",
      "Iteration 4942, loss = 0.02368410\n",
      "Iteration 4943, loss = 0.02368218\n",
      "Iteration 4944, loss = 0.02367615\n",
      "Iteration 4945, loss = 0.02367060\n",
      "Iteration 4946, loss = 0.02366371\n",
      "Iteration 4947, loss = 0.02367107\n",
      "Iteration 4948, loss = 0.02366289\n",
      "Iteration 4949, loss = 0.02365717\n",
      "Iteration 4950, loss = 0.02365659\n",
      "Iteration 4951, loss = 0.02365453\n",
      "Iteration 4952, loss = 0.02364785\n",
      "Iteration 4953, loss = 0.02364618\n",
      "Iteration 4954, loss = 0.02364433\n",
      "Iteration 4955, loss = 0.02364228\n",
      "Iteration 4956, loss = 0.02363542\n",
      "Iteration 4957, loss = 0.02363198\n",
      "Iteration 4958, loss = 0.02363219\n",
      "Iteration 4959, loss = 0.02362997\n",
      "Iteration 4960, loss = 0.02361981\n",
      "Iteration 4961, loss = 0.02362059\n",
      "Iteration 4962, loss = 0.02361614\n",
      "Iteration 4963, loss = 0.02361079\n",
      "Iteration 4964, loss = 0.02360932\n",
      "Iteration 4965, loss = 0.02360670\n",
      "Iteration 4966, loss = 0.02360511\n",
      "Iteration 4967, loss = 0.02360254\n",
      "Iteration 4968, loss = 0.02360081\n",
      "Iteration 4969, loss = 0.02360292\n",
      "Iteration 4970, loss = 0.02358512\n",
      "Iteration 4971, loss = 0.02358812\n",
      "Iteration 4972, loss = 0.02358508\n",
      "Iteration 4973, loss = 0.02358324\n",
      "Iteration 4974, loss = 0.02358044\n",
      "Iteration 4975, loss = 0.02357217\n",
      "Iteration 4976, loss = 0.02356642\n",
      "Iteration 4977, loss = 0.02356924\n",
      "Iteration 4978, loss = 0.02357189\n",
      "Iteration 4979, loss = 0.02356090\n",
      "Iteration 4980, loss = 0.02355771\n",
      "Iteration 4981, loss = 0.02355733\n",
      "Iteration 4982, loss = 0.02355369\n",
      "Iteration 4983, loss = 0.02354292\n",
      "Iteration 4984, loss = 0.02354406\n",
      "Iteration 4985, loss = 0.02354167\n",
      "Iteration 4986, loss = 0.02353968\n",
      "Iteration 4987, loss = 0.02353634\n",
      "Iteration 4988, loss = 0.02352804\n",
      "Iteration 4989, loss = 0.02352691\n",
      "Iteration 4990, loss = 0.02352714\n",
      "Iteration 4991, loss = 0.02352297\n",
      "Iteration 4992, loss = 0.02352316\n",
      "Iteration 4993, loss = 0.02351063\n",
      "Iteration 4994, loss = 0.02350951\n",
      "Iteration 4995, loss = 0.02351218\n",
      "Iteration 4996, loss = 0.02350752\n",
      "Iteration 4997, loss = 0.02350710\n",
      "Iteration 4998, loss = 0.02350224\n",
      "Iteration 4999, loss = 0.02349552\n",
      "Iteration 5000, loss = 0.02349758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guptap6\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "elder-mayor",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=100, max_iter=5000, solver='sgd', verbose=True, random_state=1, tol=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "democratic-darkness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.55148186\n",
      "Iteration 2, loss = 0.47344196\n",
      "Iteration 3, loss = 0.41166049\n",
      "Iteration 4, loss = 0.37065489\n",
      "Iteration 5, loss = 0.34284418\n",
      "Iteration 6, loss = 0.32277304\n",
      "Iteration 7, loss = 0.30732533\n",
      "Iteration 8, loss = 0.29474324\n",
      "Iteration 9, loss = 0.28426676\n",
      "Iteration 10, loss = 0.27512518\n",
      "Iteration 11, loss = 0.26713147\n",
      "Iteration 12, loss = 0.26000083\n",
      "Iteration 13, loss = 0.25345890\n",
      "Iteration 14, loss = 0.24756049\n",
      "Iteration 15, loss = 0.24206441\n",
      "Iteration 16, loss = 0.23692905\n",
      "Iteration 17, loss = 0.23225323\n",
      "Iteration 18, loss = 0.22784432\n",
      "Iteration 19, loss = 0.22375707\n",
      "Iteration 20, loss = 0.21989675\n",
      "Iteration 21, loss = 0.21632681\n",
      "Iteration 22, loss = 0.21286450\n",
      "Iteration 23, loss = 0.20963389\n",
      "Iteration 24, loss = 0.20665625\n",
      "Iteration 25, loss = 0.20375244\n",
      "Iteration 26, loss = 0.20103807\n",
      "Iteration 27, loss = 0.19841429\n",
      "Iteration 28, loss = 0.19595709\n",
      "Iteration 29, loss = 0.19358344\n",
      "Iteration 30, loss = 0.19130855\n",
      "Iteration 31, loss = 0.18915653\n",
      "Iteration 32, loss = 0.18707682\n",
      "Iteration 33, loss = 0.18512113\n",
      "Iteration 34, loss = 0.18318974\n",
      "Iteration 35, loss = 0.18137996\n",
      "Iteration 36, loss = 0.17957917\n",
      "Iteration 37, loss = 0.17785690\n",
      "Iteration 38, loss = 0.17620253\n",
      "Iteration 39, loss = 0.17460533\n",
      "Iteration 40, loss = 0.17304587\n",
      "Iteration 41, loss = 0.17155023\n",
      "Iteration 42, loss = 0.17010999\n",
      "Iteration 43, loss = 0.16870569\n",
      "Iteration 44, loss = 0.16733449\n",
      "Iteration 45, loss = 0.16596710\n",
      "Iteration 46, loss = 0.16464603\n",
      "Iteration 47, loss = 0.16340738\n",
      "Iteration 48, loss = 0.16215959\n",
      "Iteration 49, loss = 0.16094158\n",
      "Iteration 50, loss = 0.15976569\n",
      "Iteration 51, loss = 0.15862007\n",
      "Iteration 52, loss = 0.15750767\n",
      "Iteration 53, loss = 0.15639947\n",
      "Iteration 54, loss = 0.15532028\n",
      "Iteration 55, loss = 0.15427988\n",
      "Iteration 56, loss = 0.15323968\n",
      "Iteration 57, loss = 0.15225316\n",
      "Iteration 58, loss = 0.15126445\n",
      "Iteration 59, loss = 0.15031132\n",
      "Iteration 60, loss = 0.14936615\n",
      "Iteration 61, loss = 0.14843927\n",
      "Iteration 62, loss = 0.14754865\n",
      "Iteration 63, loss = 0.14667074\n",
      "Iteration 64, loss = 0.14582058\n",
      "Iteration 65, loss = 0.14495607\n",
      "Iteration 66, loss = 0.14412183\n",
      "Iteration 67, loss = 0.14331773\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "expanded-saturn",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "noble-classics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1331,    8],\n",
       "       [  82,   74]], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "broken-kitty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3152,   13],\n",
       "       [ 176,  146]], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred2 = clf.predict(x_train)\n",
    "confusion_matrix(y_train, pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fifty-support",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.97      1339\n",
      "           1       0.90      0.47      0.62       156\n",
      "\n",
      "    accuracy                           0.94      1495\n",
      "   macro avg       0.92      0.73      0.79      1495\n",
      "weighted avg       0.94      0.94      0.93      1495\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fatty-labor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      3165\n",
      "           1       0.92      0.45      0.61       322\n",
      "\n",
      "    accuracy                           0.95      3487\n",
      "   macro avg       0.93      0.72      0.79      3487\n",
      "weighted avg       0.94      0.95      0.94      3487\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "realistic-tribune",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "polished-gossip",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC :  0.966405659729377\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# AUC & ROC for the training data\n",
    "# Predict probabilities\n",
    "probs = clf.predict_proba(x_train)\n",
    "probs = probs[: , 1]\n",
    "\n",
    "# Calculate AUC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "auc = roc_auc_score(y_train, probs)\n",
    "print('AUC : ', auc)\n",
    "\n",
    "# Calculate roc curve\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_train, probs)\n",
    "plt.plot([0,1], [0,1], linestyle = '--')\n",
    "\n",
    "# plot the roc curve for the model\n",
    "plt.plot(fpr, tpr, marker='.')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-dinner",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
